% ============================================================
% Batch 6: Results and Analysis (Part 2 of 2)
% TD7 Jump Learning and Algorithm Evolution
% Created: 2026-01-06
% ============================================================

\subsection{TD7 Dual-Jump Learning Phenomenon}
\label{sec:results_td7}

TD7 demonstrates a unique \textbf{dual-jump learning pattern} that warrants detailed analysis, revealing insights into representation learning dynamics and policy optimization convergence. Figure~\ref{fig:td7_jump_learning} illustrates the complete training trajectory with the two critical jump moments.

\begin{figure}[t]
\centering
\includegraphics[width=\textwidth]{Figures/publication/figure4_td7_jump_learning.png}
\caption{TD7 double-jump learning phenomenon over 500K training steps. Two consecutive performance jumps occur at steps 26,689 (+857\%, from 138 to 1,321) and 26,989 (+95\%, from 2,209 to 4,309), separated by only 300 steps. The first jump corresponds to SALE representation learning breakthrough where the embedding network suddenly captures the 5-layer inverted pyramid structure, while the second jump reflects policy optimization convergence accelerated by LAP prioritized replay and Checkpoints mechanisms. Performance stabilizes at 4,351.8±51.1 after step 27K, demonstrating sustained high performance without degradation.}
\label{fig:td7_jump_learning}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=0.9\textwidth]{Figures/publication/figure4_td7_jump_learning_zoom.png}
\caption{Zoomed view of TD7's double-jump learning dynamics (steps 26,000-28,000). The rapid succession of two jumps within 300 steps reveals the synergistic effect of SALE+LAP+Checkpoints mechanisms. This non-linear breakthrough pattern contrasts sharply with the smooth convergence curves of other algorithms (see Figure~\ref{fig:learning_curves}), highlighting TD7's unique representation learning capabilities. The tight temporal coupling between jumps indicates that once SALE learns the correct state representation (Jump 1), policy optimization rapidly converges (Jump 2).}
\label{fig:td7_jump_zoom}
\end{figure}

\textbf{Complete Training Profile.}
\begin{itemize}
\item Total training steps: 499,801 ($\approx$500k)
\item Wall-clock time: 126 minutes ($\approx$2.1 hours)
\item Final evaluation reward (500k): 4,409
\item Average reward: 4,352 $\pm$ 51 (across all evaluations)
\end{itemize}

\textbf{Two Critical Jump Moments} (interval $\approx$300 steps, completion span $<$1,500 steps):

\textbf{Jump 1 @ Step 26,689}: Reward 138 $\rightarrow$ 1,321 (\textbf{+857\%})
\begin{itemize}
\item \textbf{SALE representation learning breakthrough}: The embedding network $f(s)$ surpasses a critical threshold, suddenly capturing the fundamental structure of the 5-layer inverted pyramid (C=\{2,3,4,6,8\})
\item Signifies transition from random exploration to \textbf{structured learning}---the algorithm "understands" the environment's queue dynamics
\item Analogous to neural network "aha moments" where latent representations crystallize
\end{itemize}

\textbf{Jump 2 @ Step 26,989}: Reward 2,209 $\rightarrow$ 4,309 (\textbf{+95\%})
\begin{itemize}
\item \textbf{Policy optimization convergence}: Occurring just 300 steps after Jump 1, this second leap demonstrates rapid policy refinement
\item \textbf{Checkpoints mechanism} successfully locks in high-performance policy states
\item \textbf{LAP prioritized replay} accelerates convergence by focusing on high-TD-error experiences
\end{itemize}

\textbf{Stable Plateau (27k--500k steps)}:
\begin{itemize}
\item Performance stabilizes at 4,351.8 $\pm$ 51.1
\item Sustained micro-optimization with no significant degradation
\item Final evaluation (500k): 4,409, approaching SOTA (A2C: 4,438, PPO: 4,420)
\end{itemize}

\textbf{Theoretical Significance.} The dual-jump pattern reveals TD7's learning mechanism operates in \textbf{two distinct phases}:

\textbf{Phase I (Representation Learning)}: Jump 1 corresponds to SALE reaching a critical threshold where the learned state-action embeddings $z_{sa} = g(f(s), a)$ capture sufficient structural features. These features include: (1) 5-layer capacity hierarchy C=\{2,3,4,6,8\}; (2) pressure gradients $\Delta P_\ell = U_\ell - \bar{U}$; (3) inter-layer dependencies (congestion propagation patterns). Once embeddings encode these abstractions, the Q-network can suddenly generalize across states, triggering the +857\% jump.

\textbf{Phase II (Policy Optimization)}: Jump 2 represents policy convergence after representation stabilization. With a "correct" state representation from Jump 1, the actor can rapidly optimize. LAP prioritized replay focuses sampling on states with high TD-error (typically near decision boundaries), accelerating convergence. Checkpoints preserve the jump-1 policy, preventing catastrophic forgetting.

The \textbf{tight temporal coupling} (300-step interval) between jumps indicates \textbf{strong synergy} among SALE, LAP, and Checkpoints. This contrasts sharply with DDPG's blind exploration (no representation learning) and TD3's gradual improvement (no prioritization), highlighting TD7's architectural innovations.

\textbf{SALE Mechanism Technical Details.} TD7's State-Action Learning Embeddings (SALE) consist of two neural networks working in tandem:

\textbf{(1) State Encoder $f_\phi(s)$}: Maps raw 29-dimensional queue states (5 layers $\times$ [queue lengths, utilization, pressure, service rates, transfer counts] + class counters) to a compact $d$-dimensional embedding space ($d=256$ in our implementation). The encoder learns to extract task-relevant features:
\begin{itemize}
\item \textbf{Capacity hierarchy}: Encoding C=\{2,3,4,6,8\} structure
\item \textbf{Congestion patterns}: Identifying which layers face overload ($U_\ell > 1$)
\item \textbf{Inter-layer dependencies}: Capturing how pressure propagates vertically
\end{itemize}

\textbf{(2) Action Embedder $g_\psi(z_s, a)$}: Combines state embedding $z_s = f_\phi(s)$ with 11-dimensional action $a$ (5 continuous service rates $\mu \in [0.1, 2.0]$ + 5 discrete transfers $\{0,1\}$ + 1 termination flag) to produce state-action embedding $z_{sa} = g_\psi(z_s, a)$. This joint embedding enables Q-function approximation: $Q_\theta(s, a) \approx Q_\theta(z_{sa})$.

\textbf{Key Advantage over Standard DQN/DDPG}: Traditional methods directly map $(s, a) \rightarrow Q$ through fully-connected layers, treating all state dimensions equally. SALE's two-stage process ($s \rightarrow z_s$, then $(z_s, a) \rightarrow z_{sa} \rightarrow Q$) imposes an \textbf{information bottleneck} that forces the network to learn compressed, generalizable representations. This is critical for high-dimensional queue states where most features are redundant (e.g., queue lengths at different layers are correlated).

\textbf{Empirical Evidence of SALE Effectiveness}: Jump 1 (+857\% @ 26,689 steps) marks the moment when $f_\phi$ converges to a "good" representation---the Q-network suddenly generalizes across previously unseen states because embeddings now encode the right abstractions. Without SALE, DDPG requires 439k steps to reach peak 1,958, then collapses to 809 (catastrophic forgetting). TD7's representation learning prevents this collapse by maintaining stable embeddings even as policy improves.

\textbf{Comparison to DDPG/TD3.} Section~\ref{sec:algorithm_evolution} details the DDPG$\rightarrow$TD3$\rightarrow$TD7 evolution, showing how TD7's components address DDPG's catastrophic failures and TD3's sample inefficiency.

\subsection{Algorithm Evolution: From DDPG to TD3 to TD7}
\label{sec:algorithm_evolution}

We systematically evaluate the DDPG family to reveal how incremental algorithmic improvements transform performance in high-dimensional mixed action spaces. Table~\ref{tab:ddpg_evolution} summarizes the evolution.

\begin{table}[t]
\centering
\caption{DDPG Algorithm Family Evolution}
\label{tab:ddpg_evolution}
\small
\begin{tabular}{lccp{5.5cm}}
\toprule
\textbf{Algorithm} & \textbf{Performance} & \textbf{Stability} & \textbf{Key Technical Differences} \\
\midrule
DDPG & 1,490 $\pm$ 102 & $\times$ Severe & Single Q-net, per-step update \\
TD3 & 3,972 $\pm$ 169 & \checkmark~Stable & Double Q-nets, delayed update, smoothing \\
TD7 & 4,352 $\pm$ 51 & \checkmark~Highly stable & SALE repr. learning, adaptive explore \\
\bottomrule
\end{tabular}
\end{table}

\textbf{DDPG Failure Analysis.} Vanilla DDPG exhibits severe instability (1,490 $\pm$ 102) with catastrophic forgetting: peak reward 1,958 at 439k steps $\rightarrow$ final collapse to 809 at 499k steps (59\% decline). We identify three core failure modes:

\textbf{1. Unconstrained Q-Value Overestimation.} DDPG's single Critic systematically overestimates Q-values in the 29D state space and 11D mixed action space. Overestimation errors accumulate in non-stationary queue environments (dynamic Poisson arrivals $\lambda \in [0.5, 3.0]$), eventually triggering performance collapse. Without correction mechanisms, policy gradients point toward suboptimal actions.

\textbf{2. Actor-Critic Tight Coupling Instability.} DDPG synchronously updates Actor and Critic at every step, creating mutual interference. When Critic's Q-estimates err, Actor immediately learns incorrect policy, forming positive feedback loops. In rapidly changing queue states (10$\times$ load, $\bar{\rho}=1.84$), this coupling amplifies learning variance.

\textbf{3. Deterministic Policy Exploration Insufficiency.} DDPG uses fixed Ornstein-Uhlenbeck noise for exploration. In the high-dimensional policy space (service intensities $\mu \in [0.1, 2.0]^5$ + transfer decisions $\{0,1\}^5$), fixed noise cannot adequately cover the space, causing local optima entrapment.

\textbf{TD3 Improvement Effectiveness.} TD3 achieves +167\% improvement over DDPG (3,972 vs 1,490) through three key innovations:

\textbf{(1) Clipped Double Q-Learning.} Two independent Critics compute $Q_{\theta_1}(s,a)$ and $Q_{\theta_2}(s,a)$; target values use $\min(Q_{\theta_1}, Q_{\theta_2})$, effectively suppressing overestimation bias.

\textbf{(2) Delayed Policy Update.} Actor updates every 2 Critic updates, decoupling Actor-Critic dynamics and reducing variance. This delay allows Critic to stabilize before Actor adjusts, preventing oscillations.

\textbf{(3) Target Policy Smoothing.} Target actions incorporate clipped noise $\tilde{a} = \pi_{\theta'}(s') + \epsilon$, $\epsilon \sim \text{clip}(\mathcal{N}(0, \sigma), -c, c)$, smoothing value function estimates and enhancing robustness to perturbations.

Experiments confirm TD3 achieves stable convergence (3,972 $\pm$ 169), validating these techniques for non-stationary queueing environments.

\textbf{TD7 Breakthrough.} TD7 further advances to 4,352 $\pm$ 51 (final evaluation 4,409, approaching A2C 4,438 and PPO 4,420) by adding \textbf{SALE representation learning} and \textbf{adaptive exploration}. SALE learns low-dimensional embeddings of queue states, capturing the 5-layer inverted pyramid structure (C=\{2,3,4,6,8\}) and enabling effective policy generalization across state space. Crucially, TD7 exhibits \textbf{dual-jump learning} (§\ref{sec:results_td7}): Jump 1 (+857\% @ 26,689 steps) marks SALE threshold breakthrough, Jump 2 (+95\% @ 26,989 steps) represents policy convergence---this 300-step interval demonstrates strong synergy among SALE, LAP, and Checkpoints. Compared to DDPG's blind exploration, TD7's adaptive noise mechanism dynamically adjusts exploration based on state uncertainty, achieving long-term stability (27k--500k steps: 4,352 $\pm$ 51).

\textbf{Practical Insights.} The DDPG$\rightarrow$TD7 evolution clearly illustrates algorithm design imperatives for complex control tasks. For systems with \textbf{high-dimensional states, mixed actions, and non-stationary dynamics} (characteristic of vertical queueing), simple Actor-Critic frameworks (DDPG) prove inadequate. Systematic improvements through \textbf{value function regularization} (TD3 double-Q), \textbf{training decoupling} (TD3 delayed updates), and \textbf{representation learning} (TD7 SALE) are essential for stability and performance. This progression offers valuable guidance for algorithm selection in real-time decision systems such as UAM delivery.

\subsection{Capacity Threshold and State Space Complexity}

\textbf{Sharp Stability Boundary.} Our experiments reveal a clear \textbf{capacity threshold}: configurations with K $\leq$ 25 maintain viability, while K $\geq$ 30 trigger immediate collapse. This boundary exhibits \textbf{cliff-like behavior}:

\begin{itemize}
\item \textbf{Capacity 25}: Reward 7,817, crash rate 35\%, completion 65\%
\item \textbf{Capacity 30}: Reward 13, crash rate 100\%, completion 0\% (episode length = 1 step)
\item \textbf{Performance drop}: 7,817 $\rightarrow$ 13 represents \textbf{99.8\% degradation}
\end{itemize}

\textbf{State Space Explosion Mechanism.} We attribute the threshold to exponential state space growth. As capacity K increases, the joint state space over 5 layers expands approximately as $|\mathcal{S}| \approx 3^K$:

\begin{align}
\text{Capacity 10:} \quad |\mathcal{S}| &\approx 3^{10} = 59,049 \quad \text{(manageable)} \\
\text{Capacity 25:} \quad |\mathcal{S}| &\approx 3^{25} = 8.5 \times 10^{11} \quad \text{(borderline)} \\
\text{Capacity 30:} \quad |\mathcal{S}| &\approx 3^{30} = 2.1 \times 10^{14} \quad \text{(intractable)}
\end{align}

With fixed training budget (100,000 steps), algorithms face severe \textbf{sample sparsity} at K=30, visiting negligible fractions of the state space. This prevents policy convergence, leading to immediate collapse upon deployment.

\textbf{Learning Difficulty vs Buffer Utility Tradeoff.} While larger capacities theoretically provide better overflow buffers, they simultaneously increase learning difficulty exponentially. At the critical threshold (K=25), learning difficulty begins to outweigh buffering benefits. Beyond K=30, the tradeoff inverts completely---algorithms fail to learn any coherent policy despite ample buffer space.

\textbf{Extended Training Validation.} To rigorously test whether the capacity paradox stems from inherent system properties or merely insufficient training, we conducted extended training experiments with K=30 for 1M timesteps (10$\times$ the standard budget). Results show mean reward 4.47 $\pm$ 16.07 with 100\% crash rate and episode length 1.0, actually \textbf{worse} than the 100K-step baseline (reward 13). Throughout the entire 1M training steps, the learning curve showed no upward trajectory, with rewards oscillating between $-$30 and +40 while episode length remained at 1.0 (immediate crash). This confirms that the capacity paradox is \textbf{not a training artifact} but reflects fundamental learning difficulty in exponentially growing state spaces. With 1M samples covering only 0.0000005\% of the $\approx$2$\times$10$^{14}$ state space at K=30, neural network generalization fails completely even with extended training budgets.

\textbf{Neural Network Generalization Failure Mechanism.} We analyze why neural network function approximation fails catastrophically at K=30 despite ample network capacity. Our A2C implementation uses a policy network with architecture [512, 512, 256] ($\approx$656K parameters). The generalization failure stems from three fundamental mismatches:

\textbf{(1) Capacity-Complexity Mismatch.} At K=30, the state space contains $\approx 2 \times 10^{14}$ states, yielding a parameter-to-state ratio of $656\text{K} / (2 \times 10^{14}) \approx 3.3 \times 10^{-9}$ parameters per state---eight orders of magnitude below the minimum required for tabular representation. Even with perfect generalization, the network cannot encode sufficient state-specific information.

\textbf{(2) Sample Coverage Insufficiency.} With 1M training steps covering only 0.0000005\% of the state space, the network observes vanishingly sparse samples. Unlike image classification where visual similarity enables generalization (neighboring pixels convey information), queue states exhibit \textbf{combinatorial independence}---states $[n_1, n_2, n_3, n_4, n_5]$ and $[n_1+1, n_2, n_3, n_4, n_5]$ may require fundamentally different policies depending on service rates and pressure dynamics. This lack of \textbf{smooth structure} prevents neural networks from interpolating effectively between observed states.

\textbf{(3) Gradient Dilution.} During backpropagation, policy gradients from sparse rewards (mean 4.47, std 16.07) must update 656K parameters based on minimal state revisitation. At K=30, the probability of revisiting any specific state is $\approx 1\text{M} / (2 \times 10^{14}) \approx 5 \times 10^{-9}$, effectively zero. Without repeated observations to refine state-action values, gradients become \textbf{noisy and uncorrelated}, preventing coherent policy learning. This explains why the learning curve oscillates randomly ($-$30 to +40) rather than converging.

\textbf{Empirical Evidence from K=30 Training.} The extended training experiment provides direct evidence: (a) episode length remains 1.0 throughout 1M steps, indicating zero learning progress; (b) reward variance (std=16.07) exceeds mean reward (4.47), suggesting random exploration dominates; (c) performance degrades from 100K baseline (reward 13) to 1M (reward 4.47), indicating catastrophic interference as the network encounters contradictory updates from sparse, unstructured data.

\textbf{Comparison to K=10 Success.} In contrast, K=10 achieves reward 11,180 because: (a) state space 59K is 3.4 billion times smaller ($3.4 \times 10^9$ times); (b) 100K training steps cover 169\% of state space (many states visited multiple times); (c) parameter-to-state ratio $656\text{K} / 59\text{K} \approx 11$ parameters per state enables adequate representation; (d) gradient updates accumulate coherently through repeated state visitation, enabling policy convergence.

\textbf{Practical Capacity Planning.} For real-world UAM systems, this finding suggests \textbf{optimal capacity range K=10--20}, where DRL algorithms can effectively learn control policies within reasonable training budgets. Capacity expansion beyond K=25 should be avoided---extended training validation demonstrates that even 10$\times$ increased training (1M steps) cannot overcome the K=30 collapse, ruling out training budget as the limiting factor. Capacity expansion must be paired with: (1) advanced sample-efficient algorithms (e.g., model-based RL, offline RL); (2) state abstraction techniques (e.g., feature engineering, representation learning) to compress state space; or (3) fundamentally different architectural approaches beyond standard neural network function approximation.

% ============================================================
% Data authenticity statement (Part 2)
% ============================================================
% All numerical values verified against:
% - Chinese guide §6.2: TD7 jump learning (26,689 steps +857%, 26,989 steps +95%)
% - Chinese guide §6.3: DDPG→TD3→TD7 evolution table
% - DATA_SUMMARY_FOR_PAPER.md §2.1: Capacity threshold (K=25 vs K=30)
% - State space calculations: 3^10, 3^25, 3^30
% - TD7 training time: 126.1 minutes (Chinese guide §5.2)
% ============================================================
