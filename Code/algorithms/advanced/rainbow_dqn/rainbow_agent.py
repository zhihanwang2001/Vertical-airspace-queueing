"""
Rainbow DQN Agent Implementation
æ•´åˆæ‰€æœ‰Rainbow DQNçš„ç»„ä»¶ï¼š
1. Double DQN
2. Prioritized Experience Replay
3. Dueling Networks  
4. Multi-step Learning
5. Distributional RL (C51)
6. Noisy Networks
"""

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from typing import Tuple, Dict, List, Optional
import random
from collections import deque

from .networks import DuelingNoisyNetwork, create_rainbow_network
from .prioritized_replay import PrioritizedReplayBuffer, batch_to_tensors
from .distributional_loss import DistributionalLoss


class RainbowDQNAgent:
    """Rainbow DQNæ™ºèƒ½ä½“"""
    
    def __init__(self,
                 state_space,
                 action_space,
                 config: Dict = None):
        """
        åˆå§‹åŒ–Rainbow DQNæ™ºèƒ½ä½“
        
        Args:
            state_space: çŠ¶æ€ç©ºé—´
            action_space: åŠ¨ä½œç©ºé—´
            config: é…ç½®å‚æ•°
        """
        
        # ä¼˜åŒ–åé…ç½® - åŸºäºæ ‡å‡†Rainbow DQNå®ç°
        default_config = {
            # ç½‘ç»œé…ç½®
            'hidden_dim': 512,
            'num_atoms': 51,
            'v_min': -15.0,  # é€‚åº”å‚ç›´åˆ†å±‚é˜Ÿåˆ—çš„å¥–åŠ±èŒƒå›´
            'v_max': 15.0,
            'noisy_std': 0.5,
            
            # å­¦ä¹ å‚æ•° - ä¿®å¤å…³é”®è¶…å‚æ•°
            'learning_rate': 1e-4,  # ğŸ”§ ä¿®å¤: 6.25e-5 â†’ 1e-4 (æ ‡å‡†Rainbowå­¦ä¹ ç‡)
            'gamma': 0.99,
            'target_update_freq': 2000,  # ğŸ”§ ä¿®å¤: 8000 â†’ 2000 (æ ‡å‡†Rainbowæ›´æ–°é¢‘ç‡)
            'gradient_clip': 10.0,
            
            # ä¼˜å…ˆçº§å›æ”¾ - ä¼˜åŒ–ç¼“å†²åŒºå¤§å°
            'buffer_size': 200000,  # ğŸ”§ ä¿®å¤: 1M â†’ 200k (å‡å°‘è¿‡æ—¶ç»éªŒ)
            'alpha': 0.5,
            'beta': 0.4,
            'beta_increment': 0.001,
            'epsilon': 1e-6,
            
            # å¤šæ­¥å­¦ä¹  - å¢å¼ºé•¿æœŸä¾èµ–
            'n_step': 10,  # ğŸ”§ ä¿®å¤: 3 â†’ 10 (é€‚ä¸­çš„multi-stepï¼Œæ•è·é•¿æœŸä¾èµ–)
            
            # è®­ç»ƒå‚æ•° - æ—©æœŸå­¦ä¹ æœºä¼š
            'batch_size': 32,
            'learning_starts': 5000,  # ğŸ”§ ä¿®å¤: 50000 â†’ 5000 (æ—©æœŸå¼€å§‹å­¦ä¹ )
            'train_freq': 4,
            
            # å…¶ä»–
            'seed': 42,
            'device': 'auto'
        }
        
        if config:
            default_config.update(config)
        self.config = default_config
        
        # è®¾ç½®è®¾å¤‡
        if self.config['device'] == 'auto':
            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        else:
            self.device = torch.device(self.config['device'])
        
        # è®¾ç½®éšæœºç§å­
        if self.config['seed'] is not None:
            random.seed(self.config['seed'])
            np.random.seed(self.config['seed'])
            torch.manual_seed(self.config['seed'])
        
        self.state_space = state_space
        self.action_space = action_space
        
        # å¤„ç†è¿ç»­åŠ¨ä½œç©ºé—´
        if hasattr(action_space, 'n'):
            # ç¦»æ•£åŠ¨ä½œç©ºé—´
            self.num_actions = action_space.n
            self.action_type = 'discrete'
        else:
            # è¿ç»­åŠ¨ä½œç©ºé—´ - è¿›è¡Œç¦»æ•£åŒ–
            self.action_dim = action_space.shape[0]
            self.action_low = action_space.low
            self.action_high = action_space.high
            # ä¸ºæ¯ä¸ªåŠ¨ä½œç»´åº¦åˆ›å»ºç¦»æ•£åŒ–åŒºé—´
            self.action_bins = 2  # æ¯ä¸ªç»´åº¦2ä¸ªç¦»æ•£å€¼
            self.num_actions = self.action_bins ** self.action_dim
            self.action_type = 'continuous'
            
            # åˆ›å»ºç¦»æ•£åŒ–æ˜ å°„
            self._create_action_mapping()
        
        # åˆ›å»ºç½‘ç»œ
        network_config = {
            'hidden_dim': self.config['hidden_dim'],
            'num_atoms': self.config['num_atoms'],
            'v_min': self.config['v_min'],
            'v_max': self.config['v_max']
        }
        
        # å¦‚æœæ˜¯è¿ç»­åŠ¨ä½œç©ºé—´ï¼Œæ·»åŠ action_binså‚æ•°
        if self.action_type == 'continuous':
            network_config['action_bins'] = self.action_bins
        
        self.q_network = create_rainbow_network(
            state_space, action_space, network_config
        ).to(self.device)
        
        self.target_network = create_rainbow_network(
            state_space, action_space, network_config
        ).to(self.device)
        
        # åŒæ­¥ç›®æ ‡ç½‘ç»œ
        self.update_target_network()
        
        # ä¼˜åŒ–å™¨
        self.optimizer = optim.Adam(
            self.q_network.parameters(),
            lr=self.config['learning_rate']
        )
        
        # ç»éªŒå›æ”¾ç¼“å†²åŒº
        self.replay_buffer = PrioritizedReplayBuffer(
            capacity=self.config['buffer_size'],
            alpha=self.config['alpha'],
            beta=self.config['beta'],
            beta_increment=self.config['beta_increment'],
            epsilon=self.config['epsilon']
        )
        
        # åˆ†å¸ƒå¼æŸå¤±å‡½æ•°
        self.loss_fn = DistributionalLoss(
            num_atoms=self.config['num_atoms'],
            v_min=self.config['v_min'],
            v_max=self.config['v_max'],
            gamma=self.config['gamma']
        )
        
        # å¤šæ­¥å­¦ä¹ ç¼“å†²åŒº
        self.n_step_buffer = deque(maxlen=self.config['n_step'])
        
        # è®­ç»ƒç»Ÿè®¡
        self.training_step = 0
        self.episode_rewards = []
        self.losses = []
        
        print(f"ğŸŒˆ Rainbow DQN Agent initialized on {self.device}")
        print(f"   State space: {state_space.shape}")
        if self.action_type == 'discrete':
            print(f"   Action space: {self.num_actions} (discrete)")
        else:
            print(f"   Action space: {self.action_dim}D continuous -> {self.num_actions} discrete")
        print(f"   Network atoms: {self.config['num_atoms']}")
        print(f"   Value range: [{self.config['v_min']}, {self.config['v_max']}]")
    
    def _create_action_mapping(self):
        """ä¸ºè¿ç»­åŠ¨ä½œç©ºé—´åˆ›å»ºç¦»æ•£åŒ–æ˜ å°„"""
        if self.action_type == 'discrete':
            return
        
        # ä¸ºæ¯ä¸ªåŠ¨ä½œç»´åº¦åˆ›å»ºç¦»æ•£å€¼ (ä½¿ç”¨æ›´æ¸©å’Œçš„å€¼é¿å…æç«¯åŠ¨ä½œ)
        self.action_grids = []
        for i in range(self.action_dim):
            # ä¸ä½¿ç”¨æç«¯å€¼(-1,1)ï¼Œè€Œä½¿ç”¨æ›´æ¸©å’Œçš„èŒƒå›´(-0.5,0.5)
            grid = np.linspace(-0.5, 0.5, self.action_bins)
            self.action_grids.append(grid)
        
        print(f"   Action discretization: {self.action_bins}^{self.action_dim} = {self.num_actions} discrete actions")
    
    def _discrete_to_continuous_action(self, discrete_action: int) -> np.ndarray:
        """å°†ç¦»æ•£åŠ¨ä½œè½¬æ¢ä¸ºè¿ç»­åŠ¨ä½œ"""
        if self.action_type == 'discrete':
            return discrete_action
        
        # å°†ç¦»æ•£åŠ¨ä½œç´¢å¼•è½¬æ¢ä¸ºå¤šç»´åæ ‡
        continuous_action = np.zeros(self.action_dim)
        remaining = discrete_action
        
        for i in range(self.action_dim):
            idx = remaining % self.action_bins
            continuous_action[i] = self.action_grids[i][idx]
            remaining //= self.action_bins
        
        return continuous_action
    
    def act(self, state: np.ndarray, training: bool = True):
        """é€‰æ‹©åŠ¨ä½œ"""
        if not isinstance(state, np.ndarray):
            state = np.array(state)
        
        # è½¬æ¢ä¸ºtensor
        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)
        
        with torch.no_grad():
            # è·å–Qåˆ†å¸ƒ
            q_dist = self.q_network(state_tensor)
            
            # è®¡ç®—Qå€¼ï¼ˆåˆ†å¸ƒçš„æœŸæœ›ï¼‰
            q_values = self.loss_fn.q_values_from_distribution(q_dist)
            
            # é€‰æ‹©æœ€ä½³åŠ¨ä½œï¼ˆè´ªå¿ƒï¼‰
            discrete_action = q_values.argmax(dim=1).item()
        
        # è½¬æ¢ä¸ºç¯å¢ƒæ‰€éœ€çš„åŠ¨ä½œæ ¼å¼
        if self.action_type == 'continuous':
            return self._discrete_to_continuous_action(discrete_action)
        else:
            return discrete_action
    
    def store_transition(self, 
                        state: np.ndarray,
                        action, 
                        reward: float,
                        next_state: np.ndarray,
                        done: bool):
        """å­˜å‚¨è½¬æ¢åˆ°ç¼“å†²åŒº"""
        
        # å¦‚æœæ˜¯è¿ç»­åŠ¨ä½œï¼Œéœ€è¦æ‰¾åˆ°å¯¹åº”çš„ç¦»æ•£åŠ¨ä½œç´¢å¼•
        if self.action_type == 'continuous':
            if isinstance(action, np.ndarray):
                # å°†è¿ç»­åŠ¨ä½œè½¬æ¢ä¸ºç¦»æ•£åŠ¨ä½œç´¢å¼•
                discrete_action = 0
                multiplier = 1
                
                for i in range(self.action_dim):
                    # æ‰¾åˆ°æœ€æ¥è¿‘çš„ç½‘æ ¼ç‚¹
                    closest_idx = np.argmin(np.abs(self.action_grids[i] - action[i]))
                    discrete_action += closest_idx * multiplier
                    multiplier *= self.action_bins
            else:
                discrete_action = action
        else:
            discrete_action = action
        
        # æ·»åŠ åˆ°n-stepç¼“å†²åŒº
        self.n_step_buffer.append((state, discrete_action, reward, next_state, done))
        
        # å¦‚æœn-stepç¼“å†²åŒºæ»¡äº†ï¼Œè®¡ç®—n-stepå›æŠ¥
        if len(self.n_step_buffer) == self.config['n_step']:
            # è®¡ç®—n-stepå¥–åŠ±
            n_step_reward = 0.0
            gamma = 1.0
            
            for i in range(self.config['n_step']):
                n_step_reward += gamma * self.n_step_buffer[i][2]
                gamma *= self.config['gamma']
                if self.n_step_buffer[i][4]:  # å¦‚æœdone
                    break
            
            # è·å–åˆå§‹çŠ¶æ€å’Œæœ€ç»ˆçŠ¶æ€
            initial_state = self.n_step_buffer[0][0]
            initial_action = self.n_step_buffer[0][1]
            final_next_state = self.n_step_buffer[-1][3]
            final_done = any(exp[4] for exp in self.n_step_buffer)
            
            # å­˜å‚¨n-stepç»éªŒ
            self.replay_buffer.add(
                initial_state, initial_action, n_step_reward, 
                final_next_state, final_done
            )
    
    def train(self) -> Optional[Dict]:
        """è®­ç»ƒä¸€æ­¥"""
        if not self.replay_buffer.is_ready:
            return None
        
        if self.training_step % self.config['train_freq'] != 0:
            self.training_step += 1
            return None
        
        # é‡‡æ ·ç»éªŒ
        batch, weights, indices = self.replay_buffer.sample(self.config['batch_size'])
        
        # è½¬æ¢ä¸ºtensor
        states, actions, rewards, next_states, dones = batch_to_tensors(batch, self.device)
        weights = torch.FloatTensor(weights).to(self.device)
        
        # é‡ç½®å™ªå£°
        self.q_network.reset_noise()
        self.target_network.reset_noise()
        
        # å½“å‰Qåˆ†å¸ƒ
        current_q_dist = self.q_network(states)
        
        # ç›®æ ‡ç½‘ç»œçš„ä¸‹ä¸€çŠ¶æ€Qåˆ†å¸ƒ
        with torch.no_grad():
            next_q_dist = self.target_network(next_states)
            
            # Double DQNï¼šä½¿ç”¨å½“å‰ç½‘ç»œé€‰æ‹©åŠ¨ä½œ
            next_q_values = self.loss_fn.q_values_from_distribution(
                self.q_network(next_states)
            )
            next_actions = next_q_values.argmax(dim=1)
        
        # è®¡ç®—æŸå¤±
        loss, td_errors = self.loss_fn.compute_loss(
            current_q_dist, actions, rewards, next_q_dist, next_actions, dones, weights
        )
        
        # åå‘ä¼ æ’­
        self.optimizer.zero_grad()
        loss.backward()
        
        # æ¢¯åº¦è£å‰ª
        torch.nn.utils.clip_grad_norm_(
            self.q_network.parameters(), 
            self.config['gradient_clip']
        )
        
        self.optimizer.step()
        
        # æ›´æ–°ä¼˜å…ˆçº§
        self.replay_buffer.update_priorities(indices, td_errors)
        self.replay_buffer.update_beta()
        
        # æ›´æ–°ç›®æ ‡ç½‘ç»œ
        if self.training_step % self.config['target_update_freq'] == 0:
            self.update_target_network()
        
        self.training_step += 1
        self.losses.append(loss.item())
        
        return {
            'loss': loss.item(),
            'td_error_mean': np.mean(td_errors),
            'beta': self.replay_buffer.beta,
            'buffer_size': len(self.replay_buffer)
        }
    
    def update_target_network(self):
        """æ›´æ–°ç›®æ ‡ç½‘ç»œ"""
        self.target_network.load_state_dict(self.q_network.state_dict())
    
    def save(self, filepath: str):
        """ä¿å­˜æ¨¡å‹"""
        torch.save({
            'q_network': self.q_network.state_dict(),
            'target_network': self.target_network.state_dict(),
            'optimizer': self.optimizer.state_dict(),
            'config': self.config,
            'training_step': self.training_step
        }, filepath)
    
    def load(self, filepath: str):
        """åŠ è½½æ¨¡å‹"""
        checkpoint = torch.load(filepath, map_location=self.device)
        self.q_network.load_state_dict(checkpoint['q_network'])
        self.target_network.load_state_dict(checkpoint['target_network'])
        self.optimizer.load_state_dict(checkpoint['optimizer'])
        self.training_step = checkpoint['training_step']
        
        print(f"âœ… Rainbow DQN model loaded from {filepath}")
    
    def get_stats(self) -> Dict:
        """è·å–è®­ç»ƒç»Ÿè®¡"""
        return {
            'training_step': self.training_step,
            'buffer_size': len(self.replay_buffer),
            'avg_loss': np.mean(self.losses[-100:]) if self.losses else 0,
            'beta': self.replay_buffer.beta,
            'episodes_trained': len(self.episode_rewards)
        }
    
    def reset_noise(self):
        """é‡ç½®ç½‘ç»œå™ªå£°"""
        self.q_network.reset_noise()
        self.target_network.reset_noise()