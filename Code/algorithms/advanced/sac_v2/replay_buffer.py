"""
SAC v2 Replay Buffer
SAC v2ç®—æ³•çš„ç»éªŒå›æ”¾ç¼“å†²åŒº
"""

import numpy as np
import torch
import random
from typing import Dict, Tuple, Any, Optional
from collections import deque


class SAC_ReplayBuffer:
    """SACç»éªŒå›æ”¾ç¼“å†²åŒº"""
    
    def __init__(self,
                 capacity: int = 100000,
                 batch_size: int = 256,
                 device: torch.device = torch.device('cpu')):
        """
        åˆå§‹åŒ–å›æ”¾ç¼“å†²åŒº
        
        Args:
            capacity: ç¼“å†²åŒºå®¹é‡
            batch_size: æ‰¹æ¬¡å¤§å°
            device: è®¡ç®—è®¾å¤‡
        """
        self.capacity = capacity
        self.batch_size = batch_size
        self.device = device
        
        # ä½¿ç”¨dequeå­˜å‚¨ç»éªŒ
        self.buffer = deque(maxlen=capacity)
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.total_samples = 0
        
        print(f"ğŸ“¦ SAC Replay Buffer initialized")
        print(f"   Capacity: {capacity:,}")
        print(f"   Batch size: {batch_size}")
    
    def add(self,
            state: np.ndarray,
            action: np.ndarray, 
            reward: float,
            next_state: np.ndarray,
            done: bool):
        """
        æ·»åŠ ä¸€ä¸ªç»éªŒåˆ°ç¼“å†²åŒº
        
        Args:
            state: å½“å‰çŠ¶æ€
            action: æ‰§è¡Œçš„åŠ¨ä½œ
            reward: è·å¾—çš„å¥–åŠ±
            next_state: ä¸‹ä¸€çŠ¶æ€
            done: æ˜¯å¦ç»“æŸ
        """
        experience = {
            'state': np.array(state, dtype=np.float32),
            'action': np.array(action, dtype=np.float32),
            'reward': float(reward),
            'next_state': np.array(next_state, dtype=np.float32),
            'done': bool(done)
        }
        
        self.buffer.append(experience)
        self.total_samples += 1
    
    def sample(self, batch_size: Optional[int] = None) -> Optional[Dict[str, torch.Tensor]]:
        """
        ä»ç¼“å†²åŒºé‡‡æ ·ä¸€ä¸ªæ‰¹æ¬¡
        
        Args:
            batch_size: æ‰¹æ¬¡å¤§å°ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤å¤§å°
            
        Returns:
            æ‰¹æ¬¡å­—å…¸ï¼Œå¦‚æœæ ·æœ¬ä¸è¶³è¿”å›None
        """
        if batch_size is None:
            batch_size = self.batch_size
        
        if len(self.buffer) < batch_size:
            return None
        
        # éšæœºé‡‡æ ·
        batch = random.sample(self.buffer, batch_size)
        
        # åˆ†ç¦»æ•°æ®
        states = np.array([exp['state'] for exp in batch])
        actions = np.array([exp['action'] for exp in batch])
        rewards = np.array([exp['reward'] for exp in batch])
        next_states = np.array([exp['next_state'] for exp in batch])
        dones = np.array([exp['done'] for exp in batch])
        
        # è½¬æ¢ä¸ºå¼ é‡
        batch_tensors = {
            'states': torch.FloatTensor(states).to(self.device),
            'actions': torch.FloatTensor(actions).to(self.device),
            'rewards': torch.FloatTensor(rewards).unsqueeze(1).to(self.device),
            'next_states': torch.FloatTensor(next_states).to(self.device),
            'dones': torch.FloatTensor(dones.astype(np.float32)).unsqueeze(1).to(self.device)
        }
        
        return batch_tensors
    
    def __len__(self):
        """è¿”å›ç¼“å†²åŒºå½“å‰å¤§å°"""
        return len(self.buffer)
    
    @property
    def is_ready(self):
        """æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿæ ·æœ¬è¿›è¡Œè®­ç»ƒ"""
        return len(self.buffer) >= self.batch_size
    
    def clear(self):
        """æ¸…ç©ºç¼“å†²åŒº"""
        self.buffer.clear()
        self.total_samples = 0
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç¼“å†²åŒºç»Ÿè®¡ä¿¡æ¯"""
        if len(self.buffer) == 0:
            return {
                'buffer_size': 0,
                'total_samples': self.total_samples,
                'fill_percentage': 0.0,
                'avg_reward': 0.0,
                'avg_episode_length': 0.0
            }
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        rewards = [exp['reward'] for exp in self.buffer]
        
        return {
            'buffer_size': len(self.buffer),
            'total_samples': self.total_samples,
            'fill_percentage': len(self.buffer) / self.capacity * 100,
            'avg_reward': np.mean(rewards),
            'reward_std': np.std(rewards),
            'min_reward': np.min(rewards),
            'max_reward': np.max(rewards)
        }