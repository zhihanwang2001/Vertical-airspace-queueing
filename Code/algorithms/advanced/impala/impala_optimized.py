"""
IMPALA Optimized for Vertical Stratified Queue System
ä¸“é—¨é’ˆå¯¹å‚ç›´åˆ†å±‚é˜Ÿåˆ—ç¯å¢ƒä¼˜åŒ–çš„IMPALAå®ç°

æ ¸å¿ƒä¼˜åŒ–:
1. æ”¯æŒæ··åˆåŠ¨ä½œç©ºé—´ï¼ˆè¿ç»­+ç¦»æ•£ï¼‰
2. é˜Ÿåˆ—ç³»ç»Ÿä¸“ç”¨çš„ç½‘ç»œæ¶æ„
3. ä¿å®ˆçš„V-traceå‚æ•°è®¾ç½®
4. é’ˆå¯¹ç¯å¢ƒç‰¹ç‚¹çš„çŠ¶æ€ç‰¹å¾æå–
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.tensorboard import SummaryWriter
import time
from typing import Dict, Any, Optional, Tuple

from env.drl_optimized_env_fixed import DRLOptimizedQueueEnvFixed
from baselines.space_utils import SB3DictWrapper


class QueueSpecificNetwork(nn.Module):
    """ä¸“é—¨ä¸ºé˜Ÿåˆ—ç³»ç»Ÿè®¾è®¡çš„ç½‘ç»œæ¶æ„"""

    def __init__(self, state_dim: int, config: Dict = None):
        super().__init__()

        # é»˜è®¤é…ç½®
        self.config = config or {}
        self.hidden_dim = self.config.get('hidden_dim', 512)
        self.num_layers = self.config.get('num_layers', 3)

        # é˜Ÿåˆ—ç‰¹å¾ç»´åº¦ï¼ˆç¯å¢ƒå›ºå®šä¸º5å±‚ï¼‰
        self.n_layers = 5

        # åˆ†å±‚ç‰¹å¾æå–
        # 1. é˜Ÿåˆ—çŠ¶æ€ç‰¹å¾æå–å™¨ï¼ˆ5å±‚é˜Ÿåˆ—çš„ä¸“ç”¨å¤„ç†ï¼‰
        self.queue_feature_extractor = nn.Sequential(
            nn.Linear(self.n_layers * 7, 256),  # 7ä¸ªç‰¹å¾per layer: lengths, util, changes, load, service, capacity, weights
            nn.ReLU(),
            nn.Linear(256, 128),
            nn.ReLU()
        )

        # 2. ç³»ç»Ÿçº§ç‰¹å¾æå–å™¨
        self.system_feature_extractor = nn.Sequential(
            nn.Linear(4, 64),  # system_metrics (3) + prev_reward (1)
            nn.ReLU(),
            nn.Linear(64, 32),
            nn.ReLU()
        )

        # 3. èåˆå±‚
        fusion_input_dim = 128 + 32  # queue features + system features
        self.fusion_layers = nn.Sequential(
            nn.Linear(fusion_input_dim, self.hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(self.hidden_dim, self.hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1)
        )

        # 4. è¾“å‡ºå¤´
        # Actor: æ··åˆåŠ¨ä½œç©ºé—´
        # è¿ç»­åŠ¨ä½œ: service_intensities (5) + arrival_multiplier (1) = 6
        self.continuous_actor_mean = nn.Linear(self.hidden_dim, 6)
        self.continuous_actor_logstd = nn.Linear(self.hidden_dim, 6)

        # ç¦»æ•£åŠ¨ä½œ: emergency_transfers (5ä¸ªäºŒè¿›åˆ¶é€‰æ‹©)
        self.discrete_actor = nn.Linear(self.hidden_dim, self.n_layers)

        # Critic: ä»·å€¼å‡½æ•°
        self.critic = nn.Linear(self.hidden_dim, 1)

        # åˆå§‹åŒ–æƒé‡
        self._init_weights()

        print(f"ğŸ—ï¸  Queue-Specific Network initialized:")
        print(f"   - Queue layers: {self.n_layers}")
        print(f"   - Hidden dim: {self.hidden_dim}")
        print(f"   - Continuous actions: 6 (service_intensities + arrival_multiplier)")
        print(f"   - Discrete actions: {self.n_layers} (emergency_transfers)")

    def _init_weights(self):
        """åˆå§‹åŒ–ç½‘ç»œæƒé‡"""
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.orthogonal_(module.weight, gain=np.sqrt(2))
                nn.init.constant_(module.bias, 0.0)

        # Actorè¾“å‡ºå±‚ä½¿ç”¨å°çš„åˆå§‹åŒ–å€¼
        nn.init.orthogonal_(self.continuous_actor_mean.weight, gain=0.01)
        nn.init.orthogonal_(self.continuous_actor_logstd.weight, gain=0.01)
        nn.init.orthogonal_(self.discrete_actor.weight, gain=0.01)

    def extract_queue_features(self, state: torch.Tensor) -> torch.Tensor:
        """æå–é˜Ÿåˆ—ç›¸å…³ç‰¹å¾"""
        # å‡è®¾stateæ˜¯flattenåçš„35ç»´å‘é‡
        # é‡æ„ä¸ºæœ‰æ„ä¹‰çš„é˜Ÿåˆ—ç‰¹å¾

        batch_size = state.shape[0]

        # æŒ‰ç…§ç¯å¢ƒçš„è§‚æµ‹ç©ºé—´æå–ç‰¹å¾
        # queue_lengths (5) + utilization_rates (5) + queue_changes (5) +
        # load_rates (5) + service_rates (5) + prev_reward (1) + system_metrics (3) = 29
        # å‰©ä½™ç»´åº¦ä¸ºæ‰©å±•ç‰¹å¾

        queue_lengths = state[:, :5]
        utilization_rates = state[:, 5:10]
        queue_changes = state[:, 10:15]
        load_rates = state[:, 15:20]
        service_rates = state[:, 20:25]
        # prev_reward = state[:, 25:26]  # åé¢å•ç‹¬å¤„ç†
        # system_metrics = state[:, 26:29]  # åé¢å•ç‹¬å¤„ç†

        # æ·»åŠ å›ºå®šçš„é˜Ÿåˆ—ç‰¹å¾ï¼ˆå®¹é‡å’Œæƒé‡ï¼‰
        device = state.device
        capacities = torch.tensor([8, 6, 4, 3, 2], dtype=torch.float32, device=device).unsqueeze(0).expand(batch_size, -1)
        arrival_weights = torch.tensor([0.3, 0.25, 0.2, 0.15, 0.1], dtype=torch.float32, device=device).unsqueeze(0).expand(batch_size, -1)

        # åˆå¹¶é˜Ÿåˆ—ç‰¹å¾ [batch, 5*7]
        queue_features = torch.cat([
            queue_lengths, utilization_rates, queue_changes,
            load_rates, service_rates, capacities, arrival_weights
        ], dim=1)

        return self.queue_feature_extractor(queue_features)

    def extract_system_features(self, state: torch.Tensor) -> torch.Tensor:
        """æå–ç³»ç»Ÿçº§ç‰¹å¾"""
        batch_size = state.shape[0]

        # æå–ç³»ç»Ÿçº§ç‰¹å¾
        if state.shape[1] >= 29:
            prev_reward = state[:, 25:26]
            system_metrics = state[:, 26:29]
        else:
            # å¦‚æœç»´åº¦ä¸å¤Ÿï¼Œç”¨é›¶å¡«å……
            prev_reward = torch.zeros(batch_size, 1, device=state.device)
            system_metrics = torch.zeros(batch_size, 3, device=state.device)

        system_features = torch.cat([system_metrics, prev_reward], dim=1)
        return self.system_feature_extractor(system_features)

    def forward(self, state: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        å‰å‘ä¼ æ’­

        Returns:
            continuous_mean: è¿ç»­åŠ¨ä½œå‡å€¼ [batch, 6]
            continuous_logstd: è¿ç»­åŠ¨ä½œlogæ ‡å‡†å·® [batch, 6]
            discrete_logits: ç¦»æ•£åŠ¨ä½œlogits [batch, 5]
            value: çŠ¶æ€ä»·å€¼ [batch, 1]
        """
        # ç‰¹å¾æå–
        queue_features = self.extract_queue_features(state)
        system_features = self.extract_system_features(state)

        # ç‰¹å¾èåˆ
        combined_features = torch.cat([queue_features, system_features], dim=1)
        fused_features = self.fusion_layers(combined_features)

        # è¾“å‡ºè®¡ç®—
        continuous_mean = self.continuous_actor_mean(fused_features)
        continuous_logstd = torch.clamp(self.continuous_actor_logstd(fused_features), -10, 2)
        discrete_logits = self.discrete_actor(fused_features)
        value = self.critic(fused_features)

        return continuous_mean, continuous_logstd, discrete_logits, value

    def get_action_and_value(self, state: torch.Tensor, deterministic: bool = False):
        """è·å–åŠ¨ä½œå’Œä»·å€¼"""
        continuous_mean, continuous_logstd, discrete_logits, value = self.forward(state)

        if deterministic:
            # ç¡®å®šæ€§ç­–ç•¥
            continuous_action = continuous_mean
            discrete_action = torch.sigmoid(discrete_logits) > 0.5

            # è®¡ç®—log_prob (ç”¨äºä¸€è‡´æ€§)
            continuous_log_prob = torch.zeros_like(continuous_mean).sum(dim=-1, keepdim=True)
            discrete_log_prob = torch.zeros_like(discrete_logits).sum(dim=-1, keepdim=True)
        else:
            # éšæœºç­–ç•¥
            # è¿ç»­åŠ¨ä½œé‡‡æ ·
            continuous_std = torch.exp(continuous_logstd)
            continuous_dist = torch.distributions.Normal(continuous_mean, continuous_std)
            continuous_action = continuous_dist.sample()
            continuous_log_prob = continuous_dist.log_prob(continuous_action).sum(dim=-1, keepdim=True)

            # ç¦»æ•£åŠ¨ä½œé‡‡æ ·
            discrete_dist = torch.distributions.Bernoulli(logits=discrete_logits)
            discrete_action = discrete_dist.sample()
            discrete_log_prob = discrete_dist.log_prob(discrete_action).sum(dim=-1, keepdim=True)

        # åˆå¹¶log_prob
        total_log_prob = continuous_log_prob + discrete_log_prob

        # ç»„åˆåŠ¨ä½œ
        action = torch.cat([continuous_action, discrete_action], dim=-1)

        return action, total_log_prob, value

    def evaluate_action(self, state: torch.Tensor, action: torch.Tensor):
        """è¯„ä¼°ç»™å®šçŠ¶æ€å’ŒåŠ¨ä½œ"""
        continuous_mean, continuous_logstd, discrete_logits, value = self.forward(state)

        # åˆ†ç¦»è¿ç»­å’Œç¦»æ•£åŠ¨ä½œ
        continuous_action = action[:, :6]
        discrete_action = action[:, 6:]

        # è®¡ç®—è¿ç»­åŠ¨ä½œçš„log_probå’Œç†µ
        continuous_std = torch.exp(continuous_logstd)
        continuous_dist = torch.distributions.Normal(continuous_mean, continuous_std)
        continuous_log_prob = continuous_dist.log_prob(continuous_action).sum(dim=-1, keepdim=True)
        continuous_entropy = continuous_dist.entropy().sum(dim=-1, keepdim=True)

        # è®¡ç®—ç¦»æ•£åŠ¨ä½œçš„log_probå’Œç†µ
        discrete_dist = torch.distributions.Bernoulli(logits=discrete_logits)
        discrete_log_prob = discrete_dist.log_prob(discrete_action).sum(dim=-1, keepdim=True)
        discrete_entropy = discrete_dist.entropy().sum(dim=-1, keepdim=True)

        # åˆå¹¶
        total_log_prob = continuous_log_prob + discrete_log_prob
        total_entropy = continuous_entropy + discrete_entropy

        return total_log_prob, value, total_entropy


class OptimizedIMPALAAgent:
    """ä¼˜åŒ–çš„IMPALAæ™ºèƒ½ä½“"""

    def __init__(self, state_space, action_space, config: Dict = None):
        # ä¿å®ˆçš„ä¼˜åŒ–é…ç½®
        default_config = {
            # ç½‘ç»œé…ç½® - å¢åŠ ç½‘ç»œå®¹é‡
            'hidden_dim': 512,
            'num_layers': 3,

            # å­¦ä¹ å‚æ•° - æ›´ä¿å®ˆçš„è®¾ç½®
            'learning_rate': 5e-5,  # é™ä½å­¦ä¹ ç‡
            'gamma': 0.99,
            'entropy_coeff': 0.02,  # å¢åŠ æ¢ç´¢
            'value_loss_coeff': 0.5,
            'gradient_clip': 10.0,  # æ›´ä¸¥æ ¼çš„æ¢¯åº¦è£å‰ª

            # V-traceå‚æ•° - ä¿å®ˆè®¾ç½®é¿å…è®­ç»ƒå´©æºƒ
            'rho_bar': 0.8,  # é™ä½é‡è¦æ€§æƒé‡æˆªæ–­
            'c_bar': 0.8,    # é™ä½TDæƒé‡æˆªæ–­

            # å›æ”¾ç¼“å†²åŒº - å¢åŠ å®¹é‡å’Œåºåˆ—é•¿åº¦
            'buffer_size': 50000,  # å¢åŠ ç¼“å†²åŒº
            'sequence_length': 32,  # å¢åŠ åºåˆ—é•¿åº¦æ•è·é•¿æœŸä¾èµ–
            'batch_size': 32,       # å¢åŠ æ‰¹æ¬¡å¤§å°

            # è®­ç»ƒå‚æ•° - æ›´é¢‘ç¹çš„æ›´æ–°
            'learning_starts': 2000,
            'train_freq': 2,  # æ›´é¢‘ç¹è®­ç»ƒ
            'update_freq': 50,

            # å…¶ä»–
            'seed': 42,
            'device': 'auto'
        }

        if config:
            default_config.update(config)
        self.config = default_config

        # è®¾ç½®è®¾å¤‡
        if self.config['device'] == 'auto':
            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        else:
            self.device = torch.device(self.config['device'])

        # è®¾ç½®éšæœºç§å­
        if self.config['seed'] is not None:
            torch.manual_seed(self.config['seed'])
            np.random.seed(self.config['seed'])

        self.state_space = state_space
        self.action_space = action_space

        # è·å–çŠ¶æ€ç»´åº¦
        if hasattr(state_space, 'shape'):
            self.state_dim = state_space.shape[0]
        else:
            # å¤„ç†DictçŠ¶æ€ç©ºé—´
            self.state_dim = sum([space.shape[0] for space in state_space.spaces.values()])

        # åˆ›å»ºä¸“ç”¨ç½‘ç»œ
        self.network = QueueSpecificNetwork(
            state_dim=self.state_dim,
            config=self.config
        ).to(self.device)

        # ä¼˜åŒ–å™¨
        self.optimizer = torch.optim.Adam(
            self.network.parameters(),
            lr=self.config['learning_rate'],
            eps=1e-8  # å¢åŠ æ•°å€¼ç¨³å®šæ€§
        )

        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
            self.optimizer, T_max=100000, eta_min=1e-6
        )

        # ç®€å•çš„ç»éªŒå­˜å‚¨
        self.memory = []
        self.max_memory_size = self.config['buffer_size']

        # è®­ç»ƒç»Ÿè®¡
        self.training_step = 0
        self.episode_count = 0

        print(f"ğŸš€ Optimized IMPALA Agent initialized on {self.device}")
        print(f"   - Conservative V-trace: rho_bar={self.config['rho_bar']}, c_bar={self.config['c_bar']}")
        print(f"   - Lower learning rate: {self.config['learning_rate']}")
        print(f"   - Larger buffer: {self.config['buffer_size']}")
        print(f"   - Longer sequences: {self.config['sequence_length']}")

    def act(self, state, training: bool = True):
        """é€‰æ‹©åŠ¨ä½œ"""
        if isinstance(state, dict):
            # å°†DictçŠ¶æ€è½¬æ¢ä¸ºæ‰å¹³å‘é‡
            state_vector = []
            for key in ['queue_lengths', 'utilization_rates', 'queue_changes',
                       'load_rates', 'service_rates', 'prev_reward', 'system_metrics']:
                if key in state:
                    value = state[key]
                    if isinstance(value, np.ndarray):
                        state_vector.extend(value.flatten())
                    elif hasattr(value, 'flatten'):
                        state_vector.extend(value.flatten())
                    elif isinstance(value, (list, tuple)):
                        state_vector.extend(value)
                    else:
                        state_vector.append(float(value))
            state = np.array(state_vector, dtype=np.float32)

        if not isinstance(state, np.ndarray):
            state = np.array(state, dtype=np.float32)

        # è½¬æ¢ä¸ºtensor
        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)

        with torch.no_grad():
            action, log_prob, value = self.network.get_action_and_value(
                state_tensor, deterministic=not training
            )

            action = action.cpu().numpy()[0]
            log_prob = log_prob.cpu().numpy()[0]
            value = value.cpu().numpy()[0]

        # å­˜å‚¨ç”¨äºè®­ç»ƒçš„åŸå§‹åŠ¨ä½œå’Œè½¬æ¢åçš„åŠ¨ä½œ
        self._last_raw_action = action
        self._last_log_prob = log_prob[0]
        self._last_value = value[0]

        # è¿”å›åŸå§‹åŠ¨ä½œå‘é‡ï¼ˆè®©SB3DictWrapperè¿›è¡Œè½¬æ¢ï¼‰
        return action

    def store_transition(self, state, action, reward, next_state, done):
        """å­˜å‚¨ç»éªŒ"""
        if hasattr(self, '_last_raw_action'):
            self.memory.append({
                'state': state,
                'action': self._last_raw_action,
                'reward': reward,
                'next_state': next_state,
                'done': done,
                'log_prob': self._last_log_prob,
                'value': self._last_value
            })

            # é™åˆ¶å†…å­˜å¤§å°
            if len(self.memory) > self.max_memory_size:
                self.memory.pop(0)

    def train(self):
        """è®­ç»ƒä¸€æ­¥"""
        if len(self.memory) < self.config['sequence_length'] * self.config['batch_size']:
            return None

        if self.training_step % self.config['train_freq'] != 0:
            self.training_step += 1
            return None

        # ç®€åŒ–çš„V-traceè®­ç»ƒ
        batch_size = min(self.config['batch_size'], len(self.memory) // self.config['sequence_length'])

        total_loss = 0.0
        pg_loss_sum = 0.0
        value_loss_sum = 0.0
        entropy_loss_sum = 0.0

        for _ in range(batch_size):
            # éšæœºé‡‡æ ·åºåˆ—
            start_idx = np.random.randint(0, len(self.memory) - self.config['sequence_length'])
            sequence = self.memory[start_idx:start_idx + self.config['sequence_length']]

            # æ„å»ºbatchæ•°æ®
            states = torch.FloatTensor([self._process_state(exp['state']) for exp in sequence]).to(self.device)
            actions = torch.FloatTensor([exp['action'] for exp in sequence]).to(self.device)
            rewards = torch.FloatTensor([exp['reward'] for exp in sequence]).to(self.device)
            dones = torch.FloatTensor([exp['done'] for exp in sequence]).to(self.device)
            old_log_probs = torch.FloatTensor([exp['log_prob'] for exp in sequence]).to(self.device)

            # è®¡ç®—å½“å‰ç­–ç•¥çš„è¾“å‡º
            new_log_probs, values, entropies = self.network.evaluate_action(states, actions)
            values = values.squeeze(-1)
            new_log_probs = new_log_probs.squeeze(-1)
            entropies = entropies.squeeze(-1)

            # ç®€åŒ–çš„V-traceè®¡ç®—
            with torch.no_grad():
                # è®¡ç®—é‡è¦æ€§æƒé‡
                importance_weights = torch.exp(new_log_probs - old_log_probs)
                clipped_importance_weights = torch.clamp(importance_weights, max=self.config['rho_bar'])

                # è®¡ç®—V-trace targets
                next_values = torch.cat([values[1:], torch.zeros(1, device=self.device)])
                td_targets = rewards + self.config['gamma'] * next_values * (1 - dones)
                advantages = clipped_importance_weights * (td_targets - values)

            # è®¡ç®—æŸå¤±
            pg_loss = -(new_log_probs * advantages.detach()).mean()
            value_loss = F.mse_loss(values, td_targets.detach())
            entropy_loss = -entropies.mean()

            # ç»„åˆæŸå¤±
            loss = pg_loss + self.config['value_loss_coeff'] * value_loss + self.config['entropy_coeff'] * entropy_loss

            total_loss += loss
            pg_loss_sum += pg_loss.item()
            value_loss_sum += value_loss.item()
            entropy_loss_sum += entropy_loss.item()

        # åå‘ä¼ æ’­
        self.optimizer.zero_grad()
        total_loss.backward()
        torch.nn.utils.clip_grad_norm_(self.network.parameters(), self.config['gradient_clip'])
        self.optimizer.step()
        self.scheduler.step()

        self.training_step += 1

        return {
            'total_loss': total_loss.item() / batch_size,
            'pg_loss': pg_loss_sum / batch_size,
            'value_loss': value_loss_sum / batch_size,
            'entropy_loss': entropy_loss_sum / batch_size,
            'mean_advantage': advantages.mean().item(),
            'buffer_size': len(self.memory),
            'learning_rate': self.scheduler.get_last_lr()[0]
        }

    def _process_state(self, state):
        """å¤„ç†çŠ¶æ€ä¸ºå‘é‡æ ¼å¼"""
        if isinstance(state, dict):
            state_vector = []
            for key in ['queue_lengths', 'utilization_rates', 'queue_changes',
                       'load_rates', 'service_rates', 'prev_reward', 'system_metrics']:
                if key in state:
                    value = state[key]
                    if isinstance(value, np.ndarray):
                        state_vector.extend(value.flatten())
                    elif hasattr(value, 'flatten'):
                        state_vector.extend(value.flatten())
                    elif isinstance(value, (list, tuple)):
                        state_vector.extend(value)
                    else:
                        state_vector.append(float(value))
            return np.array(state_vector, dtype=np.float32)
        return np.array(state, dtype=np.float32)

    def save(self, filepath: str):
        """ä¿å­˜æ¨¡å‹"""
        torch.save({
            'network': self.network.state_dict(),
            'optimizer': self.optimizer.state_dict(),
            'scheduler': self.scheduler.state_dict(),
            'config': self.config,
            'training_step': self.training_step
        }, filepath)

    def load(self, filepath: str):
        """åŠ è½½æ¨¡å‹"""
        checkpoint = torch.load(filepath, map_location=self.device)
        self.network.load_state_dict(checkpoint['network'])
        self.optimizer.load_state_dict(checkpoint['optimizer'])
        if 'scheduler' in checkpoint:
            self.scheduler.load_state_dict(checkpoint['scheduler'])
        self.training_step = checkpoint['training_step']


class OptimizedIMPALABaseline:
    """ä¼˜åŒ–çš„IMPALAåŸºçº¿ç®—æ³•"""

    def __init__(self, config: Dict[str, Any] = None):
        self.config = config or {}
        self.agent = None
        self.env = None
        self.training_history = {
            'episode_rewards': [],
            'episode_lengths': [],
            'avg_rewards': [],
            'loss_values': [],
            'training_steps': []
        }

        print("ğŸ¯ Optimized IMPALA Baseline initialized with queue-specific optimizations")

    def setup_env(self):
        """è®¾ç½®ç¯å¢ƒ"""
        base_env = DRLOptimizedQueueEnvFixed()
        self.env = SB3DictWrapper(base_env)

        print(f"âœ… Environment setup completed")
        print(f"   Observation space: {self.env.observation_space}")
        print(f"   Action space: {self.env.action_space}")

        return self.env

    def create_agent(self):
        """åˆ›å»ºä¼˜åŒ–çš„IMPALAæ™ºèƒ½ä½“"""
        if self.env is None:
            self.setup_env()

        self.agent = OptimizedIMPALAAgent(
            state_space=self.env.observation_space,
            action_space=self.env.action_space,
            config=self.config
        )

        print("âœ… Optimized IMPALA Agent created successfully")
        return self.agent

    def train(self, total_timesteps: int, eval_freq: int = 10000, save_freq: int = 50000):
        """è®­ç»ƒä¼˜åŒ–çš„IMPALAæ¨¡å‹"""
        if self.agent is None:
            self.create_agent()

        # åˆ›å»ºTensorBoard writer
        tb_log_name = f"IMPALA_Optimized_{int(time.time())}"
        writer = SummaryWriter(log_dir=f"./tensorboard_logs/{tb_log_name}")

        print(f"ğŸš€ Starting Optimized IMPALA training for {total_timesteps:,} timesteps...")
        print(f"   TensorBoard log: {tb_log_name}")
        print(f"   Key optimizations:")
        print(f"   - Mixed action space support")
        print(f"   - Queue-specific network architecture")
        print(f"   - Conservative V-trace parameters")
        print(f"   - Lower learning rate with scheduling")

        # è®­ç»ƒå¾ªç¯
        episode = 0
        timestep = 0
        episode_reward = 0.0
        episode_length = 0

        state, _ = self.env.reset()
        start_time = time.time()

        while timestep < total_timesteps:
            # é€‰æ‹©åŠ¨ä½œ
            action = self.agent.act(state, training=True)

            # æ‰§è¡ŒåŠ¨ä½œ
            try:
                step_result = self.env.step(action)
                if len(step_result) == 5:
                    next_state, reward, terminated, truncated, info = step_result
                    done = terminated or truncated
                else:
                    next_state, reward, done, info = step_result
            except Exception as e:
                print(f"âŒ Environment step error: {e}")
                break

            # å­˜å‚¨ç»éªŒ
            self.agent.store_transition(state, action, reward, next_state, done)

            # æ›´æ–°ç»Ÿè®¡
            episode_reward += reward
            episode_length += 1
            timestep += 1

            # è®­ç»ƒæ™ºèƒ½ä½“
            if timestep >= self.config.get('learning_starts', 2000):
                train_info = self.agent.train()

                if train_info and timestep % 1000 == 0:
                    # è®°å½•è®­ç»ƒä¿¡æ¯
                    writer.add_scalar('train/total_loss', train_info['total_loss'], timestep)
                    writer.add_scalar('train/pg_loss', train_info['pg_loss'], timestep)
                    writer.add_scalar('train/value_loss', train_info['value_loss'], timestep)
                    writer.add_scalar('train/entropy_loss', train_info['entropy_loss'], timestep)
                    writer.add_scalar('train/mean_advantage', train_info['mean_advantage'], timestep)
                    writer.add_scalar('train/buffer_size', train_info['buffer_size'], timestep)
                    writer.add_scalar('train/learning_rate', train_info['learning_rate'], timestep)

            # Episodeç»“æŸå¤„ç†
            if done:
                # è®°å½•episodeä¿¡æ¯
                self.training_history['episode_rewards'].append(episode_reward)
                self.training_history['episode_lengths'].append(episode_length)

                # TensorBoardè®°å½•
                writer.add_scalar('train/episode_reward', episode_reward, episode)
                writer.add_scalar('train/episode_length', episode_length, episode)

                # è®¡ç®—æ»‘åŠ¨å¹³å‡
                if len(self.training_history['episode_rewards']) >= 100:
                    avg_reward = np.mean(self.training_history['episode_rewards'][-100:])
                    self.training_history['avg_rewards'].append(avg_reward)
                    writer.add_scalar('train/avg_reward_100', avg_reward, episode)

                # æ‰“å°è¿›åº¦
                if episode % 100 == 0:
                    elapsed_time = time.time() - start_time
                    recent_rewards = self.training_history['episode_rewards'][-100:] if len(self.training_history['episode_rewards']) >= 100 else self.training_history['episode_rewards']
                    avg_recent = np.mean(recent_rewards) if recent_rewards else 0

                    print(f"Episode {episode:5d} | "
                          f"Timestep {timestep:8d} | "
                          f"Reward: {episode_reward:8.2f} | "
                          f"Avg(100): {avg_recent:8.2f} | "
                          f"Length: {episode_length:4d} | "
                          f"Time: {elapsed_time:.1f}s")

                # é‡ç½®episode
                episode += 1
                episode_reward = 0.0
                episode_length = 0
                state, _ = self.env.reset()
            else:
                state = next_state

            # è¯„ä¼°
            if eval_freq > 0 and timestep % eval_freq == 0 and timestep > 0:
                eval_results = self.evaluate(n_episodes=5, deterministic=True, verbose=False)
                writer.add_scalar('eval/mean_reward', eval_results['mean_reward'], timestep)
                writer.add_scalar('eval/std_reward', eval_results['std_reward'], timestep)

                print(f"ğŸ“Š Evaluation at step {timestep}: "
                      f"Mean reward: {eval_results['mean_reward']:.2f} Â± {eval_results['std_reward']:.2f}")

            # ä¿å­˜æ¨¡å‹
            if save_freq > 0 and timestep % save_freq == 0 and timestep > 0:
                save_path = f"../../../../Models/impala_optimized_step_{timestep}.pt"
                os.makedirs(os.path.dirname(save_path), exist_ok=True)
                self.agent.save(save_path)
                print(f"ğŸ’¾ Model saved at step {timestep}: {save_path}")

        # è®­ç»ƒå®Œæˆ
        total_time = time.time() - start_time
        writer.close()

        print(f"âœ… Optimized IMPALA training completed!")
        print(f"   Total episodes: {episode}")
        print(f"   Total time: {total_time:.2f}s")
        final_avg = np.mean(self.training_history['episode_rewards'][-100:]) if len(self.training_history['episode_rewards']) >= 100 else np.mean(self.training_history['episode_rewards']) if self.training_history['episode_rewards'] else 0
        print(f"   Average reward (last 100): {final_avg:.2f}")

        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        final_save_path = "../../../../Models/impala_optimized_final.pt"
        os.makedirs(os.path.dirname(final_save_path), exist_ok=True)
        self.agent.save(final_save_path)

        return {
            'episodes': episode,
            'total_timesteps': timestep,
            'final_reward': final_avg,
            'training_time': total_time
        }

    def evaluate(self, n_episodes: int = 10, deterministic: bool = True, verbose: bool = True):
        """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
        if self.agent is None:
            raise ValueError("Agent not initialized. Please train first.")

        episode_rewards = []
        episode_lengths = []

        for episode in range(n_episodes):
            state, _ = self.env.reset()
            episode_reward = 0.0
            episode_length = 0
            done = False

            while not done:
                action = self.agent.act(state, training=False)

                try:
                    step_result = self.env.step(action)
                    if len(step_result) == 5:
                        next_state, reward, terminated, truncated, info = step_result
                        done = terminated or truncated
                    else:
                        next_state, reward, done, info = step_result
                except Exception as e:
                    print(f"âŒ Evaluation error: {e}")
                    break

                episode_reward += reward
                episode_length += 1
                state = next_state

                if episode_length >= 1000:
                    break

            episode_rewards.append(episode_reward)
            episode_lengths.append(episode_length)

            if verbose:
                print(f"  Episode {episode+1}/{n_episodes}: Reward = {episode_reward:.2f}, Length = {episode_length}")

        results = {
            'mean_reward': np.mean(episode_rewards),
            'std_reward': np.std(episode_rewards),
            'mean_length': np.mean(episode_lengths),
            'episode_rewards': episode_rewards,
            'episode_lengths': episode_lengths
        }

        if verbose:
            print(f"ğŸ“ˆ Optimized IMPALA Evaluation Results:")
            print(f"   Mean reward: {results['mean_reward']:.2f} Â± {results['std_reward']:.2f}")
            print(f"   Mean length: {results['mean_length']:.1f}")

        return results

    def save_results(self, path_prefix: str):
        """ä¿å­˜è®­ç»ƒç»“æœ"""
        os.makedirs(os.path.dirname(path_prefix) if os.path.dirname(path_prefix) else ".", exist_ok=True)

        import json
        with open(f"{path_prefix}_history.json", 'w') as f:
            serializable_history = {}
            for key, value in self.training_history.items():
                if isinstance(value, list):
                    serializable_history[key] = [float(x) if isinstance(x, (np.floating, np.integer)) else x for x in value]
                else:
                    serializable_history[key] = value
            json.dump(serializable_history, f, indent=2)

        print(f"ğŸ’¾ Optimized IMPALA results saved to: {path_prefix}")

    def save(self, path: str):
        """ä¿å­˜æ¨¡å‹"""
        if self.agent is None:
            raise ValueError("Agent not trained yet!")

        self.agent.save(path)
        print(f"ğŸ’¾ Optimized IMPALA model saved to: {path}")

    def load(self, path: str):
        """åŠ è½½æ¨¡å‹"""
        if self.env is None:
            self.setup_env()

        if self.agent is None:
            self.create_agent()

        self.agent.load(path)
        print(f"ğŸ“‚ Optimized IMPALA model loaded from: {path}")

        return self.agent


def test_optimized_impala():
    """æµ‹è¯•ä¼˜åŒ–çš„IMPALA"""
    print("ğŸ§ª Testing Optimized IMPALA...")

    baseline = OptimizedIMPALABaseline()

    # å¿«é€Ÿè®­ç»ƒæµ‹è¯•
    results = baseline.train(total_timesteps=5000)
    print(f"Training results: {results}")

    # è¯„ä¼°æµ‹è¯•
    eval_results = baseline.evaluate(n_episodes=3)
    print(f"Evaluation results: {eval_results}")

    baseline.save("../../../../Models/impala_optimized_test.pt")
    print("âœ… Optimized IMPALA test completed!")


if __name__ == "__main__":
    test_optimized_impala()