"""
IMPALA Agent Implementation
IMPALAæ™ºèƒ½ä½“å®ç°ï¼Œé›†æˆV-traceå’ŒActor-Criticæ¶æ„
"""

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from typing import Dict, Tuple, Optional, List, Any
import random
from collections import deque

from .networks import create_impala_network
from .replay_buffer import IMPALAReplayBuffer
from .vtrace import VTrace, compute_vtrace_loss


class IMPALAAgent:
    """IMPALAæ™ºèƒ½ä½“"""
    
    def __init__(self,
                 state_space,
                 action_space,
                 config: Dict = None):
        """
        åˆå§‹åŒ–IMPALAæ™ºèƒ½ä½“
        
        Args:
            state_space: çŠ¶æ€ç©ºé—´
            action_space: åŠ¨ä½œç©ºé—´
            config: é…ç½®å‚æ•°
        """
        
        # ä¼˜åŒ–é…ç½® - ä¿å®ˆV-traceç­–ç•¥é˜²æ­¢æ—©æœŸå´©æºƒ
        default_config = {
            # ç½‘ç»œé…ç½®
            'hidden_dim': 512,
            'num_layers': 2,

            # å­¦ä¹ å‚æ•° - è¿›ä¸€æ­¥é™ä½å­¦ä¹ ç‡é˜²æ­¢late-stageå´©æºƒ
            'learning_rate': 3e-5,      # ğŸ”§ ä¼˜åŒ–v2: 5e-5 â†’ 3e-5 (é˜²æ­¢150kæ­¥å´©æºƒ)
            'gamma': 0.99,
            'entropy_coeff': 0.01,
            'value_loss_coeff': 0.5,
            'gradient_clip': 20.0,      # ğŸ”§ ä¼˜åŒ–: 40.0 â†’ 20.0 (æ›´å¼ºæ¢¯åº¦è£å‰ª)

            # V-traceå‚æ•° - æåº¦ä¿å®ˆé¿å…é‡è¦æ€§é‡‡æ ·çˆ†ç‚¸
            'rho_bar': 0.7,             # ğŸ”§ ä¼˜åŒ–v2: 0.9 â†’ 0.7 (æ›´ä¿å®ˆçš„ISè£å‰ª)
            'c_bar': 0.7,               # ğŸ”§ ä¼˜åŒ–v2: 0.9 â†’ 0.7 (æ›´ä¿å®ˆçš„valueè£å‰ª)

            # å›æ”¾ç¼“å†²åŒº - å‡å°ç¼“å†²åŒºé™ä½ç­–ç•¥é™ˆæ—§æ€§
            'buffer_size': 30000,       # ğŸ”§ ä¼˜åŒ–v2: 50000 â†’ 30000 (å‡å°‘off-policyç¨‹åº¦)
            'sequence_length': 10,      # ğŸ”§ ä¼˜åŒ–: 20 â†’ 10 (ç¼©çŸ­åºåˆ—é•¿åº¦æé«˜ç¨³å®šæ€§)
            'batch_size': 32,           # ğŸ”§ ä¼˜åŒ–: 16 â†’ 32 (å¢åŠ æ‰¹æ¬¡å¤§å°)

            # è®­ç»ƒå‚æ•° - æ›´é¢‘ç¹æ›´æ–°ä½†å»¶åå¯åŠ¨
            'learning_starts': 2000,    # ğŸ”§ ä¼˜åŒ–: 1000 â†’ 2000 (å»¶åå­¦ä¹ ç§¯ç´¯æ›´å¤šç»éªŒ)
            'train_freq': 2,            # ğŸ”§ ä¼˜åŒ–: 4 â†’ 2 (æ›´é¢‘ç¹è®­ç»ƒ)
            'update_freq': 100,

            # å…¶ä»–
            'seed': 42,
            'device': 'auto'
        }
        
        if config:
            default_config.update(config)
        self.config = default_config
        
        # è®¾ç½®è®¾å¤‡
        if self.config['device'] == 'auto':
            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        else:
            self.device = torch.device(self.config['device'])
        
        # è®¾ç½®éšæœºç§å­
        if self.config['seed'] is not None:
            random.seed(self.config['seed'])
            np.random.seed(self.config['seed'])
            torch.manual_seed(self.config['seed'])
        
        self.state_space = state_space
        self.action_space = action_space
        
        # åˆ›å»ºç½‘ç»œ
        network_config = {
            'hidden_dim': self.config['hidden_dim'],
            'num_layers': self.config['num_layers']
        }
        
        self.network = create_impala_network(
            state_space, action_space, network_config
        ).to(self.device)
        
        # ä¼˜åŒ–å™¨
        self.optimizer = optim.Adam(
            self.network.parameters(),
            lr=self.config['learning_rate']
        )
        
        # ç»éªŒå›æ”¾ç¼“å†²åŒº
        self.replay_buffer = IMPALAReplayBuffer(
            capacity=self.config['buffer_size'],
            sequence_length=self.config['sequence_length'],
            device=self.device
        )
        
        # V-trace
        self.vtrace = VTrace(
            rho_bar=self.config['rho_bar'],
            c_bar=self.config['c_bar'],
            gamma=self.config['gamma']
        )
        
        # è®­ç»ƒç»Ÿè®¡
        self.training_step = 0
        self.episode_rewards = []
        self.losses = []
        
        # å½“å‰episodeçš„è¡Œä¸ºç­–ç•¥log_probsï¼ˆç”¨äºV-traceï¼‰
        self.behavior_log_probs = []
        
        print(f"ğŸ¯ IMPALA Agent initialized on {self.device}")
        print(f"   State space: {state_space.shape}")
        print(f"   Action space: {action_space.shape}")
        print(f"   Network: Actor-Critic with V-trace")
        print(f"   Sequence length: {self.config['sequence_length']}")
    
    def act(self, state: np.ndarray, training: bool = True) -> np.ndarray:
        """é€‰æ‹©åŠ¨ä½œ"""
        if not isinstance(state, np.ndarray):
            state = np.array(state)
        
        # è½¬æ¢ä¸ºtensor
        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)
        
        with torch.no_grad():
            # è·å–åŠ¨ä½œå’Œä»·å€¼
            action, log_prob, value = self.network.get_action_and_value(
                state_tensor, deterministic=not training
            )
            
            action = action.cpu().numpy()[0]
            log_prob = log_prob.cpu().numpy()[0]
            value = value.cpu().numpy()[0]
        
        # å­˜å‚¨è¡Œä¸ºç­–ç•¥çš„log_probç”¨äºV-trace
        if training:
            self.behavior_log_probs.append(log_prob[0])  # å–å‡ºæ ‡é‡å€¼
        
        # ç¡®ä¿åŠ¨ä½œåœ¨æœ‰æ•ˆèŒƒå›´å†…
        action = np.clip(action, self.action_space.low, self.action_space.high)
        
        return action
    
    def store_transition(self,
                        state: np.ndarray,
                        action: np.ndarray,
                        reward: float,
                        next_state: np.ndarray,
                        done: bool):
        """å­˜å‚¨è½¬æ¢åˆ°ç¼“å†²åŒº"""
        
        # è·å–å½“å‰çŠ¶æ€çš„ä»·å€¼ä¼°è®¡
        with torch.no_grad():
            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)
            _, _, value = self.network.get_action_and_value(state_tensor)
            value = value.cpu().numpy()[0, 0]
        
        # è·å–è¡Œä¸ºç­–ç•¥çš„log_prob
        behavior_log_prob = self.behavior_log_probs[-1] if self.behavior_log_probs else 0.0
        
        # å­˜å‚¨åˆ°å›æ”¾ç¼“å†²åŒº
        self.replay_buffer.add_step(
            state=state,
            action=action,
            reward=reward,
            next_state=next_state,
            done=done,
            log_prob=behavior_log_prob,
            value=value
        )
        
        # å¦‚æœepisodeç»“æŸï¼Œæ¸…ç©ºè¡Œä¸ºç­–ç•¥log_probs
        if done:
            self.behavior_log_probs.clear()
    
    def train(self) -> Optional[Dict]:
        """è®­ç»ƒä¸€æ­¥"""
        if not self.replay_buffer.is_ready:
            return None
        
        if self.training_step % self.config['train_freq'] != 0:
            self.training_step += 1
            return None
        
        # é‡‡æ ·åºåˆ—æ‰¹æ¬¡
        batch = self.replay_buffer.sample_sequences(self.config['batch_size'])
        if batch is None:
            self.training_step += 1
            return None
        
        # å¶å°”æ£€æŸ¥æ‰¹æ¬¡å½¢çŠ¶ï¼ˆå‡å°‘è¾“å‡ºé¢‘ç‡ï¼‰
        if self.training_step % 5000 == 0:
            print(f"Debug: Batch shapes - states: {batch['states'].shape}, actions: {batch['actions'].shape}")
        
        # è§£åŒ…æ‰¹æ¬¡æ•°æ®
        states = batch['states']  # [T, B, state_dim]
        actions = batch['actions']  # [T, B, action_dim]
        rewards = batch['rewards']  # [T, B]
        dones = batch['dones']  # [T, B]
        behavior_log_probs = batch['log_probs']  # [T, B]
        
        T, B = states.shape[:2]
        
        # å‰å‘ä¼ æ’­è·å–å½“å‰ç­–ç•¥çš„è¾“å‡º
        states_flat = states.reshape(-1, states.shape[-1])
        actions_flat = actions.reshape(-1, actions.shape[-1])
        
        target_log_probs_flat, values_flat, entropies_flat = self.network.evaluate_action(
            states_flat, actions_flat
        )
        
        # é‡æ–°æ•´å½¢
        target_log_probs = target_log_probs_flat.reshape(T, B)
        values = values_flat.reshape(T, B)
        entropies = entropies_flat.reshape(T, B)
        
        # è®¡ç®—bootstrapä»·å€¼ï¼ˆæœ€åä¸€ä¸ªçŠ¶æ€çš„ä»·å€¼ï¼‰
        with torch.no_grad():
            last_states = states[-1]  # [B, state_dim]
            _, _, bootstrap_values = self.network.get_action_and_value(last_states)
            bootstrap_values = bootstrap_values.squeeze(-1)  # [B]
        
        # è®¡ç®—V-traceæŸå¤±
        total_loss, loss_info = compute_vtrace_loss(
            self.vtrace,
            behavior_log_probs,
            target_log_probs,
            rewards,
            values,
            bootstrap_values,
            dones,
            entropies,
            entropy_coeff=self.config['entropy_coeff']
        )
        
        # åå‘ä¼ æ’­
        self.optimizer.zero_grad()
        total_loss.backward()
        
        # æ¢¯åº¦è£å‰ª
        torch.nn.utils.clip_grad_norm_(
            self.network.parameters(),
            self.config['gradient_clip']
        )
        
        self.optimizer.step()
        
        self.training_step += 1
        self.losses.append(loss_info['total_loss'])
        
        # è¿”å›è®­ç»ƒä¿¡æ¯
        return {
            'total_loss': loss_info['total_loss'],
            'pg_loss': loss_info['pg_loss'],
            'value_loss': loss_info['value_loss'],
            'entropy_loss': loss_info['entropy_loss'],
            'mean_advantage': loss_info['mean_advantage'],
            'mean_value': loss_info['mean_value'],
            'buffer_size': len(self.replay_buffer)
        }
    
    def save(self, filepath: str):
        """ä¿å­˜æ¨¡å‹"""
        torch.save({
            'network': self.network.state_dict(),
            'optimizer': self.optimizer.state_dict(),
            'config': self.config,
            'training_step': self.training_step
        }, filepath)
    
    def load(self, filepath: str):
        """åŠ è½½æ¨¡å‹"""
        checkpoint = torch.load(filepath, map_location=self.device)
        self.network.load_state_dict(checkpoint['network'])
        self.optimizer.load_state_dict(checkpoint['optimizer'])
        self.training_step = checkpoint['training_step']
        
        print(f"âœ… IMPALA model loaded from {filepath}")
    
    def get_stats(self) -> Dict:
        """è·å–è®­ç»ƒç»Ÿè®¡"""
        return {
            'training_step': self.training_step,
            'buffer_size': len(self.replay_buffer),
            'avg_loss': np.mean(self.losses[-100:]) if self.losses else 0,
            'episodes_trained': len(self.episode_rewards)
        }