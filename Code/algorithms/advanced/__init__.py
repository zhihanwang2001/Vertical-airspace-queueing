"""
Advanced DRL Algorithms Package
é«˜çº§æ·±åº¦å¼ºåŒ–å­¦ä¹ ç®—æ³•é›†åˆï¼Œç”¨äºCCF BåŒºè®ºæ–‡

åŒ…å«æœ€æ–°çš„DRLç®—æ³•å®ç°ï¼š
1. Rainbow DQN - æ•´åˆ6é¡¹DQNæ”¹è¿›çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•
2. IMPALA - åˆ†å¸ƒå¼é‡è¦æ€§åŠ æƒActor-Learneræ¶æ„
3. R2D2 - å¾ªç¯ç»éªŒå›æ”¾åˆ†å¸ƒå¼DQN
4. SAC v2 - è½¯æ¼”å‘˜-è¯„è®ºå®¶ç®—æ³•æ”¹è¿›ç‰ˆ
5. TD7 - TD3ç®—æ³•çš„è¿›ä¸€æ­¥æ”¹è¿›

æ‰€æœ‰ç®—æ³•éƒ½ï¼š
- é›†æˆTensorBoardç›‘æ§
- å…¼å®¹ç°æœ‰åŸºçº¿æ¡†æ¶
- æ”¯æŒå‚ç›´åˆ†å±‚é˜Ÿåˆ—ç¯å¢ƒ
- åŒ…å«å®Œæ•´çš„è®­ç»ƒå’Œè¯„ä¼°åŠŸèƒ½
"""

# Rainbow DQN
from .rainbow_dqn import RainbowDQNAgent, RainbowDQNBaseline

# IMPALA
from .impala import IMPALAAgent, IMPALABaseline
from .impala.impala_optimized import OptimizedIMPALABaseline

# R2D2
from .r2d2 import R2D2Agent, R2D2Baseline

# SAC v2
from .sac_v2 import SAC_v2_Agent, SAC_v2_Baseline

# TD7
from .td7 import TD7_Agent, TD7Baseline

__version__ = "1.0.0"
__all__ = [
    # Rainbow DQN
    "RainbowDQNAgent",
    "RainbowDQNBaseline",

    # IMPALA
    "IMPALAAgent",
    "IMPALABaseline",
    "OptimizedIMPALABaseline",

    # R2D2
    "R2D2Agent", "R2D2Baseline",

    # SAC v2
    "SAC_v2_Agent", "SAC_v2_Baseline",

    # TD7
    "TD7_Agent", "TD7Baseline",
]

# ç®—æ³•æ³¨å†Œè¡¨
AVAILABLE_ALGORITHMS = {
    "rainbow_dqn": {
        "name": "Rainbow DQN",
        "type": "value_based",
        "baseline_class": "RainbowDQNBaseline", 
        "description": "Deep Q-Network with 6 improvements: Double DQN, Prioritized Replay, Dueling Networks, Multi-step Learning, Distributional RL, Noisy Networks",
        "paper": "Rainbow: Combining Improvements in Deep Reinforcement Learning (Hessel et al., 2018)",
        "status": "implemented"
    },
    "impala": {
        "name": "IMPALA",
        "type": "actor_critic",
        "baseline_class": "IMPALABaseline",
        "description": "Importance Weighted Actor-Learner Architecture for distributed RL",
        "paper": "IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures (Espeholt et al., 2018)",
        "status": "implemented"
    },
    "impala_optimized": {
        "name": "IMPALA Optimized",
        "type": "actor_critic",
        "baseline_class": "OptimizedIMPALABaseline",
        "description": "Queue-specific optimized IMPALA with mixed action space support and conservative V-trace parameters",
        "paper": "IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures (Espeholt et al., 2018) + Queue-specific optimizations",
        "status": "implemented"
    },
    "r2d2": {
        "name": "R2D2",
        "type": "value_based",
        "baseline_class": "R2D2Baseline", 
        "description": "Recurrent Replay Distributed DQN for partially observable environments",
        "paper": "Recurrent Experience Replay in Distributed Reinforcement Learning (Kapturowski et al., 2019)",
        "status": "implemented"
    },
    "sac_v2": {
        "name": "SAC v2", 
        "type": "actor_critic",
        "baseline_class": "SAC_v2_Baseline",
        "description": "Soft Actor-Critic with automatic entropy tuning and other improvements",
        "paper": "Soft Actor-Critic Algorithms and Applications (Haarnoja et al., 2019)",
        "status": "implemented"
    },
    "td7": {
        "name": "TD7",
        "type": "actor_critic",
        "baseline_class": "TD7Baseline", 
        "description": "TD3 with additional improvements for better stability and performance",
        "paper": "TD7: A Regularized Actor-Critic Method for Robotic Reinforcement Learning (Fujimoto & Gu, 2021)",
        "status": "implemented"
    }
}


def get_available_algorithms():
    """è·å–å¯ç”¨ç®—æ³•åˆ—è¡¨"""
    return AVAILABLE_ALGORITHMS


def get_algorithm_info(algorithm_name: str):
    """è·å–ç®—æ³•è¯¦ç»†ä¿¡æ¯"""
    return AVAILABLE_ALGORITHMS.get(algorithm_name, None)


def create_algorithm_baseline(algorithm_name: str, config=None):
    """å·¥å‚å‡½æ•°ï¼šåˆ›å»ºç®—æ³•åŸºçº¿"""
    if algorithm_name == "rainbow_dqn":
        return RainbowDQNBaseline(config)
    elif algorithm_name == "impala":
        return IMPALABaseline(config)
    elif algorithm_name == "impala_optimized":
        return OptimizedIMPALABaseline(config)
    elif algorithm_name == "r2d2":
        return R2D2Baseline(config)
    elif algorithm_name == "sac_v2":
        return SAC_v2_Baseline(config)
    elif algorithm_name == "td7":
        return TD7Baseline(config)
    else:
        available = ", ".join(AVAILABLE_ALGORITHMS.keys())
        raise ValueError(f"Unknown algorithm: {algorithm_name}. Available: {available}")


def print_algorithms_status():
    """æ‰“å°æ‰€æœ‰ç®—æ³•çš„å®ç°çŠ¶æ€"""
    print("ğŸ¤– Advanced DRL Algorithms Status:")
    print("=" * 60)
    
    for name, info in AVAILABLE_ALGORITHMS.items():
        status_emoji = {
            "implemented": "âœ…",
            "in_progress": "ğŸš§", 
            "planned": "ğŸ“‹"
        }
        
        print(f"{status_emoji.get(info['status'], 'â“')} {info['name']}")
        print(f"   Type: {info['type']}")
        print(f"   Status: {info['status']}")
        print(f"   Paper: {info['paper']}")
        print()


if __name__ == "__main__":
    print_algorithms_status()