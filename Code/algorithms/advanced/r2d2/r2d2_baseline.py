"""
R2D2 Baseline for Vertical Stratified Queue System
æ•´åˆåˆ°ç°æœ‰åŸºçº¿ç®—æ³•æ¡†æ¶ä¸­çš„R2D2å®ç°
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

import numpy as np
import torch
from torch.utils.tensorboard import SummaryWriter
import time
from typing import Dict, Any, Optional

from env.drl_optimized_env_fixed import DRLOptimizedQueueEnvFixed
from baselines.space_utils import SB3DictWrapper
from .r2d2_agent import R2D2Agent


class R2D2Baseline:
    """R2D2åŸºçº¿ç®—æ³•ï¼Œå…¼å®¹ç°æœ‰æ¡†æ¶"""
    
    def __init__(self, config: Dict[str, Any] = None):
        """
        åˆå§‹åŒ–R2D2åŸºçº¿
        
        Args:
            config: é…ç½®å‚æ•°å­—å…¸
        """
        # é»˜è®¤é…ç½®
        default_config = {
            # ç½‘ç»œé…ç½® - æç®€æ¶æ„å¿«é€Ÿå­¦ä¹ 
            'hidden_dim': 128,  # æ›´å°ç½‘ç»œï¼Œé˜²è¿‡æ‹Ÿåˆï¼Œå¿«æ”¶æ•›
            'recurrent_dim': 64, # å°RNNï¼Œé€‚åˆçŸ­åºåˆ—
            'num_layers': 1,
            'recurrent_type': 'LSTM',
            'dueling': True,
            
            # å­¦ä¹ å‚æ•° - è¶…æ¿€è¿›å­¦ä¹ 
            'learning_rate': 1e-3,  # å¾ˆé«˜çš„å­¦ä¹ ç‡ï¼Œå¿«é€Ÿå­¦ä¹ 
            'gamma': 0.95,          # é™ä½æŠ˜æ‰£ï¼Œæ›´å…³æ³¨å³æ—¶å¥–åŠ±
            'target_update_freq': 500,   # éå¸¸é¢‘ç¹çš„ç›®æ ‡ç½‘ç»œæ›´æ–°
            'gradient_clip': 5.0,   # æ›´ä¸¥æ ¼æ¢¯åº¦è£å‰ªé˜²æ­¢çˆ†ç‚¸
            
            # DQNå‚æ•° - è¶…å¿«æ¢ç´¢åˆ°åˆ©ç”¨è½¬æ¢
            'epsilon_start': 0.9,   # é™ä½åˆå§‹æ¢ç´¢
            'epsilon_end': 0.1,     # ä¿æŒä¸€å®šæ¢ç´¢
            'epsilon_decay_steps': 10000,  # æ›´å¿«è¡°å‡
            'double_dqn': True,
            
            # åºåˆ—å›æ”¾é…ç½® - æåº¦ä¼˜åŒ–çŸ­episode
            'buffer_size': 500,    # æ›´å°ç¼“å†²åŒºï¼Œå¿«é€Ÿå‘¨è½¬
            'sequence_length': 3,   # è¶…çŸ­åºåˆ—ï¼ŒåŒ¹é…å®é™…episodeé•¿åº¦
            'burn_in_length': 1,    # æœ€å°burn-in
            'overlap_length': 1,
            'batch_size': 32,       # æ›´å¤§batchè¡¥å¿çŸ­åºåˆ—
            
            # è®­ç»ƒå‚æ•° - ç«‹å³å¼€å§‹å­¦ä¹ 
            'learning_starts': 500,   # æçŸ­ç­‰å¾…ï¼Œå¿«é€Ÿå¼€å§‹å­¦ä¹   
            'train_freq': 2,          # éå¸¸é¢‘ç¹è®­ç»ƒï¼Œæ¯2æ­¥ä¸€æ¬¡
            
            # åŠ¨ä½œç¦»æ•£åŒ– - å…³é”®ä¼˜åŒ–ï¼
            'action_bins': 2,  # å‡å°‘åˆ°2ï¼Œé™ä½åŠ¨ä½œç©ºé—´å¤æ‚åº¦
            
            # TensorBoardæ—¥å¿—
            'tensorboard_log': "./tensorboard_logs/",
            'verbose': 1,
            'seed': 42
        }
        
        if config:
            default_config.update(config)
        
        self.config = default_config
        self.agent = None
        self.env = None
        self.training_history = {
            'episode_rewards': [],
            'episode_lengths': [],
            'avg_rewards': [],
            'loss_values': [],
            'training_steps': []
        }
        
        print("ğŸ”„ R2D2 Baseline initialized")
    
    def setup_env(self):
        """è®¾ç½®ç¯å¢ƒ"""
        base_env = DRLOptimizedQueueEnvFixed()
        self.env = SB3DictWrapper(base_env)
        
        print(f"âœ… Environment setup completed")
        print(f"   Observation space: {self.env.observation_space}")
        print(f"   Action space: {self.env.action_space}")
        
        return self.env
    
    def create_agent(self):
        """åˆ›å»ºR2D2æ™ºèƒ½ä½“"""
        if self.env is None:
            self.setup_env()
        
        self.agent = R2D2Agent(
            state_space=self.env.observation_space,
            action_space=self.env.action_space,
            config=self.config
        )
        
        print("âœ… R2D2 Agent created successfully")
        return self.agent
    
    def train(self, total_timesteps: int, eval_freq: int = 10000, save_freq: int = 50000):
        """
        è®­ç»ƒR2D2æ¨¡å‹
        
        Args:
            total_timesteps: æ€»è®­ç»ƒæ­¥æ•°
            eval_freq: è¯„ä¼°é¢‘ç‡
            save_freq: ä¿å­˜é¢‘ç‡
            
        Returns:
            è®­ç»ƒå†å²å­—å…¸
        """
        if self.agent is None:
            self.create_agent()
        
        # åˆ›å»ºTensorBoard writer
        tb_log_name = f"R2D2_{int(time.time())}"
        writer = SummaryWriter(
            log_dir=os.path.join(self.config['tensorboard_log'], tb_log_name)
        )
        
        print(f"ğŸš€ Starting R2D2 training for {total_timesteps:,} timesteps...")
        print(f"   TensorBoard log: {tb_log_name}")
        
        # è®­ç»ƒå˜é‡
        episode = 0
        timestep = 0
        episode_reward = 0.0
        episode_length = 0
        
        # é‡ç½®ç¯å¢ƒ
        state, _ = self.env.reset()
        
        start_time = time.time()
        
        while timestep < total_timesteps:
            # é€‰æ‹©åŠ¨ä½œ
            action = self.agent.act(state, training=True)
            
            # æ‰§è¡ŒåŠ¨ä½œ
            try:
                step_result = self.env.step(action)
                if len(step_result) == 5:  # Gymnasiumæ ¼å¼
                    next_state, reward, terminated, truncated, info = step_result
                    done = terminated or truncated
                else:  # Gymæ ¼å¼
                    next_state, reward, done, info = step_result
            except Exception as e:
                print(f"âŒ Environment step error: {e}")
                break
            
            # å­˜å‚¨ç»éªŒ
            self.agent.store_transition(state, action, reward, next_state, done)
            
            # æ›´æ–°ç»Ÿè®¡
            episode_reward += reward
            episode_length += 1
            timestep += 1
            
            # è®­ç»ƒæ™ºèƒ½ä½“
            if timestep >= self.config['learning_starts']:
                train_info = self.agent.train()
                
                if train_info and timestep % 1000 == 0:
                    # è®°å½•è®­ç»ƒä¿¡æ¯
                    writer.add_scalar('train/loss', train_info['loss'], timestep)
                    writer.add_scalar('train/epsilon', train_info['epsilon'], timestep)
                    writer.add_scalar('train/buffer_size', train_info['buffer_size'], timestep)
                    writer.add_scalar('train/avg_q_value', train_info['avg_q_value'], timestep)
                    writer.add_scalar('train/valid_steps', train_info['valid_steps'], timestep)
            
            # æ£€æŸ¥æ˜¯å¦episodeç»“æŸ
            if done:
                # è®°å½•episodeä¿¡æ¯
                self.training_history['episode_rewards'].append(episode_reward)
                self.training_history['episode_lengths'].append(episode_length)
                
                # TensorBoardè®°å½•
                writer.add_scalar('train/episode_reward', episode_reward, episode)
                writer.add_scalar('train/episode_length', episode_length, episode)
                
                # è®¡ç®—æ»‘åŠ¨å¹³å‡
                if len(self.training_history['episode_rewards']) >= 100:
                    avg_reward = np.mean(self.training_history['episode_rewards'][-100:])
                    self.training_history['avg_rewards'].append(avg_reward)
                    writer.add_scalar('train/avg_reward_100', avg_reward, episode)
                
                # æ‰“å°è¿›åº¦
                if self.config['verbose'] and episode % 100 == 0:
                    elapsed_time = time.time() - start_time
                    print(f"Episode {episode:5d} | "
                          f"Timestep {timestep:8d} | "
                          f"Reward: {episode_reward:8.2f} | "
                          f"Length: {episode_length:4d} | "
                          f"Epsilon: {self.agent.get_epsilon():.3f} | "
                          f"Time: {elapsed_time:.1f}s")
                
                # é‡ç½®episode
                episode += 1
                episode_reward = 0.0
                episode_length = 0
                state, _ = self.env.reset()
                # R2D2ä¼šåœ¨done=Trueæ—¶è‡ªåŠ¨é‡ç½®éšè—çŠ¶æ€
            else:
                state = next_state
            
            # è¯„ä¼°
            if eval_freq > 0 and timestep % eval_freq == 0 and timestep > 0:
                eval_results = self.evaluate(n_episodes=5, deterministic=True, verbose=False)
                writer.add_scalar('eval/mean_reward', eval_results['mean_reward'], timestep)
                writer.add_scalar('eval/std_reward', eval_results['std_reward'], timestep)
                
                print(f"ğŸ“Š Evaluation at step {timestep}: "
                      f"Mean reward: {eval_results['mean_reward']:.2f} Â± {eval_results['std_reward']:.2f}")
            
            # ä¿å­˜æ¨¡å‹
            if save_freq > 0 and timestep % save_freq == 0 and timestep > 0:
                save_path = f"../../../../Models/r2d2_step_{timestep}.pt"
                os.makedirs(os.path.dirname(save_path), exist_ok=True)
                self.agent.save(save_path)
                print(f"ğŸ’¾ Model saved at step {timestep}: {save_path}")
        
        # è®­ç»ƒå®Œæˆ
        total_time = time.time() - start_time
        writer.close()
        
        print(f"âœ… R2D2 training completed!")
        print(f"   Total episodes: {episode}")
        print(f"   Total time: {total_time:.2f}s")
        print(f"   Average reward (last 100): {np.mean(self.training_history['episode_rewards'][-100:]):.2f}")
        
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        final_save_path = "../../../../Models/r2d2_final.pt"
        os.makedirs(os.path.dirname(final_save_path), exist_ok=True)
        self.agent.save(final_save_path)
        
        return {
            'episodes': episode,
            'total_timesteps': timestep,
            'final_reward': np.mean(self.training_history['episode_rewards'][-10:]) if self.training_history['episode_rewards'] else 0,
            'training_time': total_time
        }
    
    def evaluate(self, n_episodes: int = 10, deterministic: bool = True, verbose: bool = True):
        """
        è¯„ä¼°æ¨¡å‹æ€§èƒ½
        
        Args:
            n_episodes: è¯„ä¼°episodeæ•°é‡
            deterministic: æ˜¯å¦ä½¿ç”¨ç¡®å®šæ€§ç­–ç•¥
            verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯
            
        Returns:
            è¯„ä¼°ç»“æœå­—å…¸
        """
        if self.agent is None:
            raise ValueError("Agent not initialized. Please train first.")
        
        episode_rewards = []
        episode_lengths = []
        
        for episode in range(n_episodes):
            state, _ = self.env.reset()
            self.agent.reset_hidden_state()  # é‡ç½®RNNçŠ¶æ€
            
            episode_reward = 0.0
            episode_length = 0
            done = False
            
            while not done:
                action = self.agent.act(state, training=False)
                
                try:
                    step_result = self.env.step(action)
                    if len(step_result) == 5:
                        next_state, reward, terminated, truncated, info = step_result
                        done = terminated or truncated
                    else:
                        next_state, reward, done, info = step_result
                except Exception as e:
                    print(f"âŒ Evaluation error: {e}")
                    break
                
                episode_reward += reward
                episode_length += 1
                state = next_state
                
                # é˜²æ­¢æ— é™å¾ªç¯
                if episode_length >= 1000:
                    break
            
            episode_rewards.append(episode_reward)
            episode_lengths.append(episode_length)
            
            if verbose:
                print(f"  Episode {episode+1}/{n_episodes}: Reward = {episode_reward:.2f}, Length = {episode_length}")
        
        results = {
            'mean_reward': np.mean(episode_rewards),
            'std_reward': np.std(episode_rewards),
            'mean_length': np.mean(episode_lengths),
            'episode_rewards': episode_rewards,
            'episode_lengths': episode_lengths,
            'system_metrics': []  # R2D2ç‰¹å®šæŒ‡æ ‡å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ 
        }
        
        if verbose:
            print(f"ğŸ“ˆ R2D2 Evaluation Results:")
            print(f"   Mean reward: {results['mean_reward']:.2f} Â± {results['std_reward']:.2f}")
            print(f"   Mean length: {results['mean_length']:.1f}")
        
        return results
    
    def save_results(self, path_prefix: str):
        """ä¿å­˜è®­ç»ƒç»“æœ"""
        os.makedirs(os.path.dirname(path_prefix) if os.path.dirname(path_prefix) else ".", exist_ok=True)
        
        # ä¿å­˜è®­ç»ƒå†å²
        import json
        with open(f"{path_prefix}_history.json", 'w') as f:
            # è½¬æ¢numpyç±»å‹ä¸ºPythonåŸç”Ÿç±»å‹
            serializable_history = {}
            for key, value in self.training_history.items():
                if isinstance(value, list):
                    serializable_history[key] = [float(x) if isinstance(x, (np.floating, np.integer)) else x for x in value]
                else:
                    serializable_history[key] = value
            
            json.dump(serializable_history, f, indent=2)
        
        print(f"ğŸ’¾ R2D2 results saved to: {path_prefix}")
    
    def save(self, path: str):
        """ä¿å­˜æ¨¡å‹"""
        if self.agent is None:
            raise ValueError("Agent not trained yet!")
        
        self.agent.save(path)
        print(f"ğŸ’¾ R2D2 model saved to: {path}")
    
    def load(self, path: str):
        """åŠ è½½æ¨¡å‹"""
        if self.env is None:
            self.setup_env()
        
        if self.agent is None:
            self.create_agent()
        
        self.agent.load(path)
        print(f"ğŸ“‚ R2D2 model loaded from: {path}")
        
        return self.agent


def test_r2d2():
    """æµ‹è¯•R2D2"""
    print("ğŸ§ª Testing R2D2...")
    
    # åˆ›å»ºåŸºçº¿
    baseline = R2D2Baseline()
    
    # å¿«é€Ÿè®­ç»ƒæµ‹è¯•
    results = baseline.train(total_timesteps=5000)
    print(f"Training results: {results}")
    
    # è¯„ä¼°æµ‹è¯•
    eval_results = baseline.evaluate(n_episodes=3)
    print(f"Evaluation results: {eval_results}")
    
    # ä¿å­˜æµ‹è¯•
    baseline.save("../../../../Models/r2d2_test.pt")
    
    print("âœ… R2D2 test completed!")


if __name__ == "__main__":
    test_r2d2()