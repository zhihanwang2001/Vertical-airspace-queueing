"""
R2D2 Agent Implementation
R2D2æ™ºèƒ½ä½“å®ç°ï¼Œé›†æˆå¾ªç¯ç½‘ç»œå’Œåºåˆ—ç»éªŒå›æ”¾
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import numpy as np
from typing import Dict, Tuple, Optional, List, Any
import random
import copy
from collections import deque

from .networks import create_r2d2_network
from .sequence_replay import R2D2SequenceReplayBuffer


class R2D2Agent:
    """R2D2æ™ºèƒ½ä½“"""
    
    def __init__(self,
                 state_space,
                 action_space,
                 config: Dict = None):
        """
        åˆå§‹åŒ–R2D2æ™ºèƒ½ä½“
        
        Args:
            state_space: çŠ¶æ€ç©ºé—´
            action_space: åŠ¨ä½œç©ºé—´
            config: é…ç½®å‚æ•°
        """
        
        # é»˜è®¤é…ç½®
        default_config = {
            # ç½‘ç»œé…ç½®
            'hidden_dim': 512,
            'recurrent_dim': 256,
            'num_layers': 1,
            'recurrent_type': 'LSTM',
            'dueling': True,
            
            # å­¦ä¹ å‚æ•°
            'learning_rate': 1e-4,
            'gamma': 0.99,
            'target_update_freq': 2500,
            'gradient_clip': 40.0,
            
            # DQNå‚æ•°
            'epsilon_start': 1.0,
            'epsilon_end': 0.01,
            'epsilon_decay_steps': 250000,
            'double_dqn': True,
            
            # åºåˆ—å›æ”¾é…ç½®
            'buffer_size': 5000,
            'sequence_length': 40,
            'burn_in_length': 20,
            'overlap_length': 10,
            'batch_size': 16,
            
            # è®­ç»ƒå‚æ•°
            'learning_starts': 5000,
            'train_freq': 4,
            
            # åŠ¨ä½œç¦»æ•£åŒ–ï¼ˆè¿ç»­åŠ¨ä½œç©ºé—´ï¼‰
            'action_bins': 3,
            
            # å…¶ä»–
            'seed': 42,
            'device': 'auto'
        }
        
        if config:
            default_config.update(config)
        self.config = default_config
        
        # è®¾ç½®è®¾å¤‡
        if self.config['device'] == 'auto':
            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        else:
            self.device = torch.device(self.config['device'])
        
        # è®¾ç½®éšæœºç§å­
        if self.config['seed'] is not None:
            random.seed(self.config['seed'])
            np.random.seed(self.config['seed'])
            torch.manual_seed(self.config['seed'])
        
        self.state_space = state_space
        self.action_space = action_space
        
        # å¤„ç†åŠ¨ä½œç©ºé—´
        self._setup_action_space()
        
        # åˆ›å»ºç½‘ç»œ
        network_config = {
            'hidden_dim': self.config['hidden_dim'],
            'recurrent_dim': self.config['recurrent_dim'],
            'num_layers': self.config['num_layers'],
            'recurrent_type': self.config['recurrent_type'],
            'dueling': self.config['dueling'],
            'action_bins': self.config['action_bins']
        }
        
        self.q_network = create_r2d2_network(
            state_space, action_space, network_config
        ).to(self.device)
        
        self.target_network = create_r2d2_network(
            state_space, action_space, network_config  
        ).to(self.device)
        
        # åŒæ­¥ç›®æ ‡ç½‘ç»œ
        self.update_target_network()
        
        # ä¼˜åŒ–å™¨
        self.optimizer = optim.Adam(
            self.q_network.parameters(),
            lr=self.config['learning_rate']
        )
        
        # åºåˆ—ç»éªŒå›æ”¾ç¼“å†²åŒº
        self.replay_buffer = R2D2SequenceReplayBuffer(
            capacity=self.config['buffer_size'],
            sequence_length=self.config['sequence_length'],
            burn_in_length=self.config['burn_in_length'],
            overlap_length=self.config['overlap_length'],
            device=self.device
        )
        
        # è®­ç»ƒç»Ÿè®¡
        self.training_step = 0
        self.episode_rewards = []
        self.losses = []
        
        # RNNçŠ¶æ€ç®¡ç†
        self.current_hidden_state = None
        self.reset_hidden_state()
        
        print(f"ğŸ”„ R2D2 Agent initialized on {self.device}")
        print(f"   State space: {state_space.shape}")
        if self.action_type == 'discrete':
            print(f"   Action space: {self.num_actions} (discrete)")
        else:
            print(f"   Action space: {self.action_dim}D -> {self.num_actions} discrete")
        print(f"   Recurrent: {self.config['recurrent_type']} ({self.config['recurrent_dim']} units)")
        print(f"   Sequence length: {self.config['sequence_length']} + {self.config['burn_in_length']} burn-in")
    
    def _setup_action_space(self):
        """è®¾ç½®åŠ¨ä½œç©ºé—´"""
        if hasattr(self.action_space, 'n'):
            # ç¦»æ•£åŠ¨ä½œç©ºé—´
            self.num_actions = self.action_space.n
            self.action_type = 'discrete'
            self.action_dim = None
        else:
            # è¿ç»­åŠ¨ä½œç©ºé—´ï¼Œéœ€è¦ç¦»æ•£åŒ–
            self.action_dim = self.action_space.shape[0]
            self.action_low = self.action_space.low
            self.action_high = self.action_space.high
            self.action_bins = self.config['action_bins']
            self.num_actions = self.action_bins ** self.action_dim
            self.action_type = 'continuous'
            
            # åˆ›å»ºåŠ¨ä½œæ˜ å°„
            self._create_action_mapping()
    
    def _create_action_mapping(self):
        """ä¸ºè¿ç»­åŠ¨ä½œç©ºé—´åˆ›å»ºæ™ºèƒ½ç¦»æ•£åŒ–æ˜ å°„"""
        if self.action_type == 'discrete':
            return
        
        # æ™ºèƒ½ç¦»æ•£åŒ–ï¼šåªä½¿ç”¨å…³é”®åŠ¨ä½œå€¼
        # å¯¹äºå¤§å¤šæ•°æ§åˆ¶ä»»åŠ¡ï¼Œ{-1, 0, 1} æˆ–è€… {-0.5, 0, 0.5} å°±å¤Ÿç”¨äº†
        self.action_grids = []
        for i in range(self.action_dim):
            if self.action_bins == 2:
                # äºŒè¿›åˆ¶æ§åˆ¶ï¼šåªæœ‰è´Ÿå€¼å’Œæ­£å€¼
                grid = np.array([self.action_low[i], self.action_high[i]])
            elif self.action_bins == 3:
                # ä¸‰å€¼æ§åˆ¶ï¼šè´Ÿå€¼ã€é›¶ã€æ­£å€¼
                grid = np.array([self.action_low[i], 0.0, self.action_high[i]])
            else:
                # ä¿æŒåŸæ¥çš„çº¿æ€§åˆ†å¸ƒ
                grid = np.linspace(self.action_low[i], self.action_high[i], self.action_bins)
            self.action_grids.append(grid)
            
        print(f"ğŸ¯ R2D2 Action discretization: {self.action_bins}^{self.action_dim} = {self.num_actions} actions")
        print(f"   First dimension grid: {self.action_grids[0]}")
    
    def _discrete_to_continuous_action(self, discrete_action: int) -> np.ndarray:
        """å°†ç¦»æ•£åŠ¨ä½œè½¬æ¢ä¸ºè¿ç»­åŠ¨ä½œ"""
        if self.action_type == 'discrete':
            return discrete_action
        
        continuous_action = np.zeros(self.action_dim)
        remaining = discrete_action
        
        for i in range(self.action_dim):
            idx = remaining % self.action_bins
            continuous_action[i] = self.action_grids[i][idx]
            remaining //= self.action_bins
        
        return continuous_action
    
    def reset_hidden_state(self):
        """é‡ç½®RNNéšè—çŠ¶æ€"""
        self.current_hidden_state = self.q_network.init_hidden_state(1, self.device)
    
    def get_epsilon(self) -> float:
        """è·å–å½“å‰epsilonå€¼"""
        if self.training_step < self.config['epsilon_decay_steps']:
            epsilon = self.config['epsilon_start'] - (
                self.config['epsilon_start'] - self.config['epsilon_end']
            ) * self.training_step / self.config['epsilon_decay_steps']
        else:
            epsilon = self.config['epsilon_end']
        return epsilon
    
    def act(self, state: np.ndarray, training: bool = True) -> np.ndarray:
        """é€‰æ‹©åŠ¨ä½œ"""
        if not isinstance(state, np.ndarray):
            state = np.array(state)
        
        # è½¬æ¢ä¸ºtensorå¹¶æ·»åŠ æ‰¹æ¬¡ç»´åº¦å’Œåºåˆ—ç»´åº¦
        state_tensor = torch.FloatTensor(state).unsqueeze(0).unsqueeze(0).to(self.device)
        
        # epsilon-greedyç­–ç•¥
        if training and random.random() < self.get_epsilon():
            # éšæœºåŠ¨ä½œ
            discrete_action = random.randint(0, self.num_actions - 1)
            # ä»éœ€è¦æ›´æ–°éšè—çŠ¶æ€
            with torch.no_grad():
                _, self.current_hidden_state = self.q_network(state_tensor, self.current_hidden_state)
        else:
            # è´ªå¿ƒåŠ¨ä½œ
            with torch.no_grad():
                q_values, self.current_hidden_state = self.q_network(state_tensor, self.current_hidden_state)
                discrete_action = q_values.squeeze(0).squeeze(0).argmax().item()
        
        # è½¬æ¢ä¸ºç¯å¢ƒæ‰€éœ€çš„åŠ¨ä½œæ ¼å¼
        if self.action_type == 'continuous':
            return self._discrete_to_continuous_action(discrete_action)
        else:
            return discrete_action
    
    def store_transition(self,
                        state: np.ndarray,
                        action,
                        reward: float,
                        next_state: np.ndarray,
                        done: bool):
        """å­˜å‚¨è½¬æ¢åˆ°åºåˆ—ç¼“å†²åŒº"""
        
        # å¦‚æœæ˜¯è¿ç»­åŠ¨ä½œï¼Œè½¬æ¢ä¸ºç¦»æ•£åŠ¨ä½œç´¢å¼•
        if self.action_type == 'continuous':
            discrete_action = self._continuous_to_discrete_action(action)
        else:
            discrete_action = action
        
        # å­˜å‚¨åˆ°åºåˆ—å›æ”¾ç¼“å†²åŒº
        self.replay_buffer.add_step(
            state=state,
            action=discrete_action,
            reward=reward,
            done=done,
            hidden_state=copy.deepcopy(self.current_hidden_state) if self.current_hidden_state else None
        )
        
        # å¦‚æœepisodeç»“æŸï¼Œé‡ç½®éšè—çŠ¶æ€
        if done:
            self.reset_hidden_state()
    
    def _continuous_to_discrete_action(self, action: np.ndarray) -> int:
        """å°†è¿ç»­åŠ¨ä½œè½¬æ¢ä¸ºç¦»æ•£åŠ¨ä½œç´¢å¼•"""
        discrete_action = 0
        multiplier = 1
        
        for i in range(self.action_dim):
            closest_idx = np.argmin(np.abs(self.action_grids[i] - action[i]))
            discrete_action += closest_idx * multiplier
            multiplier *= self.action_bins
        
        return discrete_action
    
    def train(self) -> Optional[Dict]:
        """è®­ç»ƒä¸€æ­¥"""
        if not self.replay_buffer.is_ready:
            return None
        
        if self.training_step % self.config['train_freq'] != 0:
            self.training_step += 1
            return None
        
        # é‡‡æ ·åºåˆ—æ‰¹æ¬¡
        batch = self.replay_buffer.sample_sequences(self.config['batch_size'])
        if batch is None:
            self.training_step += 1
            return None
        
        # è§£åŒ…æ‰¹æ¬¡æ•°æ®
        states = batch['states']  # [batch_size, seq_len, state_dim]
        actions = batch['actions'].long()  # [batch_size, seq_len]
        rewards = batch['rewards']  # [batch_size, seq_len]
        dones = batch['dones']  # [batch_size, seq_len]
        burn_in_states = batch['burn_in_states']  # [batch_size, burn_in_len, state_dim]
        sequence_lengths = batch['sequence_lengths']  # [batch_size]
        
        batch_size, seq_len = states.shape[:2]
        
        # Burn-iné˜¶æ®µï¼šé¢„çƒ­RNNéšè—çŠ¶æ€
        burn_in_hidden_states = []
        for i in range(batch_size):
            hidden_state = self.q_network.init_hidden_state(1, self.device)
            
            # å¦‚æœæœ‰burn-inæ•°æ®ï¼Œè¿›è¡Œé¢„çƒ­
            if burn_in_states.shape[1] > 0:
                burn_in_seq = burn_in_states[i:i+1]  # [1, burn_in_len, state_dim]
                with torch.no_grad():
                    _, hidden_state = self.q_network(burn_in_seq, hidden_state)
            
            burn_in_hidden_states.append(hidden_state)
        
        # åˆå¹¶éšè—çŠ¶æ€ä¸ºæ‰¹æ¬¡æ ¼å¼
        if self.config['recurrent_type'].upper() == 'LSTM':
            h_states = torch.cat([h for h, c in burn_in_hidden_states], dim=1)
            c_states = torch.cat([c for h, c in burn_in_hidden_states], dim=1)
            batch_hidden_state = (h_states, c_states)
        else:  # GRU
            h_states = torch.cat([h for h in burn_in_hidden_states], dim=1)
            batch_hidden_state = (h_states,)
        
        # å‰å‘ä¼ æ’­è®¡ç®—å½“å‰Qå€¼
        current_q_values, _ = self.q_network(states, batch_hidden_state)
        current_q_values = current_q_values.gather(2, actions.unsqueeze(2)).squeeze(2)
        
        # è®¡ç®—ç›®æ ‡Qå€¼
        with torch.no_grad():
            if self.config['double_dqn']:
                # Double DQNï¼šç”¨ä¸»ç½‘ç»œé€‰æ‹©åŠ¨ä½œï¼Œç”¨ç›®æ ‡ç½‘ç»œè¯„ä¼°
                next_q_values_main, _ = self.q_network(states, batch_hidden_state)
                next_actions = next_q_values_main.argmax(2)
                
                next_q_values_target, _ = self.target_network(states, batch_hidden_state)
                next_q_values = next_q_values_target.gather(2, next_actions.unsqueeze(2)).squeeze(2)
            else:
                # æ™®é€šDQN
                next_q_values_target, _ = self.target_network(states, batch_hidden_state)
                next_q_values = next_q_values_target.max(2)[0]
            
            # è®¡ç®—ç›®æ ‡å€¼
            target_q_values = rewards + self.config['gamma'] * next_q_values * (1 - dones)
        
        # è®¡ç®—æŸå¤±ï¼ˆåªå¯¹æœ‰æ•ˆæ—¶é—´æ­¥è®¡ç®—ï¼‰
        loss = 0
        valid_steps = 0
        
        for i in range(batch_size):
            seq_len_i = min(sequence_lengths[i].item(), seq_len)
            if seq_len_i > 1:  # è‡³å°‘éœ€è¦2ä¸ªæ—¶é—´æ­¥æ¥è®¡ç®—TDè¯¯å·®
                loss += F.mse_loss(
                    current_q_values[i, :seq_len_i-1],
                    target_q_values[i, 1:seq_len_i]
                )
                valid_steps += seq_len_i - 1
        
        if valid_steps > 0:
            loss = loss / batch_size
        else:
            self.training_step += 1
            return None
        
        # åå‘ä¼ æ’­
        self.optimizer.zero_grad()
        loss.backward()
        
        # æ¢¯åº¦è£å‰ª
        torch.nn.utils.clip_grad_norm_(
            self.q_network.parameters(),
            self.config['gradient_clip']
        )
        
        self.optimizer.step()
        
        # æ›´æ–°ç›®æ ‡ç½‘ç»œ
        if self.training_step % self.config['target_update_freq'] == 0:
            self.update_target_network()
        
        self.training_step += 1
        self.losses.append(loss.item())
        
        # è¿”å›è®­ç»ƒä¿¡æ¯
        return {
            'loss': loss.item(),
            'epsilon': self.get_epsilon(),
            'buffer_size': len(self.replay_buffer),
            'valid_steps': valid_steps,
            'avg_q_value': current_q_values.mean().item()
        }
    
    def update_target_network(self):
        """æ›´æ–°ç›®æ ‡ç½‘ç»œ"""
        self.target_network.load_state_dict(self.q_network.state_dict())
    
    def save(self, filepath: str):
        """ä¿å­˜æ¨¡å‹"""
        torch.save({
            'q_network': self.q_network.state_dict(),
            'target_network': self.target_network.state_dict(),
            'optimizer': self.optimizer.state_dict(),
            'config': self.config,
            'training_step': self.training_step
        }, filepath)
    
    def load(self, filepath: str):
        """åŠ è½½æ¨¡å‹"""
        checkpoint = torch.load(filepath, map_location=self.device)
        self.q_network.load_state_dict(checkpoint['q_network'])
        self.target_network.load_state_dict(checkpoint['target_network'])
        self.optimizer.load_state_dict(checkpoint['optimizer'])
        self.training_step = checkpoint['training_step']
        
        print(f"âœ… R2D2 model loaded from {filepath}")
    
    def get_stats(self) -> Dict:
        """è·å–è®­ç»ƒç»Ÿè®¡"""
        buffer_stats = self.replay_buffer.get_stats()
        
        return {
            'training_step': self.training_step,
            'epsilon': self.get_epsilon(),
            'buffer_size': len(self.replay_buffer),
            'avg_loss': np.mean(self.losses[-100:]) if self.losses else 0,
            'episodes_trained': len(self.episode_rewards),
            **buffer_stats
        }