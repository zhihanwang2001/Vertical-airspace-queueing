"""
TD7 Agent Implementation
TD7æ™ºèƒ½ä½“å®ç°ï¼Œé›†æˆSALEè¡¨ç¤ºå­¦ä¹ ã€ä¼˜å…ˆçº§å›æ”¾å’Œæ£€æŸ¥ç‚¹æœºåˆ¶
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import numpy as np
from typing import Dict, Tuple, Optional, List, Any
import random
import copy

from .networks import create_td7_networks
from .replay_buffer import TD7_PrioritizedReplayBuffer


class TD7_Agent:
    """TD7æ™ºèƒ½ä½“"""
    
    def __init__(self,
                 state_space,
                 action_space,
                 config: Dict = None):
        """
        åˆå§‹åŒ–TD7æ™ºèƒ½ä½“
        
        Args:
            state_space: çŠ¶æ€ç©ºé—´
            action_space: åŠ¨ä½œç©ºé—´
            config: é…ç½®å‚æ•°
        """
        
        # é»˜è®¤é…ç½®
        default_config = {
            # ç½‘ç»œé…ç½®
            'embedding_dim': 256,
            'hidden_dim': 256,
            'max_action': 1.0,
            
            # å­¦ä¹ å‚æ•°
            'actor_lr': 3e-4,
            'critic_lr': 3e-4,
            'encoder_lr': 3e-4,
            'gamma': 0.99,
            'tau': 0.005,

            # ğŸ”§ å­¦ä¹ ç‡è°ƒåº¦ï¼ˆé˜²æ­¢åæœŸå´©æºƒï¼‰
            'use_lr_schedule': True,        # æ˜¯å¦ä½¿ç”¨å­¦ä¹ ç‡è°ƒåº¦
            'warmup_steps': 75000,          # å‰75kæ­¥ä¿æŒå›ºå®šlrï¼ˆç¨³å®šæœŸï¼‰
            'total_steps': 500000,          # æ€»è®­ç»ƒæ­¥æ•°
            'min_lr_ratio': 0.1,            # æœ€å°å­¦ä¹ ç‡æ¯”ä¾‹ï¼ˆfinal_lr = initial_lr * 0.1ï¼‰
            
            # TD3ç‰¹å®šå‚æ•°
            'policy_delay': 2,      # å»¶è¿Ÿç­–ç•¥æ›´æ–°
            'target_noise': 0.2,    # ç›®æ ‡å¹³æ»‘å™ªå£°
            'noise_clip': 0.5,      # å™ªå£°è£å‰ª
            'exploration_noise': 0.1, # æ¢ç´¢å™ªå£°
            
            # ä¼˜å…ˆçº§å›æ”¾
            'buffer_size': 1000000,
            'batch_size': 256,
            'alpha': 0.6,           # ä¼˜å…ˆçº§æŒ‡æ•°
            'beta': 0.4,            # é‡è¦æ€§é‡‡æ ·æŒ‡æ•°
            'beta_increment': 0.001,
            
            # SALEç‰¹å®šå‚æ•°
            'embedding_loss_weight': 1.0,  # åµŒå…¥æŸå¤±æƒé‡
            'embedding_update_freq': 1,     # ç¼–ç å™¨æ›´æ–°é¢‘ç‡
            
            # æ£€æŸ¥ç‚¹æœºåˆ¶
            'use_checkpoints': True,
            'checkpoint_freq': 10000,       # æ£€æŸ¥ç‚¹ä¿å­˜é¢‘ç‡
            'max_checkpoints': 5,           # æœ€å¤§æ£€æŸ¥ç‚¹æ•°é‡
            
            # è®­ç»ƒå‚æ•°
            'learning_starts': 25000,
            'train_freq': 1,
            
            # å…¶ä»–
            'seed': 42,
            'device': 'auto'
        }
        
        if config:
            default_config.update(config)
        self.config = default_config
        
        # è®¾ç½®è®¾å¤‡
        if self.config['device'] == 'auto':
            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        else:
            self.device = torch.device(self.config['device'])
        
        # è®¾ç½®éšæœºç§å­
        if self.config['seed'] is not None:
            random.seed(self.config['seed'])
            np.random.seed(self.config['seed'])
            torch.manual_seed(self.config['seed'])
        
        self.state_space = state_space
        self.action_space = action_space
        
        # åˆ›å»ºç½‘ç»œ
        network_config = {
            'embedding_dim': self.config['embedding_dim'],
            'hidden_dim': self.config['hidden_dim'],
            'max_action': self.config['max_action']
        }
        
        self.networks = create_td7_networks(
            state_space, action_space, network_config
        )
        self.networks.device = self.device
        
        # ç§»åŠ¨åˆ°æ­£ç¡®è®¾å¤‡
        for network in [self.networks.state_encoder, self.networks.actor, self.networks.critic,
                       self.networks.target_state_encoder, self.networks.target_actor, 
                       self.networks.target_critic]:
            network.to(self.device)
        
        # ä¼˜åŒ–å™¨
        self.encoder_optimizer = optim.Adam(
            self.networks.state_encoder.parameters(),
            lr=self.config['encoder_lr']
        )

        self.actor_optimizer = optim.Adam(
            self.networks.actor.parameters(),
            lr=self.config['actor_lr']
        )

        self.critic_optimizer = optim.Adam(
            self.networks.critic.parameters(),
            lr=self.config['critic_lr']
        )

        # ğŸ”§ æ·»åŠ å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆä½™å¼¦é€€ç«ï¼Œé˜²æ­¢åæœŸå´©æºƒï¼‰
        if self.config['use_lr_schedule']:
            from torch.optim.lr_scheduler import LambdaLR

            def lr_lambda(step):
                """
                å»¶è¿Ÿä½™å¼¦é€€ç«å­¦ä¹ ç‡è°ƒåº¦
                - å‰75kæ­¥: ä¿æŒå›ºå®šå­¦ä¹ ç‡1.0
                - 75kæ­¥å: ä½™å¼¦é€€ç«è‡³0.1
                """
                warmup_steps = self.config['warmup_steps']
                total_steps = self.config['total_steps']
                min_ratio = self.config['min_lr_ratio']

                if step < warmup_steps:
                    return 1.0  # å›ºå®šå­¦ä¹ ç‡
                else:
                    # ä½™å¼¦é€€ç«
                    progress = (step - warmup_steps) / (total_steps - warmup_steps)
                    progress = min(progress, 1.0)
                    cosine_factor = 0.5 * (1 + np.cos(np.pi * progress))
                    return min_ratio + (1.0 - min_ratio) * cosine_factor

            self.encoder_scheduler = LambdaLR(self.encoder_optimizer, lr_lambda)
            self.actor_scheduler = LambdaLR(self.actor_optimizer, lr_lambda)
            self.critic_scheduler = LambdaLR(self.critic_optimizer, lr_lambda)
        else:
            self.encoder_scheduler = None
            self.actor_scheduler = None
            self.critic_scheduler = None
        
        # ä¼˜å…ˆçº§ç»éªŒå›æ”¾ç¼“å†²åŒº
        self.replay_buffer = TD7_PrioritizedReplayBuffer(
            capacity=self.config['buffer_size'],
            batch_size=self.config['batch_size'],
            alpha=self.config['alpha'],
            beta=self.config['beta'],
            beta_increment=self.config['beta_increment'],
            device=self.device
        )
        
        # è®­ç»ƒç»Ÿè®¡
        self.training_step = 0
        self.episode_rewards = []
        self.losses = {
            'actor_loss': [],
            'critic_loss': [],
            'encoder_loss': []
        }
        
        # æ£€æŸ¥ç‚¹ç®¡ç†
        self.checkpoints = []
        
        print(f"ğŸ¯ TD7 Agent initialized on {self.device}")
        print(f"   State space: {state_space.shape}")
        print(f"   Action space: {action_space.shape}")
        print(f"   Embedding dim: {self.config['embedding_dim']}")
        print(f"   Use checkpoints: {self.config['use_checkpoints']}")
        print(f"   Policy delay: {self.config['policy_delay']}")
    
    def act(self, state: np.ndarray, training: bool = True) -> np.ndarray:
        """é€‰æ‹©åŠ¨ä½œ"""
        if not isinstance(state, np.ndarray):
            state = np.array(state)
        
        # è½¬æ¢ä¸ºtensor
        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)
        
        with torch.no_grad():
            # è·å–çŠ¶æ€åµŒå…¥
            state_embedding = self.networks.state_encoder(state_tensor)
            
            # è·å–åŠ¨ä½œ
            action = self.networks.actor(state_embedding).cpu().numpy()[0]
            
            # æ·»åŠ æ¢ç´¢å™ªå£°
            if training:
                noise = np.random.normal(
                    0, self.config['exploration_noise'], size=action.shape
                )
                action += noise
        
        # ç¡®ä¿åŠ¨ä½œåœ¨æœ‰æ•ˆèŒƒå›´å†…
        action = np.clip(action, self.action_space.low, self.action_space.high)
        
        return action
    
    def store_transition(self,
                        state: np.ndarray,
                        action: np.ndarray,
                        reward: float,
                        next_state: np.ndarray,
                        done: bool):
        """å­˜å‚¨è½¬æ¢åˆ°ç¼“å†²åŒº"""
        self.replay_buffer.add(state, action, reward, next_state, done)
    
    def train(self) -> Optional[Dict]:
        """è®­ç»ƒä¸€æ­¥"""
        if not self.replay_buffer.is_ready:
            return None
        
        if self.replay_buffer.tree.n_entries < self.config['learning_starts']:
            return None
        
        if self.training_step % self.config['train_freq'] != 0:
            self.training_step += 1
            return None
        
        # é‡‡æ ·æ‰¹æ¬¡
        batch = self.replay_buffer.sample()
        if batch is None:
            self.training_step += 1
            return None
        
        # æ›´æ–°ç½‘ç»œ
        losses = self._update_networks(batch)
        
        # æ£€æŸ¥ç‚¹ç®¡ç†
        if (self.config['use_checkpoints'] and 
            self.training_step % self.config['checkpoint_freq'] == 0):
            self._save_checkpoint()
        
        # è½¯æ›´æ–°ç›®æ ‡ç½‘ç»œ
        self.networks.soft_update_target_networks(self.config['tau'])
        
        self.training_step += 1
        
        # å­˜å‚¨æŸå¤±
        for key, value in losses.items():
            self.losses[key].append(value)
        
        # è¿”å›è®­ç»ƒä¿¡æ¯
        return {
            'actor_loss': losses['actor_loss'],
            'critic_loss': losses['critic_loss'],
            'encoder_loss': losses['encoder_loss'],
            'buffer_size': len(self.replay_buffer),
            'training_step': self.training_step,
            'max_priority': self.replay_buffer.max_priority
        }
    
    def _update_networks(self, batch: Dict[str, torch.Tensor]) -> Dict[str, float]:
        """æ›´æ–°ç½‘ç»œå‚æ•°"""
        states = batch['states']
        actions = batch['actions']
        rewards = batch['rewards']
        next_states = batch['next_states']
        dones = batch['dones']
        is_weights = batch['is_weights']
        indices = batch['indices']
        
        # è·å–çŠ¶æ€åµŒå…¥
        state_embeddings = self.networks.state_encoder(states)
        next_state_embeddings = self.networks.target_state_encoder(next_states)
        
        # æ›´æ–°Criticç½‘ç»œ
        critic_loss, td_errors = self._update_critic(
            state_embeddings, actions, rewards, next_state_embeddings, dones, is_weights
        )
        
        # æ›´æ–°ä¼˜å…ˆçº§
        priorities = self.replay_buffer.compute_lap_priority(td_errors, actions)
        self.replay_buffer.update_priorities(indices, priorities)
        
        # å»¶è¿Ÿç­–ç•¥æ›´æ–°
        actor_loss = 0.0
        encoder_loss = 0.0
        
        if self.training_step % self.config['policy_delay'] == 0:
            # æ›´æ–°Actorç½‘ç»œ
            actor_loss = self._update_actor(state_embeddings.detach())
            
            # æ›´æ–°çŠ¶æ€ç¼–ç å™¨
            if self.training_step % self.config['embedding_update_freq'] == 0:
                encoder_loss = self._update_encoder(states, actions)
        
        return {
            'actor_loss': actor_loss,
            'critic_loss': critic_loss,
            'encoder_loss': encoder_loss
        }
    
    def _update_critic(self, state_embeddings, actions, rewards, next_state_embeddings, dones, is_weights):
        """æ›´æ–°Criticç½‘ç»œ"""
        with torch.no_grad():
            # è®¡ç®—ç›®æ ‡åŠ¨ä½œï¼ˆå¸¦å™ªå£°ï¼‰
            target_actions = self.networks.target_actor(next_state_embeddings)
            
            # æ·»åŠ ç›®æ ‡å¹³æ»‘å™ªå£°
            noise = torch.clamp(
                torch.randn_like(target_actions) * self.config['target_noise'],
                -self.config['noise_clip'], self.config['noise_clip']
            )
            target_actions = torch.clamp(
                target_actions + noise,
                -self.networks.max_action, self.networks.max_action
            )
            
            # è®¡ç®—ç›®æ ‡Qå€¼ï¼ˆå–æœ€å°å€¼å‡å°‘è¿‡ä¼°è®¡ï¼‰
            target_q1, target_q2 = self.networks.target_critic(next_state_embeddings, target_actions)
            target_q = torch.min(target_q1, target_q2)
            target_q = rewards + self.config['gamma'] * (1 - dones) * target_q
        
        # å½“å‰Qå€¼
        current_q1, current_q2 = self.networks.critic(state_embeddings, actions)
        
        # TDè¯¯å·®
        td_error1 = target_q - current_q1
        td_error2 = target_q - current_q2
        
        # HuberæŸå¤±ï¼ˆTD7ç‰¹è‰²ï¼‰
        critic_loss1 = F.smooth_l1_loss(current_q1, target_q, reduction='none')
        critic_loss2 = F.smooth_l1_loss(current_q2, target_q, reduction='none')
        
        # é‡è¦æ€§é‡‡æ ·åŠ æƒ
        critic_loss1 = (critic_loss1 * is_weights).mean()
        critic_loss2 = (critic_loss2 * is_weights).mean()
        
        critic_loss = critic_loss1 + critic_loss2
        
        # æ›´æ–°Critic
        self.critic_optimizer.zero_grad()
        critic_loss.backward()
        self.critic_optimizer.step()

        # ğŸ”§ æ›´æ–°criticå­¦ä¹ ç‡è°ƒåº¦
        if self.critic_scheduler is not None:
            self.critic_scheduler.step()

        # è¿”å›TDè¯¯å·®ï¼ˆç”¨äºä¼˜å…ˆçº§æ›´æ–°ï¼‰
        td_errors = torch.max(torch.abs(td_error1), torch.abs(td_error2)).detach()

        return critic_loss.item(), td_errors
    
    def _update_actor(self, state_embeddings):
        """æ›´æ–°Actorç½‘ç»œ"""
        # è®¡ç®—ç­–ç•¥æŸå¤±
        actions = self.networks.actor(state_embeddings)
        actor_loss = -self.networks.critic.q1(state_embeddings, actions).mean()
        
        # æ›´æ–°Actor
        self.actor_optimizer.zero_grad()
        actor_loss.backward()
        self.actor_optimizer.step()

        # ğŸ”§ æ›´æ–°actorå­¦ä¹ ç‡è°ƒåº¦
        if self.actor_scheduler is not None:
            self.actor_scheduler.step()

        return actor_loss.item()
    
    def _update_encoder(self, states, actions):
        """æ›´æ–°çŠ¶æ€ç¼–ç å™¨ï¼ˆSALEæœºåˆ¶ï¼‰"""
        # é‡æ–°è®¡ç®—åµŒå…¥
        state_embeddings = self.networks.state_encoder(states)
        
        # SALEæŸå¤±ï¼šæœ€å¤§åŒ–çŠ¶æ€-åŠ¨ä½œç›¸äº’ä½œç”¨
        # è¿™é‡Œä½¿ç”¨ç®€åŒ–çš„SALEæŸå¤±
        pred_actions = self.networks.actor(state_embeddings)
        embedding_loss = F.mse_loss(pred_actions, actions)
        
        # åµŒå…¥æ­£åˆ™åŒ–
        embedding_reg = torch.mean(torch.norm(state_embeddings, dim=-1))
        
        total_encoder_loss = (
            self.config['embedding_loss_weight'] * embedding_loss + 
            0.01 * embedding_reg
        )
        
        # æ›´æ–°ç¼–ç å™¨
        self.encoder_optimizer.zero_grad()
        total_encoder_loss.backward()
        self.encoder_optimizer.step()

        # ğŸ”§ æ›´æ–°encoderå­¦ä¹ ç‡è°ƒåº¦
        if self.encoder_scheduler is not None:
            self.encoder_scheduler.step()

        return total_encoder_loss.item()
    
    def _save_checkpoint(self):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint = {
            'state_encoder': self.networks.state_encoder.state_dict(),
            'actor': self.networks.actor.state_dict(),
            'critic': self.networks.critic.state_dict(),
            'training_step': self.training_step,
            'losses': copy.deepcopy(self.losses)
        }
        
        self.checkpoints.append(checkpoint)
        
        # ä¿æŒæœ€å¤§æ£€æŸ¥ç‚¹æ•°é‡
        if len(self.checkpoints) > self.config['max_checkpoints']:
            self.checkpoints.pop(0)
        
        print(f"ğŸ’¾ Checkpoint saved at step {self.training_step} (total: {len(self.checkpoints)})")
    
    def load_best_checkpoint(self):
        """åŠ è½½æœ€ä½³æ£€æŸ¥ç‚¹"""
        if not self.checkpoints:
            print("âš ï¸ No checkpoints available")
            return
        
        # ç®€å•ç­–ç•¥ï¼šåŠ è½½æœ€æ–°çš„æ£€æŸ¥ç‚¹
        best_checkpoint = self.checkpoints[-1]
        
        self.networks.state_encoder.load_state_dict(best_checkpoint['state_encoder'])
        self.networks.actor.load_state_dict(best_checkpoint['actor'])
        self.networks.critic.load_state_dict(best_checkpoint['critic'])
        
        print(f"âœ… Loaded checkpoint from step {best_checkpoint['training_step']}")
    
    def save(self, filepath: str):
        """ä¿å­˜æ¨¡å‹"""
        save_dict = {
            'state_encoder': self.networks.state_encoder.state_dict(),
            'actor': self.networks.actor.state_dict(),
            'critic': self.networks.critic.state_dict(),
            'target_state_encoder': self.networks.target_state_encoder.state_dict(),
            'target_actor': self.networks.target_actor.state_dict(),
            'target_critic': self.networks.target_critic.state_dict(),
            'encoder_optimizer': self.encoder_optimizer.state_dict(),
            'actor_optimizer': self.actor_optimizer.state_dict(),
            'critic_optimizer': self.critic_optimizer.state_dict(),
            'config': self.config,
            'training_step': self.training_step,
            'checkpoints': self.checkpoints
        }
        
        torch.save(save_dict, filepath)
    
    def load(self, filepath: str):
        """åŠ è½½æ¨¡å‹"""
        checkpoint = torch.load(filepath, map_location=self.device, weights_only=False)
        
        self.networks.state_encoder.load_state_dict(checkpoint['state_encoder'])
        self.networks.actor.load_state_dict(checkpoint['actor'])
        self.networks.critic.load_state_dict(checkpoint['critic'])
        self.networks.target_state_encoder.load_state_dict(checkpoint['target_state_encoder'])
        self.networks.target_actor.load_state_dict(checkpoint['target_actor'])
        self.networks.target_critic.load_state_dict(checkpoint['target_critic'])
        
        self.encoder_optimizer.load_state_dict(checkpoint['encoder_optimizer'])
        self.actor_optimizer.load_state_dict(checkpoint['actor_optimizer'])
        self.critic_optimizer.load_state_dict(checkpoint['critic_optimizer'])
        
        self.training_step = checkpoint['training_step']
        
        if 'checkpoints' in checkpoint:
            self.checkpoints = checkpoint['checkpoints']
        
        print(f"âœ… TD7 model loaded from {filepath}")
    
    def get_stats(self) -> Dict:
        """è·å–è®­ç»ƒç»Ÿè®¡"""
        buffer_stats = self.replay_buffer.get_stats()
        
        stats = {
            'training_step': self.training_step,
            'episodes_trained': len(self.episode_rewards),
            'num_checkpoints': len(self.checkpoints),
            **buffer_stats
        }
        
        # æ·»åŠ æŸå¤±ç»Ÿè®¡
        for key, losses in self.losses.items():
            if losses:
                stats[f'avg_{key}'] = np.mean(losses[-100:])
        
        return stats