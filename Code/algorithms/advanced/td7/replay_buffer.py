"""
TD7 Replay Buffer with Prioritized Experience Replay
TD7ç®—æ³•çš„ä¼˜å…ˆçº§ç»éªŒå›æ”¾ç¼“å†²åŒºå’ŒLAPæœºåˆ¶
"""

import numpy as np
import torch
import random
from typing import Dict, Tuple, Any, Optional, List
from collections import deque
import heapq


class SumTree:
    """SumTreeæ•°æ®ç»“æ„ï¼Œç”¨äºä¼˜å…ˆçº§ç»éªŒå›æ”¾"""
    
    def __init__(self, capacity: int):
        self.capacity = capacity
        self.tree = np.zeros(2 * capacity - 1)
        self.data = np.zeros(capacity, dtype=object)
        self.n_entries = 0
        self.pending_idx = set()
    
    def _propagate(self, idx: int, change: float):
        """å‘ä¸Šä¼ æ’­ä¼˜å…ˆçº§å˜åŒ–"""
        parent = (idx - 1) // 2
        self.tree[parent] += change
        if parent != 0:
            self._propagate(parent, change)
    
    def _retrieve(self, idx: int, s: float) -> int:
        """æ£€ç´¢å¶èŠ‚ç‚¹ç´¢å¼•"""
        left = 2 * idx + 1
        right = left + 1
        
        if left >= len(self.tree):
            return idx
        
        if s <= self.tree[left]:
            return self._retrieve(left, s)
        else:
            return self._retrieve(right, s - self.tree[left])
    
    def total(self) -> float:
        """è¿”å›æ‰€æœ‰ä¼˜å…ˆçº§æ€»å’Œ"""
        return self.tree[0]
    
    def add(self, p: float, data):
        """æ·»åŠ ç»éªŒ"""
        idx = self.n_entries % self.capacity
        data_idx = idx + self.capacity - 1
        
        self.data[idx] = data
        self.update(data_idx, p)
        
        self.n_entries += 1
    
    def update(self, idx: int, p: float):
        """æ›´æ–°ä¼˜å…ˆçº§"""
        change = p - self.tree[idx]
        self.tree[idx] = p
        self._propagate(idx, change)
    
    def get(self, s: float) -> Tuple[int, float, Any]:
        """æ ¹æ®ä¼˜å…ˆçº§é‡‡æ ·"""
        idx = self._retrieve(0, s)
        data_idx = idx - self.capacity + 1
        return idx, self.tree[idx], self.data[data_idx]


class TD7_PrioritizedReplayBuffer:
    """TD7ä¼˜å…ˆçº§ç»éªŒå›æ”¾ç¼“å†²åŒºï¼ŒåŒ…å«LAPæœºåˆ¶"""
    
    def __init__(self,
                 capacity: int = 1000000,
                 batch_size: int = 256,
                 alpha: float = 0.6,  # ä¼˜å…ˆçº§æŒ‡æ•°
                 beta: float = 0.4,   # é‡è¦æ€§é‡‡æ ·æŒ‡æ•°
                 beta_increment: float = 0.001,
                 device: torch.device = torch.device('cpu')):
        """
        åˆå§‹åŒ–ä¼˜å…ˆçº§å›æ”¾ç¼“å†²åŒº
        
        Args:
            capacity: ç¼“å†²åŒºå®¹é‡
            batch_size: æ‰¹æ¬¡å¤§å°
            alpha: ä¼˜å…ˆçº§æŒ‡æ•°
            beta: é‡è¦æ€§é‡‡æ ·æŒ‡æ•°
            beta_increment: betaå¢é•¿ç‡
            device: è®¡ç®—è®¾å¤‡
        """
        self.capacity = capacity
        self.batch_size = batch_size
        self.alpha = alpha
        self.beta = beta
        self.beta_increment = beta_increment
        self.device = device
        
        self.tree = SumTree(capacity)
        self.epsilon = 0.01  # æœ€å°ä¼˜å…ˆçº§
        self.max_priority = 1.0
        
        # LAP (Learned Action Prioritization) ç»„ä»¶
        self.use_lap = True
        self.lap_weight = 0.1
        
        print(f"ğŸ“¦ TD7 Prioritized Replay Buffer initialized")
        print(f"   Capacity: {capacity:,}")
        print(f"   Batch size: {batch_size}")
        print(f"   Alpha: {alpha}, Beta: {beta}")
        print(f"   LAP enabled: {self.use_lap}")
    
    def add(self,
            state: np.ndarray,
            action: np.ndarray,
            reward: float,
            next_state: np.ndarray,
            done: bool):
        """
        æ·»åŠ ç»éªŒåˆ°ç¼“å†²åŒº
        
        Args:
            state: å½“å‰çŠ¶æ€
            action: æ‰§è¡Œçš„åŠ¨ä½œ
            reward: è·å¾—çš„å¥–åŠ±
            next_state: ä¸‹ä¸€çŠ¶æ€
            done: æ˜¯å¦ç»“æŸ
        """
        experience = {
            'state': np.array(state, dtype=np.float32),
            'action': np.array(action, dtype=np.float32),
            'reward': float(reward),
            'next_state': np.array(next_state, dtype=np.float32),
            'done': bool(done)
        }
        
        # æ–°ç»éªŒä½¿ç”¨æœ€å¤§ä¼˜å…ˆçº§
        priority = self.max_priority ** self.alpha
        self.tree.add(priority, experience)
    
    def sample(self, batch_size: Optional[int] = None) -> Optional[Dict[str, torch.Tensor]]:
        """
        ä»ç¼“å†²åŒºé‡‡æ ·ä¸€ä¸ªæ‰¹æ¬¡
        
        Args:
            batch_size: æ‰¹æ¬¡å¤§å°ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤å¤§å°
            
        Returns:
            æ‰¹æ¬¡å­—å…¸ï¼Œå¦‚æœæ ·æœ¬ä¸è¶³è¿”å›None
        """
        if batch_size is None:
            batch_size = self.batch_size
        
        if self.tree.n_entries < batch_size:
            return None
        
        batch_idx = []
        batch_experiences = []
        priorities = []
        
        # åˆ†æ®µé‡‡æ ·
        segment = self.tree.total() / batch_size
        
        # æ›´æ–°beta
        self.beta = min(1.0, self.beta + self.beta_increment)
        
        for i in range(batch_size):
            a = segment * i
            b = segment * (i + 1)
            s = random.uniform(a, b)
            
            idx, priority, experience = self.tree.get(s)
            
            batch_idx.append(idx)
            batch_experiences.append(experience)
            priorities.append(priority)
        
        # è®¡ç®—é‡è¦æ€§é‡‡æ ·æƒé‡
        sampling_probabilities = np.array(priorities) / self.tree.total()
        is_weights = np.power(self.tree.n_entries * sampling_probabilities, -self.beta)
        is_weights /= is_weights.max()
        
        # åˆ†ç¦»æ•°æ®
        states = np.array([exp['state'] for exp in batch_experiences])
        actions = np.array([exp['action'] for exp in batch_experiences])
        rewards = np.array([exp['reward'] for exp in batch_experiences])
        next_states = np.array([exp['next_state'] for exp in batch_experiences])
        dones = np.array([exp['done'] for exp in batch_experiences])
        
        # è½¬æ¢ä¸ºå¼ é‡
        batch_tensors = {
            'states': torch.FloatTensor(states).to(self.device),
            'actions': torch.FloatTensor(actions).to(self.device),
            'rewards': torch.FloatTensor(rewards).unsqueeze(1).to(self.device),
            'next_states': torch.FloatTensor(next_states).to(self.device),
            'dones': torch.FloatTensor(dones.astype(np.float32)).unsqueeze(1).to(self.device),
            'is_weights': torch.FloatTensor(is_weights).unsqueeze(1).to(self.device),
            'indices': batch_idx
        }
        
        return batch_tensors
    
    def update_priorities(self, indices: List[int], priorities: np.ndarray):
        """
        æ›´æ–°ä¼˜å…ˆçº§
        
        Args:
            indices: æ ·æœ¬ç´¢å¼•
            priorities: æ–°ä¼˜å…ˆçº§
        """
        for idx, priority in zip(indices, priorities):
            # ç¡®ä¿ä¼˜å…ˆçº§ä¸ºæ­£
            priority = abs(priority) + self.epsilon
            
            # LAPåŠ æƒ
            if self.use_lap:
                priority = priority * (1 + self.lap_weight)
            
            # æ›´æ–°æœ€å¤§ä¼˜å…ˆçº§
            self.max_priority = max(self.max_priority, priority)
            
            # åº”ç”¨alphaæŒ‡æ•°
            self.tree.update(idx, priority ** self.alpha)
    
    def compute_lap_priority(self, 
                           td_error: torch.Tensor, 
                           action: torch.Tensor) -> torch.Tensor:
        """
        è®¡ç®—LAPä¼˜å…ˆçº§
        
        Args:
            td_error: TDè¯¯å·®
            action: åŠ¨ä½œ
            
        Returns:
            LAPè°ƒæ•´çš„ä¼˜å…ˆçº§
        """
        # åŸºç¡€TDè¯¯å·®ä¼˜å…ˆçº§
        base_priority = torch.abs(td_error)
        
        # LAPç»„ä»¶ï¼šåŸºäºåŠ¨ä½œçš„é¢å¤–ä¼˜å…ˆçº§
        action_norm = torch.norm(action, dim=-1, keepdim=True)
        lap_bonus = self.lap_weight * action_norm
        
        # ç»„åˆä¼˜å…ˆçº§
        lap_priority = base_priority + lap_bonus
        
        return lap_priority.cpu().numpy()
    
    def __len__(self):
        """è¿”å›ç¼“å†²åŒºå½“å‰å¤§å°"""
        return min(self.tree.n_entries, self.capacity)
    
    @property
    def is_ready(self):
        """æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿæ ·æœ¬è¿›è¡Œè®­ç»ƒ"""
        return len(self) >= self.batch_size
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç¼“å†²åŒºç»Ÿè®¡ä¿¡æ¯"""
        if len(self) == 0:
            return {
                'buffer_size': 0,
                'beta': self.beta,
                'max_priority': self.max_priority,
                'total_priority': 0.0
            }
        
        return {
            'buffer_size': len(self),
            'beta': self.beta,
            'max_priority': self.max_priority,
            'total_priority': self.tree.total(),
            'avg_priority': self.tree.total() / len(self) if len(self) > 0 else 0
        }