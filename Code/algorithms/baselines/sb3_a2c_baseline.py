"""
SB3 A2Cç®—æ³•åŸºçº¿
SB3 A2C Baseline Algorithm
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import numpy as np
from stable_baselines3 import A2C
from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback
from stable_baselines3.common.monitor import Monitor
from stable_baselines3.common.vec_env import DummyVecEnv
import torch
import math
from env.drl_optimized_env_fixed import DRLOptimizedQueueEnvFixed
from .space_utils import SB3DictWrapper


class SB3A2CBaseline:
    """SB3 A2CåŸºçº¿ç®—æ³•"""
    
    def __init__(self, config=None):
        # ğŸ”§ ä¼˜åŒ–é…ç½®v2 - æ·»åŠ å»¶è¿Ÿä½™å¼¦é€€ç«å­¦ä¹ ç‡è°ƒåº¦è§£å†³è®­ç»ƒä¸ç¨³å®šé—®é¢˜
        default_config = {
            # å­¦ä¹ å‚æ•°ä¼˜åŒ–
            'initial_lr': 7e-4,             # ğŸ”§ åˆå§‹å­¦ä¹ ç‡ï¼ˆå‰300kæ­¥ä¿æŒï¼‰
            'min_lr': 1e-5,                 # ğŸ”§ æœ€ç»ˆå­¦ä¹ ç‡ï¼ˆ500kæ­¥é™è‡³ï¼‰
            'warmup_steps': 300000,         # ğŸ”§ å‰300kæ­¥ä¿æŒå›ºå®šlrå……åˆ†æ¢ç´¢
            'total_steps': 500000,          # ğŸ”§ æ€»è®­ç»ƒæ­¥æ•°
            'n_steps': 32,                  # ğŸ”§ ä¼˜åŒ–: 5 â†’ 32 (å¢åŠ rollouté•¿åº¦æ”¹è¿›ä¼˜åŠ¿ä¼°è®¡)
            'gamma': 0.99,                  # âœ… ä¿æŒä¸å˜
            'gae_lambda': 0.95,             # ğŸ”§ ä¼˜åŒ–: 1.0 â†’ 0.95 (bias-varianceæƒè¡¡)

            # æ¢ç´¢ä¸ä»·å€¼å‡½æ•°
            'ent_coef': 0.01,               # ğŸ”§ ä¼˜åŒ–: 0.0 â†’ 0.01 (æ·»åŠ ç†µæ­£åˆ™åŒ–ä¿ƒè¿›æ¢ç´¢)
            'vf_coef': 0.5,                 # âœ… ä¿æŒä¸å˜
            'max_grad_norm': 0.5,           # âœ… ä¿æŒä¸å˜

            # ä¼˜åŒ–å™¨é…ç½®
            'rms_prop_eps': 1e-5,           # âœ… ä¿æŒä¸å˜
            'use_rms_prop': True,           # âœ… ä¿æŒä¸å˜
            'use_sde': False,               # âœ… ä¿æŒä¸å˜
            'normalize_advantage': True,    # ğŸ”§ ä¼˜åŒ–: False â†’ True (å½’ä¸€åŒ–ä¼˜åŠ¿å‡å°‘æ–¹å·®)

            # å…¶ä»–é…ç½®
            'tensorboard_log': "./tensorboard_logs/",
            'verbose': 1,
            'seed': 42
        }

        if config:
            default_config.update(config)

        self.config = default_config
        self.model = None
        self.env = None
        
    def setup_env(self):
        """è®¾ç½®ç¯å¢ƒ"""
        base_env = DRLOptimizedQueueEnvFixed()
        wrapped_env = SB3DictWrapper(base_env)
        self.env = Monitor(wrapped_env, filename=None)
        
        # åˆ›å»ºå‘é‡åŒ–ç¯å¢ƒ
        self.vec_env = DummyVecEnv([lambda: self.env])
        
        return self.env
    
    def create_model(self):
        """åˆ›å»ºA2Cæ¨¡å‹"""
        if self.env is None:
            self.setup_env()

        # ğŸ”§ åˆ›å»ºå»¶è¿Ÿä½™å¼¦é€€ç«å­¦ä¹ ç‡è°ƒåº¦å‡½æ•°ï¼ˆå‰300kæ­¥ä¿æŒå›ºå®šlrï¼Œä¹‹åä½™å¼¦ä¸‹é™ï¼‰
        def delayed_cosine_annealing_schedule(progress_remaining):
            """
            å»¶è¿Ÿä½™å¼¦é€€ç«å­¦ä¹ ç‡è°ƒåº¦
            å‰300kæ­¥: ä¿æŒå›ºå®šå­¦ä¹ ç‡7e-4 (å……åˆ†æ¢ç´¢)
            300kæ­¥å: ä½™å¼¦é€€ç«ä¸‹é™åˆ°1e-5 (ç¨³å®šæ”¶æ•›)

            progress_remaining: 1.0 -> 0.0 (ä»å¼€å§‹åˆ°ç»“æŸ)
            """
            initial_lr = self.config.get('initial_lr', 7e-4)
            min_lr = self.config.get('min_lr', 1e-5)
            warmup_steps = self.config.get('warmup_steps', 300000)  # å‰300kæ­¥ä¿æŒå›ºå®šlr
            total_steps = self.config.get('total_steps', 500000)     # æ€»è®­ç»ƒæ­¥æ•°

            # è®¡ç®—å½“å‰æ­¥æ•°
            current_step = int((1.0 - progress_remaining) * total_steps)

            # å‰300kæ­¥: ä¿æŒå›ºå®šå­¦ä¹ ç‡
            if current_step < warmup_steps:
                return initial_lr

            # 300kæ­¥å: ä½™å¼¦é€€ç« (å°†å‰©ä½™200kæ­¥æ˜ å°„åˆ°0â†’1)
            annealing_progress = (current_step - warmup_steps) / (total_steps - warmup_steps)
            cosine_factor = 0.5 * (1 + math.cos(math.pi * annealing_progress))
            current_lr = min_lr + (initial_lr - min_lr) * cosine_factor

            return current_lr

        # ğŸ”§ ä¼˜åŒ–: å¢åŠ ç½‘ç»œå®¹é‡æé«˜è¡¨è¾¾èƒ½åŠ›
        policy_kwargs = dict(
            net_arch=dict(
                pi=[512, 512, 256],  # ğŸ”§ Policyç½‘ç»œ: å¢åŠ æ·±åº¦å’Œå®½åº¦
                vf=[512, 512, 256]   # ğŸ”§ Valueç½‘ç»œ: ç‹¬ç«‹çš„å¤§å®¹é‡ç½‘ç»œ
            )
        )

        # åˆ›å»ºA2Cæ¨¡å‹
        self.model = A2C(
            "MlpPolicy",
            self.vec_env,
            learning_rate=delayed_cosine_annealing_schedule,  # ğŸ”§ v2: ä½¿ç”¨å»¶è¿Ÿä½™å¼¦é€€ç«è°ƒåº¦
            n_steps=self.config['n_steps'],
            gamma=self.config['gamma'],
            gae_lambda=self.config['gae_lambda'],
            ent_coef=self.config['ent_coef'],
            vf_coef=self.config['vf_coef'],
            max_grad_norm=self.config['max_grad_norm'],
            rms_prop_eps=self.config['rms_prop_eps'],
            use_rms_prop=self.config['use_rms_prop'],
            use_sde=self.config['use_sde'],
            normalize_advantage=self.config['normalize_advantage'],
            policy_kwargs=policy_kwargs,           # ğŸ”§ æ–°å¢: ç½‘ç»œæ¶æ„é…ç½®
            tensorboard_log=self.config['tensorboard_log'],
            verbose=self.config['verbose'],
            seed=self.config['seed'],
            device='auto'
        )
        
        print(f"SB3 A2C model created with device: {self.model.device}")
        return self.model
    
    def train(self, total_timesteps, eval_freq=10000, save_freq=50000):
        """è®­ç»ƒæ¨¡å‹"""
        if self.model is None:
            self.create_model()
        
        # åˆ›å»ºå¿…è¦çš„ç›®å½•
        os.makedirs('./logs/', exist_ok=True)
        os.makedirs('../../../Models/sb3_a2c_best/', exist_ok=True)
        os.makedirs('../../../Models/sb3_a2c_checkpoints/', exist_ok=True)
        
        # åˆ›å»ºè¯„ä¼°ç¯å¢ƒ
        eval_env = DummyVecEnv([lambda: Monitor(
            SB3DictWrapper(DRLOptimizedQueueEnvFixed()), 
            filename=None
        )])
        
        # åˆ›å»ºå›è°ƒ
        eval_callback = EvalCallback(
            eval_env,
            best_model_save_path='../../../Models/sb3_a2c_best/',
            log_path='./logs/',
            eval_freq=eval_freq,
            deterministic=True,
            render=False,
            n_eval_episodes=10,
            verbose=1
        )
        
        checkpoint_callback = CheckpointCallback(
            save_freq=save_freq,
            save_path='../../../Models/sb3_a2c_checkpoints/',
            name_prefix='sb3_a2c'
        )
        
        # å¼€å§‹è®­ç»ƒ
        print(f"Starting SB3 A2C training for {total_timesteps:,} timesteps...")
        
        self.model.learn(
            total_timesteps=total_timesteps,
            callback=None,  # ç§»é™¤æœ‰é—®é¢˜çš„callbacks
            log_interval=10,
            tb_log_name="SB3_A2C"
        )
        
        print("SB3 A2C training completed!")
        
        # è¿”å›è®­ç»ƒç»“æœå­—å…¸ä»¥å…¼å®¹æ¯”è¾ƒæ¡†æ¶
        return {
            'episodes': 0,  # SB3æ²¡æœ‰ç›´æ¥çš„episodeè®¡æ•°
            'total_timesteps': total_timesteps,
            'final_reward': 0  # å°†åœ¨è¯„ä¼°ä¸­è·å¾—
        }
    
    def evaluate(self, n_episodes=10, deterministic=True, verbose=True):
        """è¯„ä¼°æ¨¡å‹"""
        if self.model is None:
            raise ValueError("Model not trained yet!")
        
        # åˆ›å»ºè¯„ä¼°ç¯å¢ƒ
        eval_env = SB3DictWrapper(DRLOptimizedQueueEnvFixed())
        
        episode_rewards = []
        episode_lengths = []
        
        for episode in range(n_episodes):
            obs, _ = eval_env.reset()
            episode_reward = 0
            episode_length = 0
            done = False
            
            while not done:
                action, _ = self.model.predict(obs, deterministic=deterministic)
                obs, reward, terminated, truncated, info = eval_env.step(action)
                done = terminated or truncated
                
                episode_reward += reward
                episode_length += 1
                
                if episode_length >= 200:  # é˜²æ­¢æ— é™å¾ªç¯
                    break
            
            episode_rewards.append(episode_reward)
            episode_lengths.append(episode_length)
            
            if verbose:
                print(f"  Episode {episode+1}/{n_episodes}: Reward = {episode_reward:.2f}")
        
        results = {
            'mean_reward': np.mean(episode_rewards),
            'std_reward': np.std(episode_rewards),
            'mean_length': np.mean(episode_lengths),
            'episode_rewards': episode_rewards,
            'episode_lengths': episode_lengths,
            'system_metrics': []  # SB3ç®—æ³•æ²¡æœ‰ç³»ç»ŸæŒ‡æ ‡
        }
        
        return results
    
    def save_results(self, path_prefix):
        """ä¿å­˜è®­ç»ƒå†å²å’Œç»“æœ"""
        # åˆ›å»ºç›®å½•
        os.makedirs(os.path.dirname(path_prefix) if os.path.dirname(path_prefix) else ".", exist_ok=True)
        
        # SB3ç®—æ³•æ²¡æœ‰è®­ç»ƒå†å²ï¼Œåˆ›å»ºç©ºçš„å†å²è®°å½•
        self.training_history = {
            'episode_rewards': [],
            'episode_lengths': [],
            'avg_rewards': [],
            'loss_values': []
        }
        
        # ä¿å­˜ä¸ºJSONæ–‡ä»¶ï¼ˆå¦‚æœéœ€è¦çš„è¯ï¼‰
        import json
        with open(f"{path_prefix}_history.json", 'w') as f:
            json.dump(self.training_history, f, indent=2)
        
        print(f"SB3 A2C results saved to: {path_prefix}")
    
    def save(self, path):
        """ä¿å­˜æ¨¡å‹"""
        if self.model is None:
            raise ValueError("Model not trained yet!")

        try:
            # å°è¯•ä½¿ç”¨ exclude å‚æ•°æ¥é¿å… pickle é”™è¯¯
            # æ’é™¤ç¯å¢ƒå¯¹è±¡ï¼Œåªä¿å­˜æ¨¡å‹å‚æ•°
            self.model.save(path, exclude=['env', 'logger', 'ep_info_buffer', 'ep_success_buffer'])
            print(f"SB3 A2C model saved to: {path}")
        except Exception as e:
            # å¦‚æœä»ç„¶å¤±è´¥ï¼Œä¿å­˜ä¸ºPyTorch state dict
            print(f"Warning: Standard save failed ({e}), using state_dict fallback...")
            import torch
            state_dict = {
                'policy_state_dict': self.model.policy.state_dict(),
                'observation_space': self.model.observation_space,
                'action_space': self.model.action_space,
            }
            torch.save(state_dict, path + '.pth')
            print(f"SB3 A2C model saved as state_dict to: {path}.pth")
    
    def load(self, path):
        """åŠ è½½æ¨¡å‹"""
        import os
        import torch

        if self.env is None:
            self.setup_env()

        # æ£€æŸ¥æ˜¯å¦æ˜¯.pthæ–‡ä»¶ï¼ˆfallbackæ ¼å¼ï¼‰
        if path.endswith('.pth') or (not path.endswith('.zip') and os.path.exists(path + '.pth')):
            pth_path = path if path.endswith('.pth') else path + '.pth'
            print(f"Loading from state_dict format: {pth_path}")

            # åŠ è½½state dict
            state_dict = torch.load(pth_path, weights_only=False)

            # åˆ›å»ºæ–°æ¨¡å‹
            self.create_model()

            # åŠ è½½å‚æ•°
            self.model.policy.load_state_dict(state_dict['policy_state_dict'])
            print(f"âœ… SB3 A2C model loaded from state_dict: {pth_path}")
        else:
            # æ ‡å‡†SB3æ ¼å¼
            self.model = A2C.load(path, env=self.vec_env)
            print(f"SB3 A2C model loaded from: {path}")

        return self.model


def test_sb3_a2c():
    """æµ‹è¯•SB3 A2C"""
    print("Testing SB3 A2C...")
    
    # åˆ›å»ºåŸºçº¿
    baseline = SB3A2CBaseline()
    
    # è®­ç»ƒ
    baseline.train(total_timesteps=50000)
    
    # è¯„ä¼°
    results = baseline.evaluate(n_episodes=10)
    print(f"SB3 A2C Results: {results['mean_reward']:.2f} Â± {results['std_reward']:.2f}")
    
    # ä¿å­˜
    baseline.save("../../../Models/sb3_a2c_test.zip")


if __name__ == "__main__":
    test_sb3_a2c()