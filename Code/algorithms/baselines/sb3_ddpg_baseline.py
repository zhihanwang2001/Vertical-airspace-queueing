"""
SB3 DDPGç®—æ³•åŸºçº¿
SB3 DDPG Baseline Algorithm
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import numpy as np
from stable_baselines3 import DDPG
from stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise
from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback
from stable_baselines3.common.monitor import Monitor
from stable_baselines3.common.vec_env import DummyVecEnv
import torch
from env.drl_optimized_env_fixed import DRLOptimizedQueueEnvFixed
from .space_utils import SB3DictWrapper


class SB3DDPGBaseline:
    """SB3 DDPGåŸºçº¿ç®—æ³•"""
    
    def __init__(self, config=None):
        # ğŸ”§ ä¼˜åŒ–é…ç½® - æé«˜DDPGè®­ç»ƒç¨³å®šæ€§
        default_config = {
            # å­¦ä¹ å‚æ•°ä¼˜åŒ–
            'learning_rate': 5e-5,          # ğŸ”§ ä¼˜åŒ–: 1e-4 â†’ 5e-5 (é™ä½50%é˜²æ­¢éœ‡è¡)
            'buffer_size': 500000,          # ğŸ”§ ä¼˜åŒ–: 1M â†’ 500k (å‡å°‘é™ˆæ—§ç»éªŒ)
            'learning_starts': 10000,       # ğŸ”§ ä¼˜åŒ–: 100 â†’ 10000 (å……åˆ†warm-up)
            'batch_size': 256,              # âœ… ä¿æŒä¸å˜
            'tau': 0.005,                   # âœ… ä¿æŒä¸å˜ (è½¯æ›´æ–°ç‡)
            'gamma': 0.99,                  # âœ… ä¿æŒä¸å˜
            'train_freq': 1,                # âœ… ä¿æŒä¸å˜
            'gradient_steps': 1,            # âœ… ä¿æŒä¸å˜

            # æ¢ç´¢å™ªå£°é…ç½®
            'action_noise_type': 'ou',      # ğŸ”§ ä¼˜åŒ–: normal â†’ ou (OUå™ªå£°æ›´å¹³æ»‘)
            'action_noise_sigma': 0.15,     # ğŸ”§ ä¼˜åŒ–: 0.1 â†’ 0.15 (åˆå§‹æ¢ç´¢æ›´å……åˆ†)

            # å…¶ä»–é…ç½®
            'tensorboard_log': "./tensorboard_logs/",
            'verbose': 1,
            'seed': 42
        }
        
        if config:
            default_config.update(config)
        
        self.config = default_config
        self.model = None
        self.env = None
        
    def setup_env(self):
        """è®¾ç½®ç¯å¢ƒ"""
        base_env = DRLOptimizedQueueEnvFixed()
        wrapped_env = SB3DictWrapper(base_env)
        self.env = Monitor(wrapped_env, filename=None)
        
        # åˆ›å»ºå‘é‡åŒ–ç¯å¢ƒ
        self.vec_env = DummyVecEnv([lambda: self.env])
        
        return self.env
    
    def create_model(self):
        """åˆ›å»ºDDPGæ¨¡å‹"""
        if self.env is None:
            self.setup_env()
        
        # è®¾ç½®åŠ¨ä½œå™ªå£°
        # ä½¿ç”¨åŒ…è£…åçš„å‘é‡åŒ–ç¯å¢ƒè·å–åŠ¨ä½œç»´åº¦
        n_actions = self.vec_env.action_space.shape[-1]
        
        print(f"Action space dimension: {n_actions}")
        
        if self.config['action_noise_type'] == 'ou':
            # Ornstein-Uhlenbeckå™ªå£°ï¼ˆæ›´é€‚åˆè¿ç»­æ§åˆ¶ï¼‰
            action_noise = OrnsteinUhlenbeckActionNoise(
                mean=np.zeros(n_actions),
                sigma=self.config['action_noise_sigma'] * np.ones(n_actions)
            )
        else:
            # æ­£æ€å™ªå£°
            action_noise = NormalActionNoise(
                mean=np.zeros(n_actions), 
                sigma=self.config['action_noise_sigma'] * np.ones(n_actions)
            )
        
        # ğŸ”§ ä¼˜åŒ–: æ·»åŠ policyç½‘ç»œé…ç½®å’Œæ¢¯åº¦è£å‰ª
        policy_kwargs = dict(
            net_arch=[512, 512, 256],  # ğŸ”§ ä¼˜åŒ–: å¢åŠ ç½‘ç»œå®¹é‡æé«˜è¡¨è¾¾èƒ½åŠ›
        )

        # åˆ›å»ºDDPGæ¨¡å‹
        self.model = DDPG(
            "MlpPolicy",
            self.vec_env,
            learning_rate=self.config['learning_rate'],
            buffer_size=self.config['buffer_size'],
            learning_starts=self.config['learning_starts'],
            batch_size=self.config['batch_size'],
            tau=self.config['tau'],
            gamma=self.config['gamma'],
            train_freq=self.config['train_freq'],
            gradient_steps=self.config['gradient_steps'],
            action_noise=action_noise,
            policy_kwargs=policy_kwargs,           # ğŸ”§ æ–°å¢: ç½‘ç»œæ¶æ„é…ç½®
            tensorboard_log=self.config['tensorboard_log'],
            verbose=self.config['verbose'],
            seed=self.config['seed'],
            device='auto'
        )
        
        print(f"SB3 DDPG model created with device: {self.model.device}")
        print(f"Action noise type: {self.config['action_noise_type']}")
        return self.model
    
    def train(self, total_timesteps, eval_freq=10000, save_freq=50000):
        """è®­ç»ƒæ¨¡å‹"""
        if self.model is None:
            self.create_model()
        
        # åˆ›å»ºå¿…è¦çš„ç›®å½•
        os.makedirs('./logs/', exist_ok=True)
        os.makedirs('../../../Models/sb3_ddpg_best/', exist_ok=True)
        os.makedirs('../../../Models/sb3_ddpg_checkpoints/', exist_ok=True)
        
        # åˆ›å»ºè¯„ä¼°ç¯å¢ƒ
        eval_env = DummyVecEnv([lambda: Monitor(
            SB3DictWrapper(DRLOptimizedQueueEnvFixed()), 
            filename=None
        )])
        
        # åˆ›å»ºå›è°ƒ
        eval_callback = EvalCallback(
            eval_env,
            best_model_save_path='../../../Models/sb3_ddpg_best/',
            log_path='./logs/',
            eval_freq=eval_freq,
            deterministic=True,
            render=False,
            n_eval_episodes=10,
            verbose=1
        )
        
        checkpoint_callback = CheckpointCallback(
            save_freq=save_freq,
            save_path='../../../Models/sb3_ddpg_checkpoints/',
            name_prefix='sb3_ddpg'
        )
        
        # å¼€å§‹è®­ç»ƒ
        print(f"Starting SB3 DDPG training for {total_timesteps:,} timesteps...")
        
        self.model.learn(
            total_timesteps=total_timesteps,
            callback=None,  # ç§»é™¤æœ‰é—®é¢˜çš„callbacks
            log_interval=10,
            tb_log_name="SB3_DDPG"
        )
        
        print("SB3 DDPG training completed!")
        
        # è¿”å›è®­ç»ƒç»“æœå­—å…¸ä»¥å…¼å®¹æ¯”è¾ƒæ¡†æ¶
        return {
            'episodes': 0,  # SB3æ²¡æœ‰ç›´æ¥çš„episodeè®¡æ•°
            'total_timesteps': total_timesteps,
            'final_reward': 0  # å°†åœ¨è¯„ä¼°ä¸­è·å¾—
        }
    
    def evaluate(self, n_episodes=10, deterministic=True, verbose=True):
        """è¯„ä¼°æ¨¡å‹"""
        if self.model is None:
            raise ValueError("Model not trained yet!")
        
        # åˆ›å»ºè¯„ä¼°ç¯å¢ƒ
        eval_env = SB3DictWrapper(DRLOptimizedQueueEnvFixed())
        
        episode_rewards = []
        episode_lengths = []
        
        for episode in range(n_episodes):
            obs, _ = eval_env.reset()
            episode_reward = 0
            episode_length = 0
            done = False
            
            while not done:
                action, _ = self.model.predict(obs, deterministic=deterministic)
                obs, reward, terminated, truncated, info = eval_env.step(action)
                done = terminated or truncated
                
                episode_reward += reward
                episode_length += 1
                
                if episode_length >= 200:  # é˜²æ­¢æ— é™å¾ªç¯
                    break
            
            episode_rewards.append(episode_reward)
            episode_lengths.append(episode_length)
            
            if verbose:
                print(f"  Episode {episode+1}/{n_episodes}: Reward = {episode_reward:.2f}")
        
        results = {
            'mean_reward': np.mean(episode_rewards),
            'std_reward': np.std(episode_rewards),
            'mean_length': np.mean(episode_lengths),
            'episode_rewards': episode_rewards,
            'episode_lengths': episode_lengths,
            'system_metrics': []  # SB3ç®—æ³•æ²¡æœ‰ç³»ç»ŸæŒ‡æ ‡
        }
        
        return results
    
    def save_results(self, path_prefix):
        """ä¿å­˜è®­ç»ƒå†å²å’Œç»“æœ"""
        # åˆ›å»ºç›®å½•
        os.makedirs(os.path.dirname(path_prefix) if os.path.dirname(path_prefix) else ".", exist_ok=True)
        
        # SB3ç®—æ³•æ²¡æœ‰è®­ç»ƒå†å²ï¼Œåˆ›å»ºç©ºçš„å†å²è®°å½•
        self.training_history = {
            'episode_rewards': [],
            'episode_lengths': [],
            'avg_rewards': [],
            'loss_values': []
        }
        
        # ä¿å­˜ä¸ºJSONæ–‡ä»¶ï¼ˆå¦‚æœéœ€è¦çš„è¯ï¼‰
        import json
        with open(f"{path_prefix}_history.json", 'w') as f:
            json.dump(self.training_history, f, indent=2)
        
        print(f"SB3 DDPG results saved to: {path_prefix}")
    
    def save(self, path):
        """ä¿å­˜æ¨¡å‹"""
        if self.model is None:
            raise ValueError("Model not trained yet!")
        
        self.model.save(path)
        print(f"SB3 DDPG model saved to: {path}")
    
    def load(self, path):
        """åŠ è½½æ¨¡å‹"""
        if self.env is None:
            self.setup_env()
        
        self.model = DDPG.load(path, env=self.vec_env)
        print(f"SB3 DDPG model loaded from: {path}")
        return self.model


def test_sb3_ddpg():
    """æµ‹è¯•SB3 DDPG"""
    print("Testing SB3 DDPG...")
    
    # æµ‹è¯•ä¸åŒå™ªå£°ç±»å‹
    configs = [
        {'action_noise_type': 'normal', 'action_noise_sigma': 0.1},
        {'action_noise_type': 'ou', 'action_noise_sigma': 0.1}
    ]
    
    for i, config in enumerate(configs):
        print(f"\n--- Testing config {i+1}: {config['action_noise_type']} noise ---")
        
        # åˆ›å»ºåŸºçº¿
        baseline = SB3DDPGBaseline(config)
        
        # è®­ç»ƒ
        baseline.train(total_timesteps=50000)
        
        # è¯„ä¼°
        results = baseline.evaluate(n_episodes=10)
        print(f"SB3 DDPG ({config['action_noise_type']}) Results: {results['mean_reward']:.2f} Â± {results['std_reward']:.2f}")
        
        # ä¿å­˜
        baseline.save(f"../../../Models/sb3_ddpg_{config['action_noise_type']}_test.zip")


if __name__ == "__main__":
    test_sb3_ddpg()