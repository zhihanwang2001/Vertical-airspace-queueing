"""
é«˜çº§DRLç®—æ³•å¯¹æ¯”å®éªŒ
Advanced DRL Algorithm Comparison Experiments

è¿è¡ŒRainbow DQN, IMPALA, R2D2, SAC v2, TD7ç­‰æœ€æ–°ç®—æ³•ä¸ç°æœ‰åŸºçº¿çš„å¯¹æ¯”
ç”¨äºCCF BåŒºè®ºæ–‡çš„å¤§è§„æ¨¡å®éªŒéªŒè¯
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

import argparse
import time
import json
from datetime import datetime
from typing import Dict, List, Any
import numpy as np

from env.drl_optimized_env_fixed import DRLOptimizedQueueEnvFixed
from algorithms.baselines.comparison_runner import ComparisonRunner
from advanced_algorithms import (
    get_available_algorithms, 
    create_algorithm_baseline,
    print_algorithms_status
)


class AdvancedAlgorithmComparisonRunner:
    """é«˜çº§ç®—æ³•å¯¹æ¯”å®éªŒè¿è¡Œå™¨"""
    
    def __init__(self, save_dir: str = "../../Results/comparison/"):
        self.save_dir = save_dir
        os.makedirs(save_dir, exist_ok=True)
        
        # å¯ç”¨çš„é«˜çº§ç®—æ³•
        self.advanced_algorithms = get_available_algorithms()
        
        # ç°æœ‰åŸºçº¿ç®—æ³•ï¼ˆç”¨äºå¯¹æ¯”ï¼‰
        self.baseline_algorithms = [
            'SB3_PPO',   # 4399
            'SB3_TD3',   # 4255  
            'SB3_A2C',   # 1721
            'SB3_SAC',   # åŸºçº¿
            'SB3_DDPG'   # åŸºçº¿
        ]
        
        print(f"ğŸš€ Advanced Algorithm Comparison Runner initialized")
        print(f"   Save directory: {save_dir}")
        
    def run_advanced_algorithms_comparison(self,
                                         algorithms: List[str] = None,
                                         total_timesteps: int = 1000000,
                                         n_eval_episodes: int = 30,
                                         n_runs: int = 1) -> Dict:
        """
        è¿è¡Œé«˜çº§ç®—æ³•å¯¹æ¯”å®éªŒ
        
        Args:
            algorithms: è¦æµ‹è¯•çš„ç®—æ³•åˆ—è¡¨
            total_timesteps: æ¯ä¸ªç®—æ³•çš„è®­ç»ƒæ­¥æ•°
            n_eval_episodes: è¯„ä¼°å›åˆæ•°
            n_runs: æ¯ä¸ªç®—æ³•è¿è¡Œæ¬¡æ•°
        """
        if algorithms is None:
            # åªæµ‹è¯•å·²å®ç°çš„ç®—æ³•
            algorithms = [name for name, info in self.advanced_algorithms.items() 
                         if info['status'] == 'implemented']
        
        print(f"\nğŸ§ª Starting Advanced DRL Algorithm Comparison")
        print(f"   Algorithms to test: {algorithms}")
        print(f"   Training timesteps: {total_timesteps:,}")
        print(f"   Evaluation episodes: {n_eval_episodes}")
        print(f"   Runs per algorithm: {n_runs}")
        print("=" * 70)
        
        results = {}
        start_time = time.time()
        
        for algorithm_name in algorithms:
            print(f"\nğŸ¯ Testing {algorithm_name.upper()}...")
            
            algorithm_results = []
            
            for run in range(n_runs):
                print(f"   Run {run + 1}/{n_runs}")
                
                try:
                    # åˆ›å»ºç®—æ³•åŸºçº¿
                    baseline = create_algorithm_baseline(algorithm_name)
                    
                    # è®­ç»ƒ
                    train_start = time.time()
                    train_results = baseline.train(
                        total_timesteps=total_timesteps,
                        eval_freq=total_timesteps // 10,  # 10æ¬¡è¯„ä¼°
                        save_freq=total_timesteps // 4    # 4æ¬¡ä¿å­˜
                    )
                    train_time = time.time() - train_start
                    
                    # è¯„ä¼°
                    eval_results = baseline.evaluate(
                        n_episodes=n_eval_episodes,
                        deterministic=True,
                        verbose=False
                    )
                    
                    # ä¿å­˜ç»“æœ
                    run_result = {
                        'algorithm': algorithm_name,
                        'run': run,
                        'train_results': train_results,
                        'eval_results': eval_results,
                        'training_time': train_time,
                        'timestamp': datetime.now().isoformat()
                    }
                    
                    algorithm_results.append(run_result)
                    
                    print(f"     âœ… Run {run + 1} completed - "
                          f"Mean reward: {eval_results['mean_reward']:.2f} Â± {eval_results['std_reward']:.2f}")
                    
                    # ä¿å­˜æ¨¡å‹
                    model_path = f"../../Models/{algorithm_name}_run_{run}.pt"
                    os.makedirs(os.path.dirname(model_path), exist_ok=True)
                    baseline.save(model_path)
                    
                except Exception as e:
                    print(f"     âŒ Run {run + 1} failed: {str(e)}")
                    import traceback
                    traceback.print_exc()
                    continue
            
            results[algorithm_name] = algorithm_results
            
            # è®¡ç®—å¹³å‡ç»“æœ
            if algorithm_results:
                mean_rewards = [r['eval_results']['mean_reward'] for r in algorithm_results]
                mean_training_times = [r['training_time'] for r in algorithm_results]
                
                print(f"   ğŸ“Š {algorithm_name} Summary:")
                print(f"      Average reward: {np.mean(mean_rewards):.2f} Â± {np.std(mean_rewards):.2f}")
                print(f"      Average training time: {np.mean(mean_training_times):.1f}s")
        
        total_time = time.time() - start_time
        
        # ä¿å­˜å®Œæ•´ç»“æœ
        self._save_comparison_results(results, total_time, {
            'algorithms': algorithms,
            'total_timesteps': total_timesteps,
            'n_eval_episodes': n_eval_episodes,
            'n_runs': n_runs
        })
        
        # ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
        self._generate_comparison_report(results)
        
        print(f"\nğŸ‰ Advanced algorithm comparison completed!")
        print(f"   Total time: {total_time:.1f}s ({total_time/60:.1f}min)")
        print(f"   Results saved to: {self.save_dir}")
        
        return results
    
    def run_comprehensive_comparison(self,
                                   total_timesteps: int = 1000000,
                                   n_eval_episodes: int = 50,
                                   include_baselines: bool = True) -> Dict:
        """
        è¿è¡ŒåŒ…å«åŸºçº¿ç®—æ³•çš„ç»¼åˆå¯¹æ¯”å®éªŒ
        
        Args:
            total_timesteps: è®­ç»ƒæ­¥æ•°
            n_eval_episodes: è¯„ä¼°å›åˆæ•°  
            include_baselines: æ˜¯å¦åŒ…å«ç°æœ‰åŸºçº¿ç®—æ³•
        """
        print(f"\nğŸ”¬ Running Comprehensive Algorithm Comparison")
        
        # é«˜çº§ç®—æ³•ç»“æœ
        advanced_results = self.run_advanced_algorithms_comparison(
            algorithms=None,  # æ‰€æœ‰å·²å®ç°çš„ç®—æ³•
            total_timesteps=total_timesteps,
            n_eval_episodes=n_eval_episodes,
            n_runs=1
        )
        
        comprehensive_results = {
            'advanced_algorithms': advanced_results,
            'baseline_algorithms': {}
        }
        
        # å¦‚æœåŒ…å«åŸºçº¿ç®—æ³•ï¼Œè¿è¡Œç°æœ‰çš„å¯¹æ¯”å®éªŒ
        if include_baselines:
            print(f"\nğŸ”„ Running baseline algorithms for comparison...")
            
            # åˆ›å»ºç¯å¢ƒ
            env = DRLOptimizedQueueEnvFixed()
            baseline_runner = ComparisonRunner(env, save_dir=f"{self.save_dir}/baselines/")
            
            try:
                baseline_results = baseline_runner.run_comparison(
                    algorithms=self.baseline_algorithms,
                    total_timesteps=total_timesteps,
                    n_eval_episodes=n_eval_episodes,
                    n_runs=1
                )
                comprehensive_results['baseline_algorithms'] = baseline_results
                
            except Exception as e:
                print(f"âš ï¸  Baseline comparison failed: {str(e)}")
                comprehensive_results['baseline_algorithms'] = {}
        
        # ç”Ÿæˆç»¼åˆå¯¹æ¯”æŠ¥å‘Š
        self._generate_comprehensive_report(comprehensive_results)
        
        return comprehensive_results
    
    def _save_comparison_results(self, results: Dict, total_time: float, config: Dict):
        """ä¿å­˜å¯¹æ¯”ç»“æœ"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # JSONç»“æœ
        result_data = {
            'results': results,
            'total_time': total_time,
            'config': config,
            'timestamp': timestamp
        }
        
        json_path = f"{self.save_dir}/advanced_comparison_{timestamp}.json"
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(result_data, f, indent=2, default=str, ensure_ascii=False)
        
        print(f"ğŸ’¾ Results saved to: {json_path}")
    
    def _generate_comparison_report(self, results: Dict):
        """ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        report_path = f"{self.save_dir}/advanced_comparison_report_{timestamp}.md"
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("# é«˜çº§DRLç®—æ³•å¯¹æ¯”å®éªŒæŠ¥å‘Š\\n")
            f.write("# Advanced DRL Algorithm Comparison Report\\n\\n")
            
            f.write(f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n")
            f.write(f"**æµ‹è¯•ç®—æ³•æ•°é‡**: {len(results)}\\n\\n")
            
            # ç»“æœæ±‡æ€»è¡¨æ ¼
            f.write("## ğŸ“Š ç®—æ³•æ€§èƒ½å¯¹æ¯”\\n\\n")
            f.write("| ç®—æ³•åç§° | å¹³å‡å¥–åŠ± | æ ‡å‡†å·® | è®­ç»ƒæ—¶é—´(s) | çŠ¶æ€ |\\n")
            f.write("|---------|---------|--------|------------|------|\\n")
            
            # æ”¶é›†æ‰€æœ‰ç»“æœç”¨äºæ’åº
            algorithm_summaries = []
            
            for algorithm_name, algorithm_results in results.items():
                if algorithm_results:  # å¦‚æœæœ‰æˆåŠŸçš„è¿è¡Œ
                    mean_rewards = [r['eval_results']['mean_reward'] for r in algorithm_results]
                    training_times = [r['training_time'] for r in algorithm_results]
                    
                    avg_reward = np.mean(mean_rewards)
                    std_reward = np.std(mean_rewards)
                    avg_time = np.mean(training_times)
                    
                    algorithm_summaries.append({
                        'name': algorithm_name,
                        'avg_reward': avg_reward,
                        'std_reward': std_reward,
                        'avg_time': avg_time,
                        'status': 'âœ…'
                    })
                else:
                    algorithm_summaries.append({
                        'name': algorithm_name,
                        'avg_reward': 0,
                        'std_reward': 0,
                        'avg_time': 0,
                        'status': 'âŒ'
                    })
            
            # æŒ‰å¹³å‡å¥–åŠ±æ’åº
            algorithm_summaries.sort(key=lambda x: x['avg_reward'], reverse=True)
            
            for summary in algorithm_summaries:
                f.write(f"| {summary['name']} | "
                       f"{summary['avg_reward']:.2f} | "
                       f"{summary['std_reward']:.2f} | "
                       f"{summary['avg_time']:.1f} | "
                       f"{summary['status']} |\\n")
            
            # è¯¦ç»†åˆ†æ
            f.write("\\n## ğŸ” è¯¦ç»†åˆ†æ\\n\\n")
            
            if algorithm_summaries:
                best_algorithm = algorithm_summaries[0]
                f.write(f"### æœ€ä½³ç®—æ³•: {best_algorithm['name']}\\n")
                f.write(f"- å¹³å‡å¥–åŠ±: {best_algorithm['avg_reward']:.2f} Â± {best_algorithm['std_reward']:.2f}\\n")
                f.write(f"- è®­ç»ƒæ—¶é—´: {best_algorithm['avg_time']:.1f}s\\n\\n")
            
            # ç®—æ³•ç‰¹ç‚¹åˆ†æ
            f.write("### ç®—æ³•ç‰¹ç‚¹\\n\\n")
            for name, info in self.advanced_algorithms.items():
                if name in results:
                    f.write(f"**{info['name']}**: {info['description']}\\n")
                    f.write(f"- ç±»å‹: {info['type']}\\n")
                    f.write(f"- è®ºæ–‡: {info['paper']}\\n\\n")
        
        print(f"ğŸ“„ Report generated: {report_path}")
    
    def _generate_comprehensive_report(self, results: Dict):
        """ç”Ÿæˆç»¼åˆå¯¹æ¯”æŠ¥å‘Šï¼ˆåŒ…å«åŸºçº¿ç®—æ³•ï¼‰"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S') 
        report_path = f"{self.save_dir}/comprehensive_report_{timestamp}.md"
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("# é«˜çº§ç®—æ³•vsåŸºçº¿ç®—æ³•ç»¼åˆå¯¹æ¯”æŠ¥å‘Š\\n")
            f.write("# Advanced vs Baseline Algorithms Comprehensive Report\\n\\n")
            
            f.write(f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n")
            
            # è·å–æ‰€æœ‰ç®—æ³•çš„æ€§èƒ½æ•°æ®
            all_algorithms = []
            
            # é«˜çº§ç®—æ³•
            for name, algorithm_results in results.get('advanced_algorithms', {}).items():
                if algorithm_results:
                    mean_rewards = [r['eval_results']['mean_reward'] for r in algorithm_results]
                    all_algorithms.append({
                        'name': name,
                        'type': 'Advanced',
                        'reward': np.mean(mean_rewards),
                        'std': np.std(mean_rewards)
                    })
            
            # åŸºçº¿ç®—æ³•ï¼ˆä»ç°æœ‰ç»“æœè·å–ï¼‰
            baseline_performance = {
                'SB3_PPO': 4399,
                'SB3_TD3': 4255, 
                'SB3_A2C': 1721
            }
            
            for name, reward in baseline_performance.items():
                all_algorithms.append({
                    'name': name,
                    'type': 'Baseline',
                    'reward': reward,
                    'std': 0  # å•æ¬¡è¿è¡Œï¼Œæ— æ ‡å‡†å·®
                })
            
            # æ’åº
            all_algorithms.sort(key=lambda x: x['reward'], reverse=True)
            
            # ç”Ÿæˆç»¼åˆå¯¹æ¯”è¡¨
            f.write("## ğŸ“Š ç®—æ³•æ€§èƒ½ç»¼åˆæ’å\\n\\n")
            f.write("| æ’å | ç®—æ³•åç§° | ç±»å‹ | å¹³å‡å¥–åŠ± | æ ‡å‡†å·® |\\n")
            f.write("|-----|---------|------|---------|--------|\\n")
            
            for i, algo in enumerate(all_algorithms, 1):
                f.write(f"| {i} | {algo['name']} | {algo['type']} | "
                       f"{algo['reward']:.2f} | {algo['std']:.2f} |\\n")
            
            # åˆ†æ
            f.write("\\n## ğŸ¯ å…³é”®å‘ç°\\n\\n")
            
            if all_algorithms:
                top3 = all_algorithms[:3]
                f.write("### å‰ä¸‰åç®—æ³•\\n")
                for i, algo in enumerate(top3, 1):
                    f.write(f"{i}. **{algo['name']}** ({algo['type']}): {algo['reward']:.2f}\\n")
            
            f.write("\\n### é«˜çº§ç®—æ³•vsåŸºçº¿ç®—æ³•\\n")
            advanced_best = max([a for a in all_algorithms if a['type'] == 'Advanced'], 
                              key=lambda x: x['reward'], default={'reward': 0})
            baseline_best = max([a for a in all_algorithms if a['type'] == 'Baseline'],
                              key=lambda x: x['reward'], default={'reward': 0})
            
            f.write(f"- æœ€ä½³é«˜çº§ç®—æ³•: {advanced_best.get('name', 'N/A')} ({advanced_best.get('reward', 0):.2f})\\n")
            f.write(f"- æœ€ä½³åŸºçº¿ç®—æ³•: {baseline_best.get('name', 'N/A')} ({baseline_best.get('reward', 0):.2f})\\n")
            
            if advanced_best.get('reward', 0) > baseline_best.get('reward', 0):
                improvement = (advanced_best['reward'] - baseline_best['reward']) / baseline_best['reward'] * 100
                f.write(f"- **é«˜çº§ç®—æ³•æ€§èƒ½æå‡**: {improvement:.1f}%\\n")
            else:
                f.write("- åŸºçº¿ç®—æ³•åœ¨æ­¤å®éªŒä¸­è¡¨ç°æ›´ä½³\\n")
        
        print(f"ğŸ“„ Comprehensive report generated: {report_path}")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="Advanced DRL Algorithm Comparison")
    parser.add_argument('--algorithms', nargs='+',
                       help='Specific algorithms to test')
    parser.add_argument('--timesteps', type=int, default=1000000,
                       help='Training timesteps per algorithm')
    parser.add_argument('--eval-episodes', type=int, default=30,
                       help='Number of evaluation episodes')
    parser.add_argument('--runs', type=int, default=1,
                       help='Number of runs per algorithm')
    parser.add_argument('--comprehensive', action='store_true',
                       help='Run comprehensive comparison including baselines')
    parser.add_argument('--quick-test', action='store_true',
                       help='Quick test with reduced parameters')
    parser.add_argument('--list-algorithms', action='store_true',
                       help='List available algorithms and exit')
    parser.add_argument('--use-optimized-impala', action='store_true',
                       help='Use optimized IMPALA instead of original (recommended)')
    parser.add_argument('--compare-impala-versions', action='store_true',
                       help='Compare original vs optimized IMPALA')
    
    args = parser.parse_args()
    
    if args.list_algorithms:
        print_algorithms_status()
        return
    
    # åˆ›å»ºå®éªŒè¿è¡Œå™¨
    runner = AdvancedAlgorithmComparisonRunner()

    # å¤„ç†IMPALAä¼˜åŒ–é€‰é¡¹
    algorithms_to_run = args.algorithms

    if args.compare_impala_versions:
        # æ¯”è¾ƒä¸¤ä¸ªç‰ˆæœ¬çš„IMPALA
        algorithms_to_run = ['impala', 'impala_optimized']
        print("ğŸ” Comparing Original vs Optimized IMPALA")

    # ğŸ”§ ä¿®æ”¹ï¼šä¸å†è‡ªåŠ¨åˆ‡æ¢ï¼Œè®©ç”¨æˆ·æ˜¾å¼æŒ‡å®šç‰ˆæœ¬
    # elif args.use_optimized_impala or (algorithms_to_run and 'impala' in algorithms_to_run):
    #     # å¦‚æœç”¨æˆ·æŒ‡å®šäº†ä½¿ç”¨ä¼˜åŒ–ç‰ˆï¼Œæˆ–è€…åœ¨ç®—æ³•åˆ—è¡¨ä¸­æŒ‡å®šäº†impalaï¼Œè‡ªåŠ¨æ›¿æ¢ä¸ºä¼˜åŒ–ç‰ˆ
    #     if algorithms_to_run and 'impala' in algorithms_to_run:
    #         algorithms_to_run = [algo if algo != 'impala' else 'impala_optimized' for algo in algorithms_to_run]
    #         print("âœ¨ Automatically using optimized IMPALA (recommended)")
    #     elif args.use_optimized_impala:
    #         algorithms_to_run = ['impala_optimized']
    #         print("âœ¨ Using optimized IMPALA")

    elif args.use_optimized_impala:
        # ä»…å½“æ˜¾å¼æŒ‡å®š --use-optimized-impala æ—¶æ‰åˆ‡æ¢
        if algorithms_to_run and 'impala' in algorithms_to_run:
            algorithms_to_run = [algo if algo != 'impala' else 'impala_optimized' for algo in algorithms_to_run]
            print("âœ¨ Using impala_optimized (explicitly requested)")
        else:
            algorithms_to_run = ['impala_optimized']
            print("âœ¨ Using impala_optimized")

    # è°ƒæ•´å‚æ•°ï¼ˆå¿«é€Ÿæµ‹è¯•ï¼‰
    if args.quick_test:
        args.timesteps = 50000
        args.eval_episodes = 10
        print("ğŸš€ Quick test mode enabled")

    print(f"\nğŸ§ª Advanced DRL Algorithm Comparison Experiment")
    print(f"   Algorithms: {algorithms_to_run if algorithms_to_run else 'All available'}")
    print(f"   Timesteps: {args.timesteps:,}")
    print(f"   Evaluation episodes: {args.eval_episodes}")
    print(f"   Runs per algorithm: {args.runs}")

    # æ˜¾ç¤ºIMPALAä¼˜åŒ–ä¿¡æ¯
    if algorithms_to_run and 'impala_optimized' in algorithms_to_run:
        print(f"\nğŸ¯ IMPALA Optimizations Applied:")
        print(f"   âœ… Mixed action space support (continuous + discrete)")
        print(f"   âœ… Queue-specific network architecture")
        print(f"   âœ… Conservative V-trace parameters (avoid training collapse)")
        print(f"   âœ… Lower learning rate with scheduling")
        print(f"   âœ… Enhanced stability mechanisms")

    try:
        if args.comprehensive:
            # ç»¼åˆå¯¹æ¯”å®éªŒ
            results = runner.run_comprehensive_comparison(
                total_timesteps=args.timesteps,
                n_eval_episodes=args.eval_episodes,
                include_baselines=True
            )
        else:
            # ä»…é«˜çº§ç®—æ³•å¯¹æ¯”
            results = runner.run_advanced_algorithms_comparison(
                algorithms=algorithms_to_run,
                total_timesteps=args.timesteps,
                n_eval_episodes=args.eval_episodes,
                n_runs=args.runs
            )
        
        print("\\nğŸ‰ Experiment completed successfully!")
        
    except KeyboardInterrupt:
        print("\\n\\nâš ï¸  Experiment interrupted by user")
    except Exception as e:
        print(f"\\n\\nâŒ Experiment failed: {str(e)}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()