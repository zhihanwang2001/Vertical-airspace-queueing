"""
æœ€ç»ˆæ¶ˆèå®éªŒï¼šCCF-BæœŸåˆŠç‰ˆæœ¬ - TD7ç®—æ³•
Final Ablation Study: CCF-B Journal Version - TD7 Algorithm

å…³é”®å‚æ•°ï¼š
1. ç®—æ³•ï¼šTD7ï¼ˆè‡ªç ”advancedç®—æ³•ï¼‰
2. è¯„ä¼°è½®æ¬¡ï¼š50 episodesï¼ˆæ›´å¯é çš„ç»Ÿè®¡ï¼‰
3. é«˜è´Ÿè½½ï¼š10x arrival rateï¼ˆç³»ç»Ÿæ¥è¿‘é¥±å’Œï¼‰
4. å›ºå®šæµé‡æ¨¡å¼ï¼š[0.3, 0.25, 0.2, 0.15, 0.1]ï¼ˆçœŸå®UAMï¼‰
5. 5ç§å®¹é‡ç»“æ„ï¼šå€’é‡‘å­—å¡”ã€å‡åŒ€ã€é«˜å®¹é‡ã€æ­£é‡‘å­—å¡”ã€ä½å®¹é‡

ç›®æ ‡ï¼š
- éªŒè¯å€’é‡‘å­—å¡”[8,6,4,3,2]åœ¨é«˜è´Ÿè½½ä¸‹çš„ä¼˜åŠ¿
- å¯¹æ¯”TD7ä¸A2C/PPOçš„æ€§èƒ½
- æä¾›solidçš„ç»Ÿè®¡evidenceæ”¯æŒCCF-Bè®ºæ–‡
"""

import sys
import os
from pathlib import Path

project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

import numpy as np
import json
import time
from datetime import datetime

# å¯¼å…¥TD7ç»„ä»¶
from algorithms.advanced.td7.td7_agent import TD7_Agent
from algorithms.baselines.space_utils import SB3DictWrapper

# å¯¼å…¥ç¯å¢ƒ
from env.config import VerticalQueueConfig
from env.configurable_env_wrapper import ConfigurableEnvWrapper


def create_config(config_type='inverted_pyramid', high_load_multiplier=10.0):
    """
    åˆ›å»ºé«˜è´Ÿè½½é…ç½®

    high_load_multiplier: è´Ÿè½½å€æ•°ï¼ˆç›¸å¯¹v3ï¼‰
    """
    config = VerticalQueueConfig()

    # è®¾ç½®å®¹é‡
    if config_type == 'inverted_pyramid':
        config.layer_capacities = [8, 6, 4, 3, 2]  # æ€»23
    elif config_type == 'uniform':
        config.layer_capacities = [5, 5, 5, 5, 5]  # æ€»25
    elif config_type == 'high_capacity':
        config.layer_capacities = [8, 8, 8, 8, 8]  # æ€»40
    elif config_type == 'reverse_pyramid':
        config.layer_capacities = [2, 3, 4, 6, 8]  # æ€»23
    elif config_type == 'low_capacity':
        config.layer_capacities = [2, 2, 2, 2, 2]  # æ€»10
    else:
        raise ValueError(f"Unknown config type: {config_type}")

    # å›ºå®šçœŸå®UAMæµé‡æ¨¡å¼
    config.arrival_weights = [0.3, 0.25, 0.2, 0.15, 0.1]

    # å…³é”®ï¼šå¤§å¹…æé«˜åˆ°è¾¾ç‡
    total_capacity = sum(config.layer_capacities)
    avg_service_rate = np.mean(config.layer_service_rates)

    # åŸºç¡€åˆ°è¾¾ç‡ Ã— é«˜è´Ÿè½½å€æ•°
    base_rate_v3 = 0.75 * total_capacity * avg_service_rate / 5
    config.base_arrival_rate = base_rate_v3 * high_load_multiplier

    # è®¡ç®—æ¯å±‚çš„ç†è®ºè´Ÿè½½
    layer_loads = []
    for i, (w, c) in enumerate(zip(config.arrival_weights, config.layer_capacities)):
        layer_arrival = config.base_arrival_rate * w
        actual_service_rate = config.layer_service_rates[i]
        layer_load = layer_arrival / (c * actual_service_rate)
        layer_loads.append(layer_load)

    print(f"\n{'='*80}")
    print(f"é…ç½®: {config_type}")
    print(f"å®¹é‡: {config.layer_capacities} (æ€»è®¡: {total_capacity})")
    print(f"åˆ°è¾¾æƒé‡: {config.arrival_weights} (å›ºå®šçœŸå®UAMæ¨¡å¼)")
    print(f"æ€»åˆ°è¾¾ç‡: {config.base_arrival_rate:.2f} (v3çš„{high_load_multiplier:.1f}å€)")
    print(f"\nå„å±‚ç†è®ºè´Ÿè½½ (Ï = Î»/(Î¼Â·c)):")
    for i, load in enumerate(layer_loads):
        mu = config.layer_service_rates[i]
        status = "ğŸ”´è¿‡è½½!" if load >= 1.0 else "ğŸŸ¡ä¸´ç•Œ" if load > 0.8 else "ğŸŸ¢æ­£å¸¸"
        print(f"  Layer {i} (å®¹é‡{config.layer_capacities[i]}, Î¼={mu:.1f}): {load*100:.1f}% {status}")
    print(f"å¹³å‡è´Ÿè½½: {np.mean(layer_loads)*100:.1f}%")
    print(f"æœ€é«˜è´Ÿè½½: {np.max(layer_loads)*100:.1f}% (Layer {np.argmax(layer_loads)})")

    # é¢„æµ‹å´©æºƒ
    if any(load >= 1.0 for load in layer_loads):
        print(f"âš ï¸  é¢„è­¦ï¼šé¢„è®¡ç³»ç»Ÿå´©æºƒï¼ˆå­˜åœ¨Ï>=1.0çš„å±‚ï¼‰")
    else:
        print(f"âœ… é¢„è®¡ç³»ç»Ÿç¨³å®šï¼ˆæ‰€æœ‰å±‚Ï<1.0ï¼‰")
    print(f"{'='*80}\n")

    return config


def create_wrapped_env(config):
    """åˆ›å»ºåŒ…è£…åçš„ç¯å¢ƒ"""
    base_env = ConfigurableEnvWrapper(config=config)
    # TD7éœ€è¦SB3DictWrapperæ¥å¤„ç†observation
    wrapped_env = SB3DictWrapper(base_env)
    return wrapped_env


def train_and_evaluate(config_type='inverted_pyramid',
                       timesteps=100000, eval_episodes=50, high_load_multiplier=10.0):
    """è®­ç»ƒå’Œè¯„ä¼°TD7"""

    print(f"\n{'='*80}")
    print(f"å®éªŒ: TD7 + {config_type}")
    print(f"é«˜è´Ÿè½½å€æ•°: {high_load_multiplier}x")
    print(f"è¯„ä¼°è½®æ¬¡: {eval_episodes}")
    print(f"{'='*80}\n")

    config = create_config(config_type, high_load_multiplier)
    env = create_wrapped_env(config)

    save_dir = Path(project_root) / 'Results' / 'ablation_study_final' / config_type
    save_dir.mkdir(parents=True, exist_ok=True)

    start_time = time.time()

    # åˆ›å»ºTD7 agentï¼ˆä½¿ç”¨TD7æ¨èçš„é…ç½®ï¼‰
    td7_config = {
        'embedding_dim': 256,
        'hidden_dim': 256,
        'max_action': 1.0,
        'actor_lr': 3e-4,
        'critic_lr': 3e-4,
        'encoder_lr': 3e-4,
        'gamma': 0.99,
        'tau': 0.005,
        'policy_delay': 2,
        'target_noise': 0.2,
        'noise_clip': 0.5,
        'exploration_noise': 0.1,
        'buffer_size': 1000000,
        'batch_size': 256,
        'alpha': 0.6,
        'beta': 0.4,
        'beta_increment': 0.001,
        'embedding_loss_weight': 1.0,
        'embedding_update_freq': 1,
        'use_checkpoints': True,
        'checkpoint_freq': 10000,
        'max_checkpoints': 5,
        'learning_starts': 25000,
        'train_freq': 1,
        'seed': 42,
        'device': 'cuda'
    }

    agent = TD7_Agent(
        state_space=env.observation_space,
        action_space=env.action_space,
        config=td7_config
    )

    print(f"\nå¼€å§‹è®­ç»ƒTD7...")

    # è®­ç»ƒå¾ªç¯
    state, info = env.reset()
    episode_reward = 0
    episode_length = 0
    episode_count = 0

    for step in range(1, timesteps + 1):
        # é€‰æ‹©åŠ¨ä½œ
        if step < td7_config['learning_starts']:
            action = env.action_space.sample()
        else:
            action = agent.act(state, training=True)

        # æ‰§è¡ŒåŠ¨ä½œ
        next_state, reward, terminated, truncated, info = env.step(action)
        done = terminated or truncated

        episode_reward += reward
        episode_length += 1

        # å­˜å‚¨ç»éªŒ
        agent.store_transition(state, action, reward, next_state, done)

        state = next_state

        # è®­ç»ƒ
        if step >= td7_config['learning_starts']:
            agent.train()

        # Episodeç»“æŸ
        if done:
            episode_count += 1
            if episode_count % 10 == 0:
                print(f"  Episode {episode_count}: Reward = {episode_reward:.2f}, Length = {episode_length}")

            state, info = env.reset()
            episode_reward = 0
            episode_length = 0

        # è¿›åº¦æ˜¾ç¤º
        if step % 10000 == 0:
            print(f"  Training progress: {step}/{timesteps} ({step/timesteps*100:.1f}%)")

    training_time = time.time() - start_time

    # ä¿å­˜æ¨¡å‹
    model_path = save_dir / 'TD7_model.pt'
    agent.save(str(model_path))

    # è¯„ä¼°
    print(f"\nè¯„ä¼° ({eval_episodes} å›åˆ)...")
    eval_rewards = []
    eval_lengths = []
    eval_terminated_count = 0  # çœŸå®å´©æºƒ
    eval_truncated_count = 0   # æ­£å¸¸æˆªæ–­
    eval_waiting_times = []
    eval_utilizations = []

    for ep in range(eval_episodes):
        obs, info = env.reset()
        done = False
        ep_reward = 0
        ep_len = 0
        ep_waiting = []
        ep_utils = []
        episode_terminated = False
        episode_truncated = False

        while not done:
            action = agent.act(obs, training=False)  # ç¡®å®šæ€§ç­–ç•¥
            obs, reward, term, trunc, info = env.step(action)
            done = term or trunc
            ep_reward += reward
            ep_len += 1

            if done:
                episode_terminated = term
                episode_truncated = trunc

            if 'avg_waiting_time' in info:
                ep_waiting.append(info['avg_waiting_time'])
            if 'utilization_rates' in info:
                ep_utils.append(np.mean(info['utilization_rates']))

        eval_rewards.append(ep_reward)
        eval_lengths.append(ep_len)

        if episode_terminated:
            eval_terminated_count += 1
            crash_marker = " ğŸ”´[CRASHED - ç³»ç»Ÿå´©æºƒ!]"
        elif episode_truncated:
            eval_truncated_count += 1
            crash_marker = " âœ…[å®Œæˆ]"
        else:
            crash_marker = ""

        if ep_waiting:
            eval_waiting_times.append(np.mean(ep_waiting))
        if ep_utils:
            eval_utilizations.append(np.mean(ep_utils))

        if (ep + 1) % 10 == 0:
            print(f"  Episode {ep+1}: {ep_reward:.2f} (é•¿åº¦{ep_len}){crash_marker}")

    mean_reward = np.mean(eval_rewards)
    std_reward = np.std(eval_rewards)
    terminated_rate = eval_terminated_count / eval_episodes
    truncated_rate = eval_truncated_count / eval_episodes
    mean_waiting = np.mean(eval_waiting_times) if eval_waiting_times else 0
    mean_util = np.mean(eval_utilizations) if eval_utilizations else 0
    mean_length = np.mean(eval_lengths)

    print(f"\n{'='*80}")
    print(f"è¯„ä¼°ç»“æœ:")
    print(f"  å¹³å‡å¥–åŠ±: {mean_reward:.2f} Â± {std_reward:.2f}")
    print(f"  æœ€ä½³å¥–åŠ±: {np.max(eval_rewards):.2f}")
    print(f"  ğŸ”´ å´©æºƒç‡: {terminated_rate*100:.1f}% ({eval_terminated_count}/{eval_episodes})")
    print(f"  âœ… å®Œæˆç‡: {truncated_rate*100:.1f}% ({eval_truncated_count}/{eval_episodes})")
    print(f"  å¹³å‡å›åˆé•¿åº¦: {mean_length:.1f}")
    print(f"  å¹³å‡ç­‰å¾…: {mean_waiting:.2f} æ—¶é—´æ­¥")
    print(f"  å¹³å‡åˆ©ç”¨ç‡: {mean_util*100:.1f}%")
    print(f"  è®­ç»ƒæ—¶é—´: {training_time/60:.2f}åˆ†é’Ÿ")
    print(f"{'='*80}")

    results = {
        'config_type': config_type,
        'algorithm': 'TD7',
        'layer_capacities': config.layer_capacities,
        'total_capacity': sum(config.layer_capacities),
        'arrival_weights': config.arrival_weights,
        'base_arrival_rate': float(config.base_arrival_rate),
        'high_load_multiplier': high_load_multiplier,
        'mean_reward': float(mean_reward),
        'std_reward': float(std_reward),
        'best_reward': float(np.max(eval_rewards)),
        'worst_reward': float(np.min(eval_rewards)),
        'crash_rate': float(terminated_rate),
        'completion_rate': float(truncated_rate),
        'terminated_count': eval_terminated_count,
        'truncated_count': eval_truncated_count,
        'mean_episode_length': float(mean_length),
        'mean_waiting_time': float(mean_waiting),
        'mean_utilization': float(mean_util),
        'training_time_minutes': float(training_time / 60),
        'eval_rewards': [float(r) for r in eval_rewards],
        'eval_lengths': [int(l) for l in eval_lengths],
        'timestamp': datetime.now().isoformat()
    }

    results_path = save_dir / 'TD7_results.json'
    with open(results_path, 'w') as f:
        json.dump(results, f, indent=2)

    env.close()
    return results


if __name__ == '__main__':
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument('--config', required=True,
                       choices=['inverted_pyramid', 'uniform', 'high_capacity',
                               'reverse_pyramid', 'low_capacity'])
    parser.add_argument('--timesteps', type=int, default=100000)
    parser.add_argument('--eval-episodes', type=int, default=50)
    parser.add_argument('--high-load-multiplier', type=float, default=10.0,
                       help='é«˜è´Ÿè½½å€æ•°ï¼ˆç›¸å¯¹v3ï¼‰')
    args = parser.parse_args()

    try:
        result = train_and_evaluate(args.config,
                                   args.timesteps, args.eval_episodes, args.high_load_multiplier)
        print(f"\nâœ… å®Œæˆ: {result['mean_reward']:.2f} Â± {result['std_reward']:.2f}")
        print(f"ğŸ”´ å´©æºƒç‡: {result['crash_rate']*100:.1f}%")
    except Exception as e:
        print(f"\nâŒ å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
