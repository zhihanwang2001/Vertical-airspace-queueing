"""
Major Revision Experiment 1.1: Extended Training for K=30
éªŒè¯å®¹é‡æ‚–è®ºæ˜¯ç³»ç»Ÿå›ºæœ‰ç‰¹æ€§è¿˜æ˜¯è®­ç»ƒé¢„ç®—ä¸è¶³

å…³é”®é—®é¢˜ï¼š
- è®ºæ–‡å£°ç§° K=30 åœ¨ 100K æ­¥è®­ç»ƒåå´©æºƒï¼ˆ100%å´©æºƒç‡ï¼‰
- è¯„å®¡è´¨ç–‘ï¼šå¯èƒ½åªæ˜¯è®­ç»ƒä¸è¶³ï¼Œè€Œéç³»ç»Ÿå›ºæœ‰é—®é¢˜

å®éªŒè®¾è®¡ï¼š
1. K=30 (uniform [6,6,6,6,6]) è®­ç»ƒ 1M æ­¥ï¼ˆvs åŸæ¥çš„100Kï¼‰
2. K=23 (inverted pyramid) è®­ç»ƒ 1M æ­¥ä½œä¸ºå¯¹ç…§
3. K=10 (low capacity) è®­ç»ƒ 1M æ­¥ä½œä¸ºåŸºå‡†

ç®—æ³•ï¼šA2C, PPOï¼ˆåŸè®ºæ–‡ä¸»è¦ç®—æ³•ï¼‰
æ¯ä¸ªé…ç½®ï¼š3 seeds
è¯„ä¼°ï¼šæ¯10Kæ­¥è¯„ä¼°ä¸€æ¬¡ï¼Œä½¿ç”¨ T=200 ç»Ÿä¸€åè®®

é¢„æœŸç»“æœï¼š
- Best case: K=30 ä»ç„¶å´©æºƒ â†’ å®¹é‡æ‚–è®ºæ˜¯çœŸå®çš„
- Worst case: K=30 æˆåŠŸæ”¶æ•› â†’ å®¹é‡æ‚–è®ºæ˜¯è®­ç»ƒä¸è¶³
- Most likely: K=30 éƒ¨åˆ†æ”¹å–„ä½†ä»å·®äºK=10 â†’ nuanced conclusion
"""

import sys
import os
from pathlib import Path

project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

import gymnasium as gym
import numpy as np
import json
import time
from datetime import datetime
from stable_baselines3 import A2C, PPO
from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback

from env.config import VerticalQueueConfig
from env.configurable_env_wrapper import ConfigurableEnvWrapper
from env.drl_wrapper_fixed import DictToBoxActionWrapperFixed, ObservationWrapperFixed


def create_config(capacity_type='k30_uniform', high_load_multiplier=10.0):
    """
    åˆ›å»ºé…ç½®

    capacity_type:
    - 'k30_uniform': [6,6,6,6,6] æ€»30
    - 'k23_inverted': [8,6,4,3,2] æ€»23 (baseline)
    - 'k10_low': [2,2,2,2,2] æ€»10 (best performer in original)
    """
    config = VerticalQueueConfig()

    if capacity_type == 'k30_uniform':
        config.layer_capacities = [6, 6, 6, 6, 6]  # æ€»30
        name = "K=30 Uniform"
    elif capacity_type == 'k23_inverted':
        config.layer_capacities = [8, 6, 4, 3, 2]  # æ€»23
        name = "K=23 Inverted Pyramid"
    elif capacity_type == 'k10_low':
        config.layer_capacities = [2, 2, 2, 2, 2]  # æ€»10
        name = "K=10 Low Capacity"
    else:
        raise ValueError(f"Unknown capacity type: {capacity_type}")

    # å›ºå®šUAMæµé‡æ¨¡å¼ï¼ˆåŸè®ºæ–‡è®¾å®šï¼‰
    config.arrival_weights = [0.3, 0.25, 0.2, 0.15, 0.1]

    # 10Ã— é«˜è´Ÿè½½
    total_capacity = sum(config.layer_capacities)
    avg_service_rate = np.mean(config.layer_service_rates)
    base_rate_v3 = 0.75 * total_capacity * avg_service_rate / 5
    config.base_arrival_rate = base_rate_v3 * high_load_multiplier

    # è®¡ç®—ç†è®ºè´Ÿè½½
    layer_loads = []
    for i, (w, c) in enumerate(zip(config.arrival_weights, config.layer_capacities)):
        layer_arrival = config.base_arrival_rate * w
        actual_service_rate = config.layer_service_rates[i]
        layer_load = layer_arrival / (c * actual_service_rate)
        layer_loads.append(layer_load)

    print(f"\n{'='*80}")
    print(f"é…ç½®: {name}")
    print(f"å®¹é‡: {config.layer_capacities} (æ€»è®¡: {total_capacity})")
    print(f"åˆ°è¾¾æƒé‡: {config.arrival_weights}")
    print(f"æ€»åˆ°è¾¾ç‡: {config.base_arrival_rate:.2f}")
    print(f"\nå„å±‚ç†è®ºè´Ÿè½½:")
    for i, load in enumerate(layer_loads):
        status = "ğŸ”´" if load >= 1.0 else "ğŸŸ¡" if load > 0.8 else "ğŸŸ¢"
        print(f"  L{i}: {load*100:.1f}% {status}")
    print(f"å¹³å‡è´Ÿè½½: {np.mean(layer_loads)*100:.1f}%")
    print(f"æœ€å¤§è´Ÿè½½: {np.max(layer_loads)*100:.1f}%")
    print(f"{'='*80}\n")

    return config, name


def create_env(config):
    """åˆ›å»ºç¯å¢ƒ"""
    base_env = ConfigurableEnvWrapper(config)
    wrapped_env = DictToBoxActionWrapperFixed(base_env)
    env = ObservationWrapperFixed(wrapped_env)
    return env


def train_and_evaluate(
    algo_name='A2C',
    capacity_type='k30_uniform',
    seed=42,
    total_timesteps=1_000_000,  # 1M steps (vs original 100K)
    eval_freq=10_000,  # æ¯10Kè¯„ä¼°
    n_eval_episodes=10
):
    """
    è®­ç»ƒå¹¶è¯„ä¼°

    å…³é”®å‚æ•°ï¼š
    - total_timesteps: 1M (10Ã— original)
    - eval_freq: 10K (vs original 5K)
    - max_episode_steps: 200 (ç»Ÿä¸€åè®®ï¼Œä¸åŸè®ºæ–‡A2C/PPOä¸€è‡´)
    """

    # åˆ›å»ºé…ç½®
    config, config_name = create_config(capacity_type)

    # è®¾ç½®è¾“å‡ºç›®å½•
    output_dir = Path(f"Results/major_revision_exp1/{capacity_type}/{algo_name}_seed{seed}")
    output_dir.mkdir(parents=True, exist_ok=True)

    print(f"\n{'='*80}")
    print(f"Training: {algo_name} on {config_name}")
    print(f"Seed: {seed}")
    print(f"Total steps: {total_timesteps:,}")
    print(f"Output: {output_dir}")
    print(f"{'='*80}\n")

    # åˆ›å»ºè®­ç»ƒå’Œè¯„ä¼°ç¯å¢ƒ
    train_env = create_env(config)
    eval_env = create_env(config)

    # è®¾ç½®episodeé•¿åº¦ï¼ˆç»Ÿä¸€åè®®ï¼‰
    train_env.env.env._max_episode_steps = 1000  # è®­ç»ƒæ—¶è¾ƒé•¿
    eval_env.env.env._max_episode_steps = 200    # è¯„ä¼°æ—¶ç»Ÿä¸€T=200

    # åˆ›å»ºç®—æ³•
    if algo_name == 'A2C':
        # ä½¿ç”¨åŸè®ºæ–‡çš„staged learning rate
        # ä½†ç”±äºæ˜¯1Mæ­¥ï¼Œè°ƒæ•´transition point
        model = A2C(
            "MlpPolicy",
            train_env,
            learning_rate=7e-4,  # åˆå§‹é«˜å­¦ä¹ ç‡
            n_steps=32,
            gamma=0.99,
            gae_lambda=0.95,
            ent_coef=0.01,
            vf_coef=0.5,
            max_grad_norm=0.5,
            policy_kwargs=dict(net_arch=[512, 512, 256]),
            verbose=1,
            seed=seed,
            device='auto'
        )
    elif algo_name == 'PPO':
        model = PPO(
            "MlpPolicy",
            train_env,
            learning_rate=3e-4,
            n_steps=2048,
            batch_size=64,
            n_epochs=10,
            gamma=0.99,
            gae_lambda=0.95,
            clip_range=0.2,
            ent_coef=0.01,
            policy_kwargs=dict(net_arch=[256, 256]),
            verbose=1,
            seed=seed,
            device='auto'
        )
    else:
        raise ValueError(f"Unknown algorithm: {algo_name}")

    # åˆ›å»ºè¯„ä¼°å›è°ƒ
    eval_callback = EvalCallback(
        eval_env,
        best_model_save_path=str(output_dir / "best_model"),
        log_path=str(output_dir / "eval_logs"),
        eval_freq=eval_freq,
        n_eval_episodes=n_eval_episodes,
        deterministic=True,
        render=False,
        verbose=1
    )

    # åˆ›å»ºcheckpointå›è°ƒï¼ˆæ¯50Kä¿å­˜ï¼‰
    checkpoint_callback = CheckpointCallback(
        save_freq=50_000,
        save_path=str(output_dir / "checkpoints"),
        name_prefix=f"{algo_name}_checkpoint"
    )

    # è®­ç»ƒ
    print(f"\n{'='*80}")
    print(f"å¼€å§‹è®­ç»ƒ...")
    print(f"{'='*80}\n")

    start_time = time.time()

    try:
        model.learn(
            total_timesteps=total_timesteps,
            callback=[eval_callback, checkpoint_callback],
            progress_bar=True
        )

        training_time = time.time() - start_time

        print(f"\n{'='*80}")
        print(f"è®­ç»ƒå®Œæˆï¼")
        print(f"è€—æ—¶: {training_time/60:.1f} åˆ†é’Ÿ")
        print(f"{'='*80}\n")

        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        model.save(output_dir / "final_model")

        # æœ€ç»ˆè¯„ä¼°ï¼ˆT=200ï¼‰
        print(f"\n{'='*80}")
        print(f"æœ€ç»ˆè¯„ä¼° (T=200, {n_eval_episodes} episodes)...")
        print(f"{'='*80}\n")

        eval_env.env.env._max_episode_steps = 200

        episode_rewards = []
        episode_lengths = []
        crash_count = 0

        for ep in range(n_eval_episodes):
            obs, _ = eval_env.reset()
            episode_reward = 0
            episode_length = 0
            done = False

            while not done:
                action, _ = model.predict(obs, deterministic=True)
                obs, reward, terminated, truncated, info = eval_env.step(action)
                done = terminated or truncated
                episode_reward += reward
                episode_length += 1

                if terminated and episode_length < 200:
                    crash_count += 1
                    break

            episode_rewards.append(episode_reward)
            episode_lengths.append(episode_length)

            print(f"  Episode {ep+1}: Reward={episode_reward:.1f}, Length={episode_length}")

        # è®¡ç®—ç»Ÿè®¡
        mean_reward = np.mean(episode_rewards)
        std_reward = np.std(episode_rewards)
        mean_length = np.mean(episode_lengths)
        crash_rate = crash_count / n_eval_episodes * 100
        completion_rate = 100 - crash_rate

        results = {
            'algorithm': algo_name,
            'capacity_type': capacity_type,
            'config_name': config_name,
            'seed': seed,
            'total_timesteps': total_timesteps,
            'training_time_minutes': training_time / 60,

            'final_eval': {
                'episode_steps': 200,
                'n_episodes': n_eval_episodes,
                'mean_reward': float(mean_reward),
                'std_reward': float(std_reward),
                'mean_episode_length': float(mean_length),
                'crash_rate': float(crash_rate),
                'completion_rate': float(completion_rate),
                'all_rewards': [float(r) for r in episode_rewards],
                'all_lengths': [int(l) for l in episode_lengths]
            },

            'metadata': {
                'timestamp': datetime.now().isoformat(),
                'purpose': 'Major Revision Exp 1.1: Extended Training',
                'hypothesis_test': 'Capacity paradox: inherent vs training budget'
            }
        }

        # ä¿å­˜ç»“æœ
        results_file = output_dir / "results.json"
        with open(results_file, 'w') as f:
            json.dump(results, f, indent=2)

        print(f"\n{'='*80}")
        print(f"æœ€ç»ˆç»“æœ:")
        print(f"  å¹³å‡å¥–åŠ±: {mean_reward:.1f} Â± {std_reward:.1f}")
        print(f"  å¹³å‡é•¿åº¦: {mean_length:.1f}")
        print(f"  å´©æºƒç‡: {crash_rate:.1f}%")
        print(f"  å®Œæˆç‡: {completion_rate:.1f}%")
        print(f"{'='*80}\n")

        print(f"ç»“æœå·²ä¿å­˜åˆ°: {results_file}")

        return results

    except Exception as e:
        print(f"\nâŒ è®­ç»ƒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None

    finally:
        train_env.close()
        eval_env.close()


def main():
    """
    ä¸»å‡½æ•°ï¼šè¿è¡Œæ‰€æœ‰é…ç½®

    ä¼˜å…ˆçº§ï¼š
    1. K=30 (å…³é”®) - éªŒè¯å®¹é‡æ‚–è®º
    2. K=23 (å¯¹ç…§) - ç¡®è®¤æ‰©å±•è®­ç»ƒä¸ç ´åå·²çŸ¥ç»“æœ
    3. K=10 (åŸºå‡†) - éªŒè¯æœ€ä¼˜é…ç½®æ˜¯å¦è¿›ä¸€æ­¥æ”¹å–„
    """

    configurations = [
        # æœ€å…³é”®ï¼šK=30
        ('A2C', 'k30_uniform', 42),
        ('A2C', 'k30_uniform', 123),
        ('A2C', 'k30_uniform', 456),

        ('PPO', 'k30_uniform', 42),
        ('PPO', 'k30_uniform', 123),
        ('PPO', 'k30_uniform', 456),

        # å¯¹ç…§ï¼šK=23
        ('A2C', 'k23_inverted', 42),
        ('A2C', 'k23_inverted', 123),

        ('PPO', 'k23_inverted', 42),
        ('PPO', 'k23_inverted', 123),

        # åŸºå‡†ï¼šK=10
        ('A2C', 'k10_low', 42),
        ('PPO', 'k10_low', 42),
    ]

    print(f"\n{'#'*80}")
    print(f"# Major Revision Experiment 1.1: Extended Training")
    print(f"# Total configurations: {len(configurations)}")
    print(f"# Estimated time: ~{len(configurations) * 2} hours (parallelå¯ä»¥å‡å°‘)")
    print(f"{'#'*80}\n")

    all_results = []

    for i, (algo, capacity, seed) in enumerate(configurations):
        print(f"\n{'#'*80}")
        print(f"# Configuration {i+1}/{len(configurations)}")
        print(f"{'#'*80}\n")

        result = train_and_evaluate(
            algo_name=algo,
            capacity_type=capacity,
            seed=seed,
            total_timesteps=1_000_000,
            eval_freq=10_000,
            n_eval_episodes=10
        )

        if result:
            all_results.append(result)

    # ä¿å­˜æ±‡æ€»ç»“æœ
    summary_file = Path("Results/major_revision_exp1/summary.json")
    summary_file.parent.mkdir(parents=True, exist_ok=True)

    with open(summary_file, 'w') as f:
        json.dump(all_results, f, indent=2)

    print(f"\n{'#'*80}")
    print(f"# æ‰€æœ‰å®éªŒå®Œæˆï¼")
    print(f"# æ±‡æ€»ç»“æœ: {summary_file}")
    print(f"{'#'*80}\n")

    # å¿«é€Ÿåˆ†æ
    print("\nå¿«é€Ÿåˆ†æ:")
    print("="*80)

    for capacity_type in ['k30_uniform', 'k23_inverted', 'k10_low']:
        relevant = [r for r in all_results if r['capacity_type'] == capacity_type]
        if not relevant:
            continue

        print(f"\n{relevant[0]['config_name']}:")

        for algo in ['A2C', 'PPO']:
            algo_results = [r for r in relevant if r['algorithm'] == algo]
            if not algo_results:
                continue

            rewards = [r['final_eval']['mean_reward'] for r in algo_results]
            crash_rates = [r['final_eval']['crash_rate'] for r in algo_results]

            print(f"  {algo}:")
            print(f"    Reward: {np.mean(rewards):.1f} Â± {np.std(rewards):.1f}")
            print(f"    Crash:  {np.mean(crash_rates):.1f}%")

    print("\n" + "="*80)
    print("åˆ†æå®Œæˆï¼è¯·æŸ¥çœ‹è¯¦ç»†ç»“æœè¿›è¡Œè®ºæ–‡ä¿®è®¢ã€‚")


if __name__ == "__main__":
    main()
