"""
Top 3æ¨¡å‹è·¨åŒºåŸŸæ³›åŒ–æ€§æµ‹è¯•è„šæœ¬
Top 3 Models Cross-Region Generalization Test Script

ğŸ¯ æ ¸å¿ƒç›®æ ‡ï¼šéªŒè¯Top 3æ¨¡å‹ï¼ˆA2C, PPO, TD7ï¼‰åœ¨ä¸åŒå¼‚è´¨æ€§åŒºåŸŸçš„æ³›åŒ–èƒ½åŠ›
âš ï¸  é‡è¦ï¼šè¿™ä¸æ˜¯mockæµ‹è¯•ï¼Œä½¿ç”¨çœŸå®çš„ç¯å¢ƒè¿è¡Œå’Œæ¨¡å‹æ¨ç†ï¼

æµ‹è¯•é€»è¾‘ï¼š
1. åŠ è½½å·²è®­ç»ƒçš„3ä¸ªæ¨¡å‹
   - A2C: ./models/a2c/a2c_model_500000.pth (4392.86 Â± 145.42)
   - PPO: ./models/ppo/ppo_model_500000.pth (4419.98 Â± 135.71)
   - TD7: ./models/td7/td7_model_500000.pt  (4351.84 from RP1)
2. åœ¨5ä¸ªä¸åŒçš„heterogeneous regionä¸­æµ‹è¯•
3. æ¯ä¸ªregionè¿è¡Œ10ä¸ªepisodeè·å–çœŸå®æ€§èƒ½æ•°æ®
4. è®°å½•è¯¦ç»†çš„æ€§èƒ½æŒ‡æ ‡å’Œç¯å¢ƒé…ç½®
5. å¯¹æ¯”3ä¸ªæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'rpTransition'))

import numpy as np
import json
from pathlib import Path
from typing import Dict, List
import time

# å¯¼å…¥åŸºçº¿ç®—æ³•
from algorithms.baselines.sb3_a2c_baseline import SB3A2CBaseline
from algorithms.baselines.sb3_ppo_baseline import SB3PPOBaseline
from algorithms.advanced.td7.td7_baseline import TD7Baseline

# å¯¼å…¥ç¯å¢ƒå’Œé…ç½®
from env.configurable_env_wrapper import ConfigurableEnvWrapper
from algorithms.baselines.space_utils import SB3DictWrapper

# å¯¼å…¥å¼‚è´¨æ€§é…ç½®ç”Ÿæˆå™¨
import importlib.util
spec = importlib.util.spec_from_file_location(
    "heterogeneous_configs",
    os.path.join(os.path.dirname(__file__), '..', 'rpTransition', 'heterogeneous_configs.py')
)
heterogeneous_configs = importlib.util.module_from_spec(spec)
spec.loader.exec_module(heterogeneous_configs)

HeterogeneousRegionConfigs = heterogeneous_configs.HeterogeneousRegionConfigs


def test_model_in_region(model, model_type: str, config, region_name: str,
                         n_episodes: int = 10, verbose: bool = True):
    """
    åœ¨æŒ‡å®šåŒºåŸŸæµ‹è¯•æ¨¡å‹

    Args:
        model: å·²åŠ è½½æ¨¡å‹çš„baselineå®ä¾‹
        model_type: æ¨¡å‹ç±»å‹ ('A2C', 'PPO', 'TD7')
        config: VerticalQueueConfigé…ç½®
        region_name: åŒºåŸŸåç§°
        n_episodes: æµ‹è¯•episodeæ•°é‡
        verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯

    Returns:
        dict: æµ‹è¯•ç»“æœ
    """
    if verbose:
        print(f"\n{'='*80}")
        print(f"æµ‹è¯•: {model_type} @ {region_name}")
        print(f"{'='*80}")

    # åˆ›å»ºè¯¥åŒºåŸŸçš„ç¯å¢ƒ
    base_env = ConfigurableEnvWrapper(config)
    eval_env = SB3DictWrapper(base_env)

    # è®°å½•ç»“æœ
    episode_rewards = []
    episode_lengths = []
    episode_details = []

    # è¿è¡Œn_episodesä¸ªepisode
    for episode in range(n_episodes):
        obs, info = eval_env.reset()
        episode_reward = 0
        episode_length = 0
        done = False

        # è¿è¡Œä¸€ä¸ªå®Œæ•´çš„episode
        while not done:
            # æ ¹æ®æ¨¡å‹ç±»å‹é€‰æ‹©é¢„æµ‹æ–¹æ³•
            if model_type == 'TD7':
                action = model.agent.act(obs, training=False)
            else:  # A2C or PPO
                action, _ = model.model.predict(obs, deterministic=True)

            # æ‰§è¡ŒåŠ¨ä½œ
            obs, reward, terminated, truncated, info = eval_env.step(action)
            done = terminated or truncated

            episode_reward += reward
            episode_length += 1

            # é˜²æ­¢æ— é™å¾ªç¯
            if episode_length >= 1000:
                if verbose:
                    print(f"  âš ï¸  Episode {episode+1} è¾¾åˆ°æœ€å¤§æ­¥æ•°é™åˆ¶ (1000)")
                break

        # è®°å½•ç»“æœ
        episode_rewards.append(episode_reward)
        episode_lengths.append(episode_length)
        episode_details.append({
            'episode': episode + 1,
            'reward': float(episode_reward),
            'length': int(episode_length)
        })

        if verbose:
            print(f"  Episode {episode+1}/{n_episodes}: Reward = {episode_reward:.2f}, Length = {episode_length}")

    # è®¡ç®—ç»Ÿè®¡ç»“æœ
    mean_reward = np.mean(episode_rewards)
    std_reward = np.std(episode_rewards)
    mean_length = np.mean(episode_lengths)

    results = {
        'model_type': model_type,
        'region_name': region_name,
        'n_episodes': n_episodes,
        'mean_reward': float(mean_reward),
        'std_reward': float(std_reward),
        'mean_length': float(mean_length),
        'episode_rewards': [float(r) for r in episode_rewards],
        'episode_lengths': [int(l) for l in episode_lengths],
        'episode_details': episode_details,
        'config_summary': base_env.get_config_summary()
    }

    if verbose:
        print(f"\nğŸ“Š {model_type} @ {region_name} æµ‹è¯•ç»“æœ:")
        print(f"   å¹³å‡å¥–åŠ±: {mean_reward:.2f} Â± {std_reward:.2f}")
        print(f"   å¹³å‡é•¿åº¦: {mean_length:.1f}")

    # æ¸…ç†ç¯å¢ƒ
    eval_env.close()

    return results


def main():
    """ä¸»å‡½æ•°ï¼šæµ‹è¯•æ‰€æœ‰3ä¸ªæ¨¡å‹åœ¨æ‰€æœ‰å¼‚è´¨æ€§åŒºåŸŸçš„æ³›åŒ–æ€§èƒ½"""

    print("\n" + "="*80)
    print("Top 3 æ¨¡å‹è·¨åŒºåŸŸæ³›åŒ–æ€§æµ‹è¯•")
    print("Cross-Region Generalization Test for Top 3 Models (A2C, PPO, TD7)")
    print("="*80 + "\n")

    # ========== ç¬¬1æ­¥ï¼šåŠ è½½è®­ç»ƒå¥½çš„3ä¸ªæ¨¡å‹ ==========
    print("ç¬¬1æ­¥ï¼šåŠ è½½è®­ç»ƒå¥½çš„3ä¸ªæ¨¡å‹")
    print("-"*80)

    models = {}
    model_paths = {
        'A2C': '../../Models/a2c/a2c_model_500000',
        'PPO': '../../Models/ppo/ppo_model_500000',
        'TD7': '../../Models/td7/td7_model_500000.pt'
    }

    # åŠ è½½A2C
    print("\n1.1 åŠ è½½A2Cæ¨¡å‹...")
    if not os.path.exists(model_paths['A2C'] + '.pth'):
        print(f"âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°A2Cæ¨¡å‹æ–‡ä»¶ {model_paths['A2C']}.pth")
        return

    a2c = SB3A2CBaseline()
    a2c.load(model_paths['A2C'])
    models['A2C'] = a2c
    print("âœ… A2Cæ¨¡å‹åŠ è½½æˆåŠŸï¼")

    # åŠ è½½PPO
    print("\n1.2 åŠ è½½PPOæ¨¡å‹...")
    if not os.path.exists(model_paths['PPO'] + '.pth'):
        print(f"âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°PPOæ¨¡å‹æ–‡ä»¶ {model_paths['PPO']}.pth")
        return

    ppo = SB3PPOBaseline()
    ppo.load(model_paths['PPO'])
    models['PPO'] = ppo
    print("âœ… PPOæ¨¡å‹åŠ è½½æˆåŠŸï¼")

    # åŠ è½½TD7
    print("\n1.3 åŠ è½½TD7æ¨¡å‹...")
    if not os.path.exists(model_paths['TD7']):
        print(f"âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°TD7æ¨¡å‹æ–‡ä»¶ {model_paths['TD7']}")
        return

    print(f"ğŸ“‚ æ¨¡å‹æ–‡ä»¶å¤§å°: {os.path.getsize(model_paths['TD7']) / (1024*1024):.1f} MB")
    td7 = TD7Baseline()
    td7.load(model_paths['TD7'])
    models['TD7'] = td7
    print("âœ… TD7æ¨¡å‹åŠ è½½æˆåŠŸï¼")

    print("\nâœ… æ‰€æœ‰3ä¸ªæ¨¡å‹åŠ è½½å®Œæˆï¼")

    # ========== ç¬¬2æ­¥ï¼šåˆ›å»ºå¼‚è´¨æ€§åŒºåŸŸé…ç½® ==========
    print("\nç¬¬2æ­¥ï¼šåˆ›å»ºå¼‚è´¨æ€§åŒºåŸŸé…ç½®")
    print("-"*80)

    config_generator = HeterogeneousRegionConfigs()
    all_configs = config_generator.get_all_configs()

    print(f"âœ… å·²åˆ›å»º {len(all_configs)} ä¸ªåŒºåŸŸé…ç½®:")
    for region_name in all_configs.keys():
        print(f"   - {region_name}")

    # ========== ç¬¬3æ­¥ï¼šåœ¨æ¯ä¸ªåŒºåŸŸè¿è¡Œæµ‹è¯• ==========
    print("\nç¬¬3æ­¥ï¼šåœ¨æ¯ä¸ªåŒºåŸŸè¿è¡Œæ³›åŒ–æµ‹è¯•")
    print("-"*80)
    print("âš ï¸  è¿™æ˜¯çœŸå®æµ‹è¯•ï¼Œä¸æ˜¯mockæ•°æ®ï¼")
    print(f"   æ€»æµ‹è¯•æ•°: {len(models)} æ¨¡å‹ Ã— {len(all_configs)} åŒºåŸŸ Ã— 10 episodes = {len(models) * len(all_configs) * 10} episodes")

    all_results = {
        'A2C': {},
        'PPO': {},
        'TD7': {}
    }

    n_episodes_per_region = 10
    start_time = time.time()

    # å¯¹æ¯ä¸ªæ¨¡å‹å’Œæ¯ä¸ªåŒºåŸŸè¿è¡Œæµ‹è¯•
    for model_name in ['A2C', 'PPO', 'TD7']:
        print(f"\n{'='*80}")
        print(f"å¼€å§‹æµ‹è¯• {model_name} æ¨¡å‹")
        print(f"{'='*80}")

        model = models[model_name]

        for region_name, config in all_configs.items():
            results = test_model_in_region(
                model=model,
                model_type=model_name,
                config=config,
                region_name=region_name,
                n_episodes=n_episodes_per_region,
                verbose=True
            )
            all_results[model_name][region_name] = results

    total_time = time.time() - start_time

    # ========== ç¬¬4æ­¥ï¼šæ±‡æ€»ç»“æœ ==========
    print("\n" + "="*80)
    print("æµ‹è¯•å®Œæˆï¼æ±‡æ€»ç»“æœ")
    print("="*80 + "\n")

    # æ‰“å°å„æ¨¡å‹åœ¨å„åŒºåŸŸçš„æ€§èƒ½å¯¹æ¯”
    print(f"{'åŒºåŸŸ':<30} {'A2C':<20} {'PPO':<20} {'TD7':<20}")
    print("-"*90)

    baseline_rewards = {}  # è®°å½•å„æ¨¡å‹åœ¨baseline regionçš„æ€§èƒ½

    for region_name in all_configs.keys():
        a2c_reward = all_results['A2C'][region_name]['mean_reward']
        ppo_reward = all_results['PPO'][region_name]['mean_reward']
        td7_reward = all_results['TD7'][region_name]['mean_reward']

        # è®°å½•baselineæ€§èƒ½
        if 'Standard' in region_name:
            baseline_rewards['A2C'] = a2c_reward
            baseline_rewards['PPO'] = ppo_reward
            baseline_rewards['TD7'] = td7_reward

        print(f"{region_name:<30} {a2c_reward:<20.2f} {ppo_reward:<20.2f} {td7_reward:<20.2f}")

    # æ‰“å°æ€§èƒ½ä¸‹é™ç™¾åˆ†æ¯”
    print("\n" + "="*80)
    print("æ€§èƒ½ä¸‹é™ç™¾åˆ†æ¯” (ç›¸å¯¹äºRegion A Baseline)")
    print("="*80 + "\n")

    print(f"{'åŒºåŸŸ':<30} {'A2C':<20} {'PPO':<20} {'TD7':<20}")
    print("-"*90)

    for region_name in all_configs.keys():
        if 'Standard' in region_name:
            print(f"{region_name:<30} {'0.0%':<20} {'0.0%':<20} {'0.0%':<20}")
        else:
            a2c_diff = ((all_results['A2C'][region_name]['mean_reward'] - baseline_rewards['A2C'])
                       / baseline_rewards['A2C'] * 100)
            ppo_diff = ((all_results['PPO'][region_name]['mean_reward'] - baseline_rewards['PPO'])
                       / baseline_rewards['PPO'] * 100)
            td7_diff = ((all_results['TD7'][region_name]['mean_reward'] - baseline_rewards['TD7'])
                       / baseline_rewards['TD7'] * 100)

            print(f"{region_name:<30} {a2c_diff:+.1f}%{' ':<15} {ppo_diff:+.1f}%{' ':<15} {td7_diff:+.1f}%")

    print("\n" + "-"*80)
    print(f"æ€»æµ‹è¯•æ—¶é—´: {total_time:.1f}ç§’ ({total_time/60:.1f}åˆ†é’Ÿ)")
    print(f"æ€»episodeæ•°: {len(models) * len(all_configs) * n_episodes_per_region}")

    # ========== ç¬¬5æ­¥ï¼šä¿å­˜ç»“æœ ==========
    print("\nç¬¬5æ­¥ï¼šä¿å­˜æµ‹è¯•ç»“æœ")
    print("-"*80)

    # åˆ›å»ºä¿å­˜ç›®å½•
    save_dir = Path("../../Results/generalization")
    save_dir.mkdir(exist_ok=True)

    # ä¿å­˜è¯¦ç»†ç»“æœ
    results_file = save_dir / "all_models_generalization_results.json"

    full_results = {
        'test_info': {
            'test_date': time.strftime("%Y-%m-%d %H:%M:%S"),
            'n_episodes_per_region': n_episodes_per_region,
            'total_time_seconds': total_time,
            'models_tested': ['A2C', 'PPO', 'TD7'],
            'regions_tested': list(all_configs.keys())
        },
        'model_paths': model_paths,
        'baseline_performance': {
            'A2C': f"{baseline_rewards['A2C']:.2f}",
            'PPO': f"{baseline_rewards['PPO']:.2f}",
            'TD7': f"{baseline_rewards['TD7']:.2f}"
        },
        'results': all_results
    }

    with open(results_file, 'w', encoding='utf-8') as f:
        json.dump(full_results, f, indent=2, ensure_ascii=False)

    print(f"âœ… è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {results_file}")

    # ä¿å­˜æ±‡æ€»è¡¨æ ¼ï¼ˆCSVæ ¼å¼ï¼‰
    summary_file = save_dir / "all_models_generalization_summary.csv"
    import csv

    with open(summary_file, 'w', newline='', encoding='utf-8') as f:
        writer = csv.writer(f)
        writer.writerow(['Region', 'A2C Mean', 'A2C Std', 'PPO Mean', 'PPO Std',
                        'TD7 Mean', 'TD7 Std', 'Best Model'])

        for region_name in all_configs.keys():
            a2c_res = all_results['A2C'][region_name]
            ppo_res = all_results['PPO'][region_name]
            td7_res = all_results['TD7'][region_name]

            # æ‰¾å‡ºæœ€ä½³æ¨¡å‹
            best_reward = max(a2c_res['mean_reward'], ppo_res['mean_reward'], td7_res['mean_reward'])
            if a2c_res['mean_reward'] == best_reward:
                best_model = 'A2C'
            elif ppo_res['mean_reward'] == best_reward:
                best_model = 'PPO'
            else:
                best_model = 'TD7'

            writer.writerow([
                region_name,
                f"{a2c_res['mean_reward']:.2f}",
                f"{a2c_res['std_reward']:.2f}",
                f"{ppo_res['mean_reward']:.2f}",
                f"{ppo_res['std_reward']:.2f}",
                f"{td7_res['mean_reward']:.2f}",
                f"{td7_res['std_reward']:.2f}",
                best_model
            ])

    print(f"âœ… æ±‡æ€»è¡¨æ ¼å·²ä¿å­˜åˆ°: {summary_file}")

    # ä¿å­˜å„æ¨¡å‹çš„æ³›åŒ–æ€§è¯„åˆ†
    generalization_file = save_dir / "generalization_ranking.txt"

    with open(generalization_file, 'w', encoding='utf-8') as f:
        f.write("="*80 + "\n")
        f.write("è·¨åŒºåŸŸæ³›åŒ–æ€§èƒ½æ’å\n")
        f.write("Cross-Region Generalization Performance Ranking\n")
        f.write("="*80 + "\n\n")

        # è®¡ç®—å¹³å‡æ€§èƒ½ä¸‹é™
        avg_drop = {}
        for model_name in ['A2C', 'PPO', 'TD7']:
            drops = []
            for region_name in all_configs.keys():
                if 'Standard' not in region_name:
                    reward = all_results[model_name][region_name]['mean_reward']
                    baseline = baseline_rewards[model_name]
                    drop_pct = ((reward - baseline) / baseline) * 100
                    drops.append(drop_pct)
            avg_drop[model_name] = np.mean(drops)

        # æ’åºï¼ˆä¸‹é™è¶Šå°è¶Šå¥½ï¼‰
        ranking = sorted(avg_drop.items(), key=lambda x: x[1], reverse=True)

        f.write("å¹³å‡æ€§èƒ½ä¸‹é™ (è¶Šå°è¶Šå¥½ï¼Œè¡¨ç¤ºæ³›åŒ–èƒ½åŠ›è¶Šå¼º):\n")
        f.write("-"*80 + "\n")
        for rank, (model_name, drop) in enumerate(ranking, 1):
            f.write(f"{rank}. {model_name}: {drop:+.2f}%\n")

        f.write("\n" + "="*80 + "\n")
        f.write("å„åŒºåŸŸæœ€ä½³æ¨¡å‹:\n")
        f.write("-"*80 + "\n")

        for region_name in all_configs.keys():
            rewards = {
                'A2C': all_results['A2C'][region_name]['mean_reward'],
                'PPO': all_results['PPO'][region_name]['mean_reward'],
                'TD7': all_results['TD7'][region_name]['mean_reward']
            }
            best = max(rewards.items(), key=lambda x: x[1])
            f.write(f"{region_name}: {best[0]} ({best[1]:.2f})\n")

    print(f"âœ… æ³›åŒ–æ€§æ’åå·²ä¿å­˜åˆ°: {generalization_file}")

    print("\n" + "="*80)
    print("âœ… æ‰€æœ‰æ¨¡å‹æ³›åŒ–æ€§æµ‹è¯•å…¨éƒ¨å®Œæˆï¼")
    print("="*80 + "\n")

    print("ğŸ“Œ å…³é”®å‘ç°:")
    print(f"   Baselineæ€§èƒ½ (Region A):")
    print(f"     - A2C: {baseline_rewards['A2C']:.2f}")
    print(f"     - PPO: {baseline_rewards['PPO']:.2f}")
    print(f"     - TD7: {baseline_rewards['TD7']:.2f}")

    print(f"\n   å¹³å‡æ€§èƒ½ä¸‹é™:")
    for rank, (model_name, drop) in enumerate(ranking, 1):
        print(f"     {rank}. {model_name}: {drop:+.2f}% {'(æ³›åŒ–èƒ½åŠ›æœ€å¼º)' if rank == 1 else ''}")

    print("\nğŸ’¡ ä¸‹ä¸€æ­¥ï¼š")
    print("   1. æŸ¥çœ‹è¯¦ç»†ç»“æœ: cat generalization_results/all_models_generalization_results.json")
    print("   2. æŸ¥çœ‹æ±‡æ€»è¡¨æ ¼: cat generalization_results/all_models_generalization_summary.csv")
    print("   3. æŸ¥çœ‹æ³›åŒ–æ’å: cat generalization_results/generalization_ranking.txt")
    print("   4. ç»˜åˆ¶å¯è§†åŒ–å›¾è¡¨")


if __name__ == "__main__":
    main()
