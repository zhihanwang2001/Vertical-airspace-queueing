% ============================================================
% Batch 7: Discussion and Conclusion
% Created: 2026-01-06
% Based on: Documentation/guides/Final_Paper_Chinese_Version.md ยง7-8
% ============================================================

\section{Discussion}
\label{sec:discussion}

This section examines the theoretical significance, practical deployment prospects, research limitations, and future research directions of the MCRPS/SD/K framework and our deep reinforcement learning findings.

\subsection{Theoretical Contributions and Significance}
\label{subsec:theoretical_contributions}

\textbf{Extensions to Queueing Theory.} The MCRPS/SD/K framework extends classical queueing theory along multiple dimensions. First, we introduce a \textbf{vertical layered} queueing network structure, breaking from the traditional focus on horizontal resource allocation. Second, the inverted pyramid capacity profile (C=\{2,3,4,6,8\}) represents the first systematic incorporation of \textbf{altitude-dependent capacity constraints} into queueing system modeling, reflecting the physical reality of urban low-altitude airspace where lower altitudes face tighter restrictions and higher altitudes offer greater capacity. Third, the pressure-triggered inter-layer transfer mechanism combines \textbf{state-dependent control} with \textbf{Poisson splitting}, enabling dynamic cross-layer resource scheduling.

To the best of our knowledge, this is the first framework to systematically model multi-class correlated arrivals (framework supports Copula modeling, current implementation uses independent Poisson approximation), stochastic batch service (theoretically modeled as binomial distribution, current implementation uses Poisson approximation), state-dependent control, and dynamic inter-layer transfers in a vertical queueing system. This provides novel theoretical tools for three-dimensional resource allocation problems such as airspace management. Appendix B provides complete theoretical extension algorithms for Gaussian Copula correlated arrivals and binomial batch service.

\textbf{State Stability Hypothesis: Beyond State Space Size.} Our empirical measurements (Appendix A) reveal that the capacity paradox stems not from state space size per se, but from \textbf{state stability}. Under high load (10$\times$ baseline), Monte Carlo sampling revealed that high-capacity configurations cannot maintain explorable steady states: K=10 visited 25 unique states with stable operation, while K=30 visited only 1 state (empty queue [0,0,0,0,0]) due to continuous crash-reset cycles. The challenge is not that the state space is ``too large'' ($3^K$ theoretical upper bound), but that high-capacity configurations cannot maintain \textbf{explorable steady states} under load. Deep RL policies require stable state distributions to learn effective control strategies. The inverted pyramid structure [8,6,4,3,2] distributes load more evenly ($\rho \in [7.8\%, 31.3\%]$ @ 5$\times$ load) compared to normal pyramid ($\rho \in [7.8\%, 62.5\%]$), enabling stable episodes and richer state exploration that facilitates policy learning.

\textbf{Theoretical Perspective on Capacity Paradox Attribution.} A fundamental question emerges: Is the capacity paradox a \textbf{DRL training artifact} stemming from algorithmic limitations, or a \textbf{genuine system property} inherent to vertical queueing dynamics under high load? Our current evidence suggests it is \textbf{primarily DRL-specific} but likely reflects a deeper interaction between learning complexity and system characteristics:

\textit{DRL-Specific Evidence}: (1) The state space coverage ratio deteriorates exponentially with capacity ($3^{10}=59,049$ vs $3^{30} \approx 2 \times 10^{14}$), while training samples remain fixed at 500k steps, creating a fundamental sample-complexity mismatch that degrades exploration efficiency. (2) High-capacity configurations experience crash-reset cycles that prevent DRL policies from observing stable steady-state distributions---a training prerequisite for value function convergence. From a pure algorithmic perspective, this suggests the paradox could potentially be overcome with: (a) Dramatically increased training budgets ($10^7$--$10^8$ steps), (b) Hierarchical decomposition reducing effective state dimensionality, or (c) Transfer learning from lower-capacity configurations.

\textit{Potential System-Inherent Aspects}: However, queueing-theoretic analysis suggests the paradox may also reflect genuine control challenges: (1) Higher capacities under fixed high load ($\lambda=5.0$ items/s) yield lower per-layer utilization, paradoxically \textit{reducing urgency signals} that guide optimal control---extremely large buffers mask congestion until catastrophic overflow occurs. (2) From a Lyapunov stability perspective, larger state spaces require more conservative policies to ensure convergence, potentially limiting achievable performance even under optimal control. (3) The traffic-capacity mismatch in high-K uniform configurations (e.g., K=40 with equal distribution) creates inherent load imbalances that no controller---DRL or otherwise---can fully resolve without violating capacity constraints.

\textit{Open Question}: Definitively distinguishing DRL limitations from system properties requires comparison with \textbf{optimal control baselines} (e.g., model predictive control, dynamic programming solutions) or \textbf{simple heuristic policies} (e.g., greedy scheduling prioritizing the most congested layer). If heuristics also exhibit the K=10 $>$ K=30 performance inversion, this would validate the system-inherent hypothesis. Conversely, if optimal controllers or heuristics monotonically improve with capacity, the paradox is purely a DRL training difficulty. We acknowledge this baseline gap as a key limitation (ยง\ref{subsec:limitations}) and propose heuristic comparison experiments as immediate future work. Our current interpretation is that the capacity paradox represents a \textbf{synergistic effect}: DRL training difficulty exacerbated by genuine queueing control challenges in high-dimensional, weakly-signaled state spaces.

\textbf{Empirical Insights for Deep Reinforcement Learning.} Through systematic experiments with 15 methods totaling 7.5 million training steps, we obtained several important algorithm design insights. First, \textbf{policy-based methods} (PPO, A2C) demonstrate significant advantages in hybrid action spaces (11-dimensional continuous + discrete), validating the adaptability of policy gradient methods to complex action structures. Second, \textbf{memory mechanisms} (R2D2's LSTM) play a critical role in capturing temporal dependencies of queue dynamics, enabling algorithms to reason about long-term queue evolution patterns. Third, \textbf{representation learning} (TD7's SALE) triggers a dual-jump learning phenomenon when reaching critical thresholds (first jump +857\%@26,689 steps, second jump +95\%@26,989 steps, separated by only 300 steps), revealing the remarkable impact of embedding space structure on reinforcement learning. Finally, \textbf{staged learning rate scheduling} demonstrates the importance of exploration-exploitation balance, providing a new design paradigm for hyperparameter optimization.

\textbf{Multi-Objective Optimization Methodology Innovation.} Our Pareto frontier analysis (262 optimal solutions, 13 knee points) provides systematic decision support for practical deployment. Compared to traditional single-objective optimization or simple weighted sum methods, our MCDM knee point detection method (quality 40\% + diversity 40\% + balance 20\%) identifies representative trade-off configurations suitable for different operational scenarios (adverse weather, peak hours, regular operations). This methodology can be generalized to other complex systems requiring real-time policy switching.

\subsection{Practical Application Prospects}
\label{subsec:practical_prospects}

\textbf{Urban Air Traffic Management.} This framework can be directly applied to scenarios such as urban drone delivery, air taxis, and emergency medical logistics. The inverted pyramid capacity design aligns with actual constraints of urban low-altitude airspace: lower layers (20-40m) face limited capacity due to buildings, trees, and densely populated areas; higher layers (80-100m) offer ample capacity with open space. The pressure-triggered transfer mechanism enables the system to dynamically allocate drone altitudes based on real-time congestion states, avoiding local overload. A2C's 6.9-minute training time and PPO's 30.8-minute training time indicate that the system can complete policy optimization within reasonable computational budgets, supporting \textbf{near-real-time deployment}.

\textbf{Scalability and Modularity.} The MCRPS/SD/K framework employs modular design, supporting scalability from single-corridor pilots to city-wide systems. The environment configuration (\texttt{DRLOptimizedQueueEnvFixed}) uses parameterized layer count L, capacity C, service rates $\mu$, etc., easily adapting to different urban airspace characteristics. The reinforcement learning algorithm adapters (discrete RL, continuous RL, distributed RL) enable flexible selection of the most suitable algorithm. The adaptive weight mechanism of the multi-objective reward function allows real-time adjustment of system behavior based on operational priorities (throughput-priority vs. safety-priority).

\textbf{Smart City Integration.} This framework can integrate with other smart city subsystems (traffic management, emergency response, environmental monitoring). For example, meteorological data can dynamically adjust layer capacities (adverse weather reduces high-altitude layer capacity), traffic incidents can trigger priority elevation for emergency categories, and population density data can optimize safety thresholds for low-altitude layers. The 13 knee point configurations from the Pareto frontier provide predefined policy templates for different scenarios, supporting rapid policy switching.

\subsection{Research Limitations}
\label{subsec:limitations}

We acknowledge the following limitations that constrain the generalizability and applicability of our findings. These limitations also suggest important directions for future research.

\subsubsection{Methodological Limitations}

\textbf{Sample Size and Statistical Power.} Our structural comparison experiments (inverted vs normal pyramid) employed n=3 independent runs per configuration due to computational constraints (each run requires 500k training steps with evaluation). While Welch's $t$-tests demonstrate highly significant differences ($p<0.001$, Cohen's $d>30$) with statistical power exceeding 0.99 for the observed effect sizes, larger sample sizes (n=10 or higher) would improve robustness for detecting smaller secondary effects and would provide more precise variance estimates. The combination of minimal sample size with extreme effect sizes ensures adequate statistical conclusions for our primary findings, but we acknowledge that subtle interactions between capacity configurations and algorithmic hyperparameters may require larger-scale studies.

\textbf{Load Configuration Specificity.} The 5$\times$ load configuration (average utilization $\rho \approx 95\%$) was selected to balance challenge and training stability after preliminary experiments at 10$\times$ load resulted in 100\% crash rates for pyramid structures due to excessive bottom-layer overload ($\rho=345\%$ at Layer~1). While 5$\times$ load represents realistic peak demand scenarios (lunch/dinner delivery rushes in urban logistics), performance under production loads (1$\times$ baseline, $\rho \approx 20\%$) remains insufficiently validated. Our findings are most directly applicable to \textbf{high-utilization peak-demand regimes} rather than nominal operating conditions. Load-sweep experiments (3$\times$, 5$\times$, 7$\times$) would clarify the performance-load relationship across operational regimes.

\textbf{Traffic Distribution Dependency.} The structural design advantage (inverted pyramid outperforming normal pyramid by +9.2\% for A2C, +9.6\% for PPO) is specific to our baseline traffic distribution $\alpha=[0.10, 0.15, 0.20, 0.25, 0.30]$ which concentrates 55\% of arrivals in upper layers. \textbf{We have not validated this advantage under reversed traffic patterns} (e.g., $\alpha=[0.30, ..., 0.10]$ with lower-layer concentration), where the normal pyramid might demonstrate superior performance due to better traffic-capacity alignment. The generalizable principle is \textbf{capacity-traffic matching} rather than absolute structural superiority. Planned sensitivity analyses (uniform distribution $\alpha=[0.2, ..., 0.2]$ and reversed distribution) will test whether our findings generalize across traffic scenarios or represent distribution-specific artifacts.

\textbf{State Space Measurement Scope.} Our Monte Carlo state space measurements (Appendix A) were conducted exclusively at 10$\times$ load, where high-capacity configurations immediately crashed. While these measurements validated the state stability hypothesis (K=10 visited 25 states vs K=30's single empty-queue state due to crash-reset cycles), measurements at moderate loads (3--5$\times$) would provide additional insights into the capacity-stability relationship. Such experiments could reveal whether high-capacity configurations can maintain richer state spaces under less extreme conditions, clarifying whether the capacity paradox is load-dependent or represents a fundamental DRL training limitation.

\subsubsection{Theoretical and Modeling Limitations}

\textbf{Empirical vs Theoretical Nature of Findings.} This study is primarily \textbf{empirical}, relying on systematic experimentation across 7 capacity configurations, 3 DRL algorithms, and 21 independent runs. While we provide theoretical context (Jackson networks, BCMP theorem, state space complexity analysis), we do not derive formal theorems proving the capacity paradox or traffic-capacity matching principle. Consequently, our findings represent \textbf{data-driven observations} rather than provably optimal solutions. Establishing whether the capacity paradox stems from fundamental DRL limitations (curse of dimensionality) or system-inherent properties would require either: (1) Theoretical analysis deriving optimal capacity bounds under queueing stability constraints, or (2) Comparisons with optimal control baselines (e.g., model predictive control, dynamic programming solutions). The absence of such baselines means we cannot definitively attribute the capacity paradox to DRL-specific training difficulty versus genuine system characteristics.

\textbf{Simplified Environmental Assumptions.} The current model assumes good weather conditions and reliable communication links. In practical deployment, weather variations (wind speed, rainfall, visibility) significantly affect service rates $\mu$ and capacities C, while communication interruptions lead to incomplete state observations. Future work needs to introduce \textbf{stochastic capacity models} (weather-driven time-varying C(t)) and \textbf{partially observable frameworks} (POMDP) to enhance system robustness.

\textbf{Abstracted Conflict Resolution.} This research simplifies conflict management to capacity constraints ($\sum_k n^t_{k,\ell} \leq C_\ell$), without explicitly modeling geometric conflicts between drones. In high-density scenarios, geometric conflicts may still occur even when queue lengths do not exceed capacity. Integrating \textbf{geometry-based Conflict Detection and Resolution (CDR)} algorithms is important follow-up work.

\textbf{Implementation-Theory Alignment.} The current implementation primarily employs \textbf{downward transfers} (high layers $\rightarrow$ low layers) as the load balancing mechanism, aligning with the physical logic of inverted pyramid design: higher layers have ample capacity (large C), so when high layers become congested, transferring orders to lower layers can relieve pressure. Although higher layers offer faster service ($\mu_{100m}=1.2 > \mu_{20m}=0.4$), when high layers are severely overloaded, downward transfers to lower-capacity layers (e.g., $C_{20m}=2$) serve as temporary buffers, preventing order backlog at high layers. Downward transfers act only as emergency mechanisms; the system normally relies on \textbf{service rate adjustment} (dynamic $\mu$ tuning) rather than frequent transfers to optimize performance. The bidirectional transfer framework (upward/downward/stay) described in theory represents a more general formulation; the current implementation focuses on the most critical downward transfer scenarios. Future work can extend the implementation to include upward transfers for emergency evacuation when lower layers face severe overload.

\textbf{Limited Energy Modeling.} The current framework assumes sufficient drone energy, without considering battery constraints' impact on service rates and transfer decisions. In practice, drones consume more energy when flying at higher altitudes (increased wind resistance), and frequent vertical transfers also consume additional energy. Introducing \textbf{energy-aware transfer strategies} and \textbf{dynamic charging station coordination} will enhance system practicality.

\subsubsection{Practical Deployment Limitations}

\textbf{Lack of Real-World Validation.} All experiments are conducted in a custom OpenAI Gym simulation environment (\texttt{DRLOptimizedQueueEnvFixed}) with simplified dynamics. We have not validated our findings on: (1) Real drone hardware platforms with physical dynamics, sensor noise, and actuator delays; (2) High-fidelity simulators (AirSim, Gazebo) with realistic aerodynamics; (3) Actual UAM operational data from deployment sites. Consequently, our results represent \textbf{proof-of-concept} demonstrations in a controlled simulation environment rather than field-validated operational guidelines. The gap between idealized simulation and real-world deployment (perception uncertainties, regulatory constraints, emergency handling) may significantly impact practical performance.

\textbf{Training Cost and Sample Efficiency.} Some methods (e.g., TD7's 126.1 minutes, R2D2's 115.7 minutes) have lengthy training times, limiting online learning and real-time adaptation capabilities. Although Rainbow DQN is stable, it has low sample efficiency (2361$\pm$46 performance requires 500k steps). How to improve sample efficiency while maintaining performance is a key challenge for applying reinforcement learning in real-time systems.

\textbf{Insufficient Transfer Learning.} Current models are trained for specific airspace configurations; cross-scenario generalization capabilities remain insufficiently validated. Different cities have significantly different airspace characteristics (layer count, capacity distribution, arrival rates). How to rapidly transfer to new scenarios through \textbf{meta-learning} or \textbf{domain adaptation} is an important issue for practical deployment.

\textbf{Baseline Comparison Gaps.} We compare exclusively among DRL algorithms (A2C, PPO, TD7) and do not benchmark against: (1) \textbf{Classical optimal control} methods (dynamic programming, model predictive control); (2) \textbf{Simple heuristic baselines} (greedy scheduling, round-robin allocation); (3) \textbf{Domain-specific schedulers} from air traffic management literature. The absence of such baselines limits our ability to assess whether DRL provides significant advantages over simpler approaches for this problem class. It remains possible that well-tuned heuristics could achieve competitive performance without the computational overhead and sample complexity of deep reinforcement learning.

\subsection{Future Research Directions}
\label{subsec:future_directions}

\textbf{Multi-Scale Temporal Control.} The current framework makes decisions at a single time scale (1 step = 1 second). Future work can introduce \textbf{hierarchical reinforcement learning}, planning capacity allocation strategies at long time scales (hourly) and executing real-time transfer decisions at short time scales (seconds), achieving strategic-tactical coordinated optimization.

\textbf{Stochastic Capacity and Environmental Uncertainty.} Model weather and regulation-driven stochastic capacity $C_\ell(t, \omega)$, where $\omega$ represents random events (rainfall, temporary no-fly zones). \textbf{Robust reinforcement learning} or \textbf{distributional reinforcement learning} can learn policies insensitive to environmental disturbances, and \textbf{online adaptation mechanisms} can rapidly respond to emergent events.

\textbf{Inter-Regional Coordination and Network Effects.} Extend the single-corridor model to \textbf{multi-corridor networks}, modeling inter-regional drone flows and capacity coupling. Introduce \textbf{multi-agent reinforcement learning} (e.g., QMIX, MAPPO) to coordinate transfer decisions across different corridors, optimizing global network throughput and fairness.

\textbf{Hardware-in-the-Loop Validation.} Validate framework performance on real drone platforms or high-fidelity simulators (e.g., AirSim, Gazebo). Integrate \textbf{perception-planning-control} closed loops, testing reinforcement learning policy robustness under sensor noise, execution delays, and dynamics constraints.

\textbf{Regulatory Integration and Explainability.} Interface with airspace regulatory agencies' (FAA, EASA) U-space/UTM standards, ensuring MCRPS/SD/K transfer decisions comply with safety rules. Develop \textbf{explainable reinforcement learning} methods (e.g., attention mechanisms, policy distillation) enabling regulators and operators to understand system decision logic, improving trust.

\textbf{Human-Machine Collaborative Interface.} Design intuitive visualization interfaces displaying real-time 5-layer queue states, pressure distributions, and Pareto frontiers. Support manual interventions (e.g., manually adjusting priorities, pausing layer service), combined with \textbf{human-in-the-loop reinforcement learning} to optimize human-machine collaboration modes.

\textbf{Economic and Policy Analysis.} Evaluate MCRPS/SD/K's impact on drone delivery economics (cost reduction, timeliness improvement), analyze effects of different capacity configurations and pricing strategies on operator revenue. Provide decision support for policymakers on airspace capacity planning and dynamic pricing.

\section{Conclusion}
\label{sec:conclusion}

\textbf{To the best of our knowledge}, MCRPS/SD/K is the first \textbf{vertical} queueing framework with \textbf{inverted pyramid capacity} and \textbf{pressure-triggered inter-layer transfers}, integrated with \textbf{queue-aware deep reinforcement learning}. Through \textbf{15 methods} each trained for \textbf{500,000 steps}, we establish new benchmarks for multi-objective, hierarchical airspace control, and discover novel \textbf{jump learning} behavior (TD7) and \textbf{staged learning rate scheduling} optimization strategies (A2C).

\textbf{Key Contributions Summary:}

\begin{enumerate}
\item \textbf{Theoretical Innovation}: We propose the MCRPS/SD/K vertical layered queueing framework, the first to systematically model multi-class correlated arrivals (framework supports Copula modeling, current implementation uses independent Poisson approximation), stochastic batch service (theoretically modeled as binomial distribution, current implementation uses Poisson approximation), state-dependent control, and dynamic inter-layer transfers for UAM airspace. Appendix B provides complete theoretical extension algorithms.

\item \textbf{Algorithmic Breakthrough}: Policy-based methods demonstrate exceptional performance advantages. \textbf{A2C and PPO are statistically tied for first place} ($p_{\text{adj}}=0.836$). A2C achieves 4437.86$\pm$128.41 (lowest training variance) through staged learning rate scheduling, requiring only 6.9 minutes of training to reach top-tier performance; PPO offers reliable deployment with robust performance (4419.98$\pm$135.71). All three top algorithms (A2C, PPO, TD7) achieve 4350+ performance, with TD7 (4351.8$\pm$51.1, final evaluation 4409.13) exhibiting the lowest evaluation variance. These findings demonstrate the critical role of learning rate scheduling timing on training outcomes, providing diverse high-efficiency options for production deployment.

\item \textbf{Multi-Objective Optimization Framework}: We identify 262 Pareto optimal solutions and 13 representative knee points, providing diverse configuration choices for practical deployment.

\item \textbf{Empirical Findings}: TD7's dual-jump learning phenomenon (first jump +857\%@26,689 steps, second jump +95\%@26,989 steps, separated by 300 steps) reveals the critical threshold effect of SALE representation learning and the two-stage mechanism of policy optimization convergence.
\end{enumerate}

We hope this work provides theoretical foundations and practical blueprints for safe, fair, and efficient urban air mobility. The success of the three top algorithms (A2C, PPO, TD7) demonstrates the advantages of policy-based methods in hybrid action spaces. In particular, A2C's staged learning rate scheduling strategy opens new directions for reinforcement learning hyperparameter optimization, with both methods statistically tied to provide reliable high-performance choices for real-time deployment.
