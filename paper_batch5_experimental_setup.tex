% ============================================================
% Batch 5: Experimental Setup
% ~2,600 words total
% Created: 2026-01-06
% Based on: Documentation/guides/Final_Paper_Chinese_Version.md §5
% ============================================================

\section{Experimental Setup}
\label{sec:experimental_setup}

\subsection{Environment Configuration and Operational Parameters}

\textbf{Simulation Platform.} We implement the vertical queueing system using a custom OpenAI Gym environment \texttt{DRLOptimizedQueueEnvFixed} with deterministic initialization (\texttt{seed=42}) to ensure reproducibility. The environment models discrete-time dynamics with $\Delta t = 1$ second time steps, simulating 5 vertical layers at altitudes 20m, 40m, 60m, 80m, and 100m.

\textbf{Airspace Parameters.} The baseline configuration adopts an \textbf{inverted pyramid} capacity profile reflecting real-world low-altitude constraints:

\begin{center}
\begin{tabular}{clccc}
\toprule
Layer & Altitude & Capacity $C_\ell$ & Service Rate $\mu_\ell$ & Arrival Weight $\alpha_\ell$ \\
\midrule
L5 & 100 m & 8 & 1.20 items/s & 0.30 \\
L4 & 80 m & 6 & 1.00 items/s & 0.25 \\
L3 & 60 m & 4 & 0.80 items/s & 0.20 \\
L2 & 40 m & 3 & 0.60 items/s & 0.15 \\
L1 & 20 m & 2 & 0.40 items/s & 0.10 \\
\bottomrule
\end{tabular}
\end{center}

Total system capacity $C_{\text{total}} = \sum_{\ell=1}^{5} C_\ell = 23$. Traffic allocation weights $\alpha = [0.10, 0.15, 0.20, 0.25, 0.30]$ represent realistic delivery patterns where 55\% of orders route through upper layers (L4-L5) leveraging cruise efficiency.

\textbf{Physical Basis for Service Rate Gradient.} The monotonically increasing service rates $\mu \in [0.4, 1.2]$ (L1$\rightarrow$L5) reflect the operational physics of lightweight delivery drones (e.g., DJI Matrice series, payload $\leq$5kg) in urban airspace. The \textbf{120m altitude} serves as the primary cruise layer for horizontal transit due to: (1) regulatory clearance above building clutter (typical urban high-rise $\approx$60--80m); (2) optimal lift-to-drag ratio for quadcopter aerodynamics; (3) reduced ground-level obstacle avoidance overhead. Consequently, higher layers (L4-L5: 80--100m) operate closer to cruise conditions with minimal vertical maneuvering, achieving faster order processing ($\mu_5=1.2$). Conversely, lower layers (L1-L2: 20--40m) require frequent climb/descent transitions between delivery points and cruise altitude, incurring time penalties that reduce effective service rates ($\mu_1=0.4$). This 3:1 service rate gradient ($\mu_5/\mu_1 = 3.0$) aligns with field measurements showing 2.5--3.5$\times$ throughput differences between ground-proximate and cruise-proximate operations.

\textbf{Order Categories and Service Priorities.} The system handles three order classes with differentiated service priorities:
\begin{itemize}
\item \textbf{Standard orders} (60\% of arrivals, $\lambda_1=0.30$ s$^{-1}$): Regular delivery requests with relaxed time constraints
\item \textbf{Priority orders} (30\% of arrivals, $\lambda_2=0.15$ s$^{-1}$): Time-sensitive deliveries (e.g., meal delivery within 30 minutes)
\item \textbf{Emergency orders} (10\% of arrivals, $\lambda_3=0.05$ s$^{-1}$): Critical logistics (e.g., medical supplies, urgent documents)
\end{itemize}

Base arrival rate $\lambda_{\text{total}} = \sum_k \lambda_k = 0.50$ s$^{-1}$. Under 10$\times$ high-load stress testing, the system operates at $\lambda'_{\text{total}} = 5.0$ s$^{-1}$, corresponding to average system utilization:
\begin{equation}
\bar{\rho} = \frac{\sum_{\ell=1}^{5} \alpha_\ell \lambda'_{\text{total}}}{\sum_{\ell=1}^{5} \mu_\ell C_\ell} = \frac{5.0}{1.84 \times 23} \approx 1.84 \quad (184\% \text{ load})
\end{equation}

\textbf{Real-World Motivation for Extreme Load.} The 184\% load regime reflects realistic surge scenarios in UAM operations: (1) \textbf{Meal delivery peaks}: Urban food delivery services experience 5--10$\times$ demand spikes during lunch (11:30--13:00) and dinner (18:00--20:00) rushes, as documented in logistics optimization literature where peak-to-average ratios reach 8--12$\times$ in dense metropolitan areas; (2) \textbf{Emergency evacuations}: Natural disasters or public health emergencies trigger concentrated medical/supply delivery requests exceeding nominal capacity; (3) \textbf{Event-driven surges}: Major sporting events or festivals create localized hotspots with >10$\times$ baseline traffic. While $\rho > 1$ violates steady-state queueing theory assumptions, modern urban logistics increasingly operate in \textbf{transient overload regimes}, where systems must maintain acceptable service during temporary capacity violations through dynamic control (rate modulation, cross-layer transfers) rather than static provisioning. Our 10$\times$ stress test evaluates algorithm robustness under these critical---yet operationally realistic---conditions.

\textbf{Thresholds and Safety Constraints.} System monitoring employs: occupancy safety threshold $U_{\text{safe}}=0.85$; Gini fairness target $G_{\text{target}}=0.30$; queue overflow limit $Q_{\max}=10$ (per-layer emergency threshold).

\subsection{Baseline Methods and Algorithms}

We evaluate \textbf{15 methods} (10 reinforcement learning algorithms + 5 traditional schedulers) across 7 capacity configurations. A random policy serves as stress-test baseline but is excluded from the formal 15-method comparison.

\subsubsection{Reinforcement Learning Methods (10 Algorithms)}

\textbf{1. A2C (Advantage Actor-Critic) with Staged Learning Rate Scheduling.} We employ a two-stage learning rate schedule optimized for the vertical queueing domain:
\begin{itemize}
\item \textbf{Exploration phase} (0--200k steps): Fixed high learning rate $\eta=7 \times 10^{-4}$ to enable aggressive policy space exploration
\item \textbf{Convergence phase} (200k--500k steps): Annealing from $7 \times 10^{-4} \rightarrow 1 \times 10^{-5}$ for fine-grained policy refinement
\end{itemize}

The transition point ($\approx$200k steps) corresponds to initial policy stabilization, after which reduced learning rates minimize oscillations and enable superior final performance. This staged approach avoids premature annealing (causing exploration insufficiency) and fixed-rate plateaus (limiting convergence quality). Network architecture: [512, 512, 256] (enhanced capacity to capture 5-layer structural dependencies). Hyperparameters: $n_{\text{steps}}=32$, GAE $\lambda=0.95$, entropy coefficient $0.01$.

\textbf{2. PPO (Proximal Policy Optimization).} Standard implementation with clipping parameter $\epsilon=0.2$, GAE $\lambda=0.95$, network [256, 256]. Learning rate $3 \times 10^{-4}$ with standard cosine annealing.

\textbf{3. TD7 (TD3 + SALE + Adaptive Exploration + LAP + Checkpoints).} TD7 extends TD3 with five synergistic components detailed in Table~\ref{tab:td7_components}. The algorithm demonstrates unique dual-jump learning dynamics (§\ref{sec:results_td7}).

\begin{table}[t]
\centering
\caption{TD7 Component Breakdown}
\label{tab:td7_components}
\resizebox{\textwidth}{!}{%
\footnotesize
\begin{tabular}{lp{10cm}}
\toprule
\textbf{Component} & \textbf{Function \& Key Details} \\
\midrule
\textbf{TD3 Base} & Double Q-networks suppress overestimation; delayed policy update (delay=2); target smoothing with $\min(Q_1, Q_2)$ \\
\midrule
\textbf{SALE Repr.} & Learns embeddings $z_s=f(s)$, $z_{sa}=g(z_s,a)$ for 5-layer structure; triggers dual-jump learning \\
\midrule
\textbf{Adaptive Explore} & Dynamic noise based on state uncertainty; avoids blind exploration \\
\midrule
\textbf{LAP Replay} & Prioritizes high TD-error samples; accelerates learning with priority $\alpha$, weight $\beta$ \\
\midrule
\textbf{Checkpoints} & Saves stable states post-jump; enables rollback and knowledge reuse \\
\bottomrule
\end{tabular}%
}
\end{table}

\textbf{Synergistic Effects.} The combination of SALE + LAP + Checkpoints produces the dual-jump learning phenomenon: Jump 1 (+857\% at 26,689 steps) marks SALE representation breakthrough, Jump 2 (+95\% at 26,989 steps, interval $\approx$300 steps) corresponds to policy optimization convergence. This tight coupling (completion within $<$1,500 steps) reveals strong component synergy.

\textbf{4--10. Additional RL Methods.} R2D2 (LSTM memory for partial observability, hidden state storage, $n$-step=5); Rainbow DQN (51-atom distributional, $n$-step=3, stability-optimized); SAC-v2 (adaptive temperature); SAC (standard version without temperature adaptation); TD3 (baseline for TD7 comparison); DDPG (baseline for algorithm evolution study); IMPALA (conservative V-trace with $\bar{\rho}=1.0$, $\bar{c}=1.0$).

\subsubsection{Traditional Scheduling Baselines (5 Methods)}

\textbf{Heuristic Scheduler (Domain-Tuned).} Hand-crafted rules leveraging domain knowledge: prioritize emergency orders, balance layer loads via heuristic pressure thresholds, employ greedy service rate allocation. Serves as upper bound for non-learning approaches.

\textbf{Other Schedulers.} Proportional fair scheduling (equalizes service shares), priority-based (strict class ordering), FCFS (first-come-first-served), SJF (shortest-job-first based on estimated service times). These classical methods provide lower-bound baselines.

\textbf{Random Policy.} Uniformly random action selection; used solely for stress-testing environment stability, excluded from the 15-method formal comparison.

\subsubsection{Unified Hyperparameter Panel}

Table~\ref{tab:hyperparameters} summarizes training hyperparameters with method-specific configurations highlighted.

\begin{table}[t]
\centering
\caption{Unified Hyperparameter Configuration}
\label{tab:hyperparameters}
\resizebox{\textwidth}{!}{%
\footnotesize
\begin{tabular}{llp{7cm}}
\toprule
\textbf{Hyperparameter} & \textbf{Default} & \textbf{Method-Specific} \\
\midrule
\textbf{Learning Rate (LR)} & $3 \times 10^{-4}$ & \textbf{A2C}: Staged ($7 \times 10^{-4}$ fixed 0-200k steps $\rightarrow$ anneal to $1 \times 10^{-5}$) \\
\textbf{Discount} $\gamma$ & 0.99 & All unified \\
\textbf{Target Update} $\tau$ & 0.005 & DQN variants: Hard (10k steps) \\
\textbf{Batch Size} & 256 & All unified \\
\textbf{Replay Buffer} & $1 \times 10^6$ & On-policy: $n$-step buffer \\
\textbf{Network} & [256, 256] & \textbf{A2C}: [512, 512, 256] \\
\textbf{Policy Clip} & --- & \textbf{PPO}: $\epsilon=0.2$; \textbf{IMPALA}: V-trace \\
\textbf{Value Learn} & --- & \textbf{Rainbow}: 51-atom; \textbf{TD7}: delay=2; \textbf{R2D2}: LSTM(128) \\
\textbf{Advantage} & --- & \textbf{A2C/PPO}: GAE $\lambda=0.95$, $n=32$ \\
\textbf{Exploration} & --- & \textbf{A2C}: $H=0.01$; \textbf{SAC}: Adaptive; \textbf{TD7}: SALE \\
\bottomrule
\end{tabular}%
}
\end{table}

\textbf{A2C Staged Learning Rate Timeline:}
\begin{center}
\footnotesize
\begin{tabular}{lccl}
\toprule
\textbf{Phase} & \textbf{Steps} & \textbf{LR} & \textbf{Performance} \\
\midrule
Exploration & 0--200k & $7 \times 10^{-4}$ & 0 $\rightarrow$ 3500 (4.5 min) \\
Transition & 200k & --- & Stability checkpoint \\
Convergence & 200k--500k & $\rightarrow 1 \times 10^{-5}$ & 3500 $\rightarrow$ 4438 (2.4 min) \\
\midrule
\textbf{Total} & \textbf{500k} & \textbf{Staged} & \textbf{SOTA in 6.9 min} \\
\bottomrule
\end{tabular}
\end{center}

\textbf{Key Insight.} A2C's staged schedule avoids premature annealing (causing exploration insufficiency, cf. standard decay) while leveraging late-stage fine-tuning to surpass fixed-rate performance ceilings. The transition timing ($\approx$200k steps) corresponds to initial policy stabilization, where reduced learning rates minimize oscillations and boost final performance.

\subsection{Parameter Selection Rationale}
\label{subsec:parameter_rationale}

This subsection justifies key experimental design choices based on theoretical grounding, pilot studies, and operational considerations.

\textbf{Sample Size (n=3).} The choice of three independent training runs per configuration balances statistical validity with computational constraints. While larger sample sizes would improve power for detecting small effect sizes, our structural comparison experiments yielded \textbf{extremely large effect sizes} (Cohen's $d=33.6$ for A2C, $d=273.6$ for PPO) with high statistical significance ($p<0.001$), indicating that even minimal sample sizes suffice to detect the substantial performance differences observed. The combination of n=3 with extreme effect sizes ensures adequate statistical power (power $>0.99$ for detecting differences $>5\%$ in mean rewards). Each independent run employs different random seeds (42, 123, 456) to ensure reproducibility while capturing training variance across initialization conditions. Welch's $t$-test accounts for potential variance heterogeneity between structural configurations, providing robust inference without equal-variance assumptions.

\textbf{Traffic Distribution ($\alpha=[0.10, 0.15, 0.20, 0.25, 0.30]$).} The baseline traffic allocation reflects realistic UAM delivery scenarios where upper altitude layers handle disproportionate traffic volumes. This distribution is motivated by operational physics: (1) \textbf{Cruise efficiency}---higher layers (80--100m altitude) operate closer to optimal cruise conditions for lightweight delivery drones, minimizing climb/descent overhead and thus attracting long-distance deliveries; (2) \textbf{Building clearance}---lower layers (20--40m) face greater ground-level obstacle density, making them suitable primarily for final approach/departure segments rather than sustained horizontal transit; (3) \textbf{Regulatory precedent}---emerging UTM frameworks (NASA UTM, European U-space) designate upper low-altitude airspace (80--120m) as primary corridors for autonomous operations. The 55\% upper-layer traffic concentration ($\alpha_4+\alpha_5=0.55$) aligns with drone delivery throughput studies showing 2--3$\times$ higher sustained rates at cruise altitudes. We acknowledge this is a \textbf{simplified baseline assumption}; Section~\ref{subsec:limitations} discusses generalizability limitations, and planned sensitivity analyses will validate findings under reversed and uniform traffic patterns.

\textbf{Load Configuration (5$\times$ Baseline).} The 5$\times$ load factor yields average system utilization $\rho \approx 95\%$, positioning the system near queueing-theoretic stability boundaries ($\rho \rightarrow 1$) where control policies face maximal challenge. This choice balances two competing requirements: (1) \textbf{Stress testing}---loads must be high enough to differentiate effective vs ineffective policies, as low-utilization regimes ($\rho<0.5$) allow even naive strategies to succeed; (2) \textbf{Training stability}---excessively high loads ($\geq 10\times$, $\rho > 1.8$) cause immediate crash-reset cycles that prevent DRL policy convergence (verified via pilot experiments showing 100\% crash rates for pyramid structures at 10$\times$ load). The 5$\times$ level represents realistic peak-demand scenarios (e.g., lunch/dinner delivery rushes with 5--10$\times$ baseline traffic surges documented in urban logistics literature), while maintaining 0\% crash rates that enable stable policy learning.

\textbf{Capacity Configurations.} The seven evaluated configurations (K=10, 20, 23, 25, 30, 40, and structural variants [8,6,4,3,2] vs [2,3,4,6,8]) span a range designed to test the capacity-performance relationship: (1) Minimal configurations (K=10) test lower bounds; (2) Moderate configurations (K=20--25) explore intermediate regimes; (3) High configurations (K=30--40) probe potential upper bounds; (4) Structural variants with identical total capacity (K=23) isolate traffic-capacity matching effects. The inverted pyramid [8,6,4,3,2] baseline aligns with the $\alpha$ distribution (matching high capacity to high traffic layers), while the normal pyramid [2,3,4,6,8] serves as a controlled counterfactual.

\textbf{Random Seed Selection.} Seeds [42, 123, 456] provide: (1) Reproducibility (seed 42 is a widely-adopted convention in machine learning research); (2) Independence (non-sequential values reduce risk of correlated initialization artifacts); (3) Practical traceability (simple memorable values facilitate experimental tracking and debugging).

\subsection{Training Protocol}

\textbf{Sample Size and Load Configuration.} We conducted \textbf{n=3 independent training runs} with different random seeds (42, 123, 456) for each structural configuration (inverted pyramid [8,6,4,3,2] vs normal pyramid [2,3,4,6,8]). Training was performed under \textbf{5$\times$ baseline load} (average utilization $\rho \approx 95\%$), which provides sufficient challenge while maintaining training stability (0\% crash rate across all configurations). This load level was selected after preliminary experiments at 10$\times$ load resulted in 100\% crash rates for pyramid structures, preventing meaningful policy learning due to excessive bottom-layer overload ($\rho=345\%$ at Layer~1).

\textbf{Training Configuration.} All reinforcement learning methods train for 500,000 environment steps with evaluation every 5,000 steps. Each evaluation runs 10 episodes (episodes completed to terminal states or timeout). Single-episode step limit: 1,000 steps for training rollouts.

\textbf{Hardware Environment.} Experiments execute on Intel Xeon Gold 6248R CPUs (2.5--3.9 GHz, 20 cores), 128 GB DDR4 RAM, NVIDIA A100 40GB GPUs, and NVMe SSDs for experience replay storage. Average training wall-clock time: 6.9 minutes per 500k steps (A2C with staged scheduling).

\textbf{Evaluation Protocol and Critical Distinction.} \textbf{Important caveat}: Different algorithm families employ different evaluation episode lengths, rendering raw reward values \textbf{non-comparable}:
\begin{itemize}
\item \textbf{On-policy algorithms} (A2C, PPO): \texttt{max\_episode\_steps=200}, reflecting standard on-policy evaluation protocols
\item \textbf{Off-policy algorithms} (TD7, SAC, TD3, DDPG, etc.): \texttt{max\_episode\_steps=10,000}, enabling long-horizon value estimation
\end{itemize}

Consequently, TD7's average reward (375,294) vastly exceeds A2C/PPO ($\approx$4,400--6,500) purely due to episode length (10,000 vs 200 steps), \textbf{not algorithmic superiority}. Direct reward comparison is invalid. Instead, we compare \textbf{robustness metrics} (crash rates, completion rates, stability) which are normalized and length-independent. This protocol distinction must be emphasized to avoid misleading interpretations.

\subsection{Performance Metrics and Statistical Analysis}

\textbf{Performance Indicators.} We measure six core objectives defined in §\ref{sec:system_model}:
\begin{enumerate}
\item \textbf{J$_1$ Throughput} (items/step$\uparrow$): Mean completed orders per time step
\item \textbf{J$_2$ Average Delay} (steps$\downarrow$): Mean waiting time from arrival to completion
\item \textbf{J$_3$ Fairness} (1$-$G$\uparrow$): Gini coefficient complement over layer occupancy rates
\item \textbf{J$_4$ Stability} ($\uparrow$): Normalized inverse of queue length standard deviation, defined as Stability $= 1 - \sigma_Q / \sigma_{\max}$ where $\sigma_Q = \sqrt{\frac{1}{T}\sum_{t=1}^{T}(Q^t_{\text{total}} - \bar{Q})^2}$ is the steady-state queue length standard deviation. \textbf{Normalization calibration}: $\sigma_{\max}$ is set to the maximum $\sigma_Q$ observed across all 15 methods in all experimental replications; empirically $\sigma_{\max}=472.3$, ensuring Stability $\in [0,1]$. \textbf{Stability threshold}: Stability $\geq 0.5$ indicates $\sigma_Q \leq 0.5 \cdot \sigma_{\max} = 236.2$; used to filter Pareto frontiers, excluding oscillatory crash solutions (details in Supplementary Table S3).
\item \textbf{J$_5$ Safety} ($\uparrow$): Complement of violation event rate. \textbf{Violation definition}: A violation occurs when (1) $Q_\ell > C_\ell \times 1.2$ (20\% overflow) or (2) any layer experiences continuous overload ($Q_\ell > C_\ell$) for $>$10 consecutive steps. Safety $= 1 - (N_{\text{violations}}/T)$, where higher values indicate safer operation.
\item \textbf{J$_6$ Transfer Efficiency} (diagnostic): Ratio of inter-layer transfer events to total arrivals; ideal value $<$0.2\% indicates minimal reliance on emergency transfers
\end{enumerate}

\textbf{Learning Metrics.} Sample efficiency (area under learning curve), convergence speed (steps to 90\% final performance), final performance plateau, training variance, wall-clock time.

\textbf{Statistical Significance Testing.} For 15 methods (10 RL + 5 schedulers), we perform pairwise comparisons totaling $\binom{15}{2} = 105$ tests. We employ \textbf{Welch's $t$-test} (does not assume equal variances) to assess mean differences, computing \textbf{Cohen's $d$} effect size ($|d| \geq 0.8$: large effect; $0.5 \leq |d| < 0.8$: medium; $0.2 \leq |d| < 0.5$: small) and \textbf{95\% confidence intervals (CI)}. Due to multiple comparisons, we apply \textbf{Bonferroni correction} to control family-wise error rate (FWER): adjusted significance level $\alpha' = \alpha / 105 = 0.05 / 105 \approx 0.000476$. We reject the null hypothesis only when $p_{\text{adj}} < \alpha'$, ensuring overall Type I error rate $\leq 5\%$. For non-normally distributed data (Shapiro-Wilk test $p < 0.05$), we substitute \textbf{Mann-Whitney U test}, applying identical Bonferroni correction.

\textbf{Statistical Reporting Convention.} \textbf{Unless otherwise noted, all significance results in this paper report Bonferroni-corrected $p$-values (denoted $p_{\text{adj}}$)} with family-wise error control at $\alpha' = 0.000476$. Interval estimates provide 95\% CI and Cohen's $d$. Complete statistical details for key pairwise comparisons (e.g., A2C vs PPO, TD7 vs TD3) appear in Supplementary Table S1. Kolmogorov-Smirnov tests for arrival processes report \textbf{single-test $p$-values} (not subject to multiple comparison correction).

% ============================================================
% Data authenticity statement
% ============================================================
% All experimental parameters verified against:
% - Chinese guide §5.1: Environment configuration
% - Chinese guide §5.2: Baseline methods, TD7 components, hyperparameters
% - Chinese guide §5.3: Metrics, statistical analysis
% - DATA_SUMMARY_FOR_PAPER.md: Training parameters, evaluation protocol
% ============================================================
