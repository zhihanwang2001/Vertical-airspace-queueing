%% Applied Soft Computing Manuscript
%% Deep Reinforcement Learning for Vertical Layered Queueing Systems in Urban Air Mobility
%% Document Class: Elsevier elsarticle with review mode

\documentclass[review,11pt]{elsarticle}

%% Required Packages
\usepackage[T1]{fontenc}        % Font encoding for proper character display
\usepackage[utf8]{inputenc}     % UTF-8 input encoding
\usepackage{lmodern}            % Latin Modern fonts for better PDF rendering
\usepackage{textcomp}           % Additional text symbols
\usepackage{graphicx}           % For including figures
\usepackage{amsmath,amssymb}    % Mathematical symbols and equations
\usepackage{amsthm}             % Theorem environments
\usepackage{booktabs}           % Professional quality tables
\usepackage{algorithm}          % Algorithm environment
\usepackage{algorithmic}        % Algorithmic notation
\usepackage{hyperref}           % Hyperlinks and cross-references
\usepackage{natbib}             % Bibliography management
\usepackage{multirow}           % Multi-row cells in tables
\usepackage{array}              % Enhanced array and tabular
\usepackage{xcolor}             % Color support

%% Theorem environments
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

%% Remove QED symbol at end of proofs
\renewcommand{\qedsymbol}{}

%% Adjust spacing to reduce page count
\usepackage{setspace}
\setstretch{1.0}

%% Reduce margins slightly
\usepackage[margin=1in]{geometry}

%% Hyperref configuration
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    citecolor=blue,
    urlcolor=blue
}

%% Document begins
\begin{document}

%% Frontmatter
\begin{frontmatter}

%% Title
\title{Deep Reinforcement Learning for Vertical Layered Queueing Systems in Urban Air Mobility: A Comparative Study}

%% Author
\author[inst1]{ZhiHan Wang\corref{cor1}}
\ead{wangzhihan@cup.edu.cn}

%% Affiliation
\address[inst1]{SClab, China University of Petroleum (Beijing), Beijing 102249, China}

%% Corresponding author
\cortext[cor1]{Corresponding author}

%% Abstract
\begin{abstract}
Urban Air Mobility (UAM) systems face critical challenges in managing vertical airspace congestion as drone traffic increases. This paper addresses the question: \textit{which deep reinforcement learning algorithms are most effective for optimizing vertical layered queueing systems, and what structural configurations maximize performance?} We introduce the MCRPS/D/K queueing framework that models multi-layer correlated arrivals, random batch service, and dynamic inter-layer transfers across five vertical layers. Through systematic evaluation of top-performing DRL algorithms against heuristic baselines, we establish four principal findings. First, DRL algorithms significantly outperform heuristics, with A2C emerging as the top performer. Second, inverted pyramid capacity configurations [8,6,4,3,2] consistently outperform normal pyramid structures by approximately 9.5\% at moderate loads---proven theoretically via optimal capacity allocation ($k_i^* \propto w_i$). Third, we identify a load-dependent capacity paradox: under extreme load conditions ($\geq$8$\times$ baseline), low-capacity systems (K=10) outperform high-capacity systems (K=30+) due to state space explosion ($|\mathcal{S}|_{K=30}/|\mathcal{S}|_{K=10} \approx 69$), though this reverses at moderate loads. Fourth, an ablation study comparing HCA2C with baseline algorithms (A2C, PPO) across three load levels reveals a fundamental performance-stability trade-off: while A2C achieves peak performance, it exhibits high training variance; HCA2C demonstrates exceptional stability but limited performance under extreme load. These findings, validated through extensive experiments and ablation studies, provide evidence-based guidelines for UAM system design while acknowledging the gap between our simplified model and real-world operational complexity.
\end{abstract}

%% Keywords
\begin{keyword}
Deep Reinforcement Learning \sep Urban Air Mobility \sep Queueing Systems \sep Vertical Airspace Management \sep Capacity Planning \sep A2C \sep PPO
\end{keyword}

%% Highlights (3-5 bullet points, max 85 characters each)
\begin{highlights}
\item First systematic evaluation of DRL algorithms for vertical queueing systems
\item DRL significantly outperforms heuristics; A2C is optimal choice
\item Inverted pyramid configuration outperforms normal pyramid by approximately 9.5\% at moderate loads
\item Load-dependent capacity paradox: K=10 beats K=30 only at extreme loads
\item Ablation study proves capacity-aware clipping essential: 66\% degradation without
\end{highlights}

\end{frontmatter}

%% ============================================================================
%% MAIN SECTIONS
%% ============================================================================

%% Section 1: Introduction
\section{Introduction}
\label{sec:introduction}

\subsection{Background and Motivation}
\label{subsec:background}

The Urban Air Mobility (UAM) industry is experiencing unprecedented growth, with market projections indicating substantial expansion by 2030 \cite{uam_market_report}. This growth is driven by rapid advancements in drone delivery services, with companies such as Amazon Prime Air, Wing, and Zipline deploying autonomous aerial vehicles for last-mile logistics \cite{amazon_prime_air, wing_delivery, zipline_drones}. Concurrently, electric vertical takeoff and landing (eVTOL) aircraft development by industry leaders including Joby Aviation, Volocopter, and Lilium promises to revolutionize urban transportation \cite{joby_aviation, volocopter, lilium_evtol}. However, as UAM traffic density increases, a critical challenge emerges: managing vertical airspace congestion to ensure safe and efficient operations.

\textbf{Why Vertical Layering?} Unlike traditional aviation where aircraft operate at well-separated altitudes, UAM vehicles must share a compressed vertical airspace (typically 0--400 ft AGL) in dense urban environments. This necessitates \textbf{vertical layering}---stratifying airspace into altitude-based zones where vehicles queue for service (takeoff, landing, transit). Consider a vertiport serving 100+ drone deliveries per hour: without vertical separation, collision risk becomes unacceptable; with naive first-come-first-served queueing, congestion cascades from lower to upper layers, causing system-wide delays. The challenge is fundamentally a \textit{multi-layer queueing problem} with coupled dynamics: decisions at one altitude affect all others.

\textbf{Why Not Existing Methods?} This vertical airspace management problem involves multiple competing objectives: minimizing waiting times across all layers, maximizing throughput and service efficiency, preventing system crashes and congestion collapse, and balancing load distribution across vertical layers. Conventional approaches face significant limitations:
\begin{itemize}
\item \textit{Heuristic methods} (FCFS, SJF, priority-based) lack adaptability to dynamic traffic conditions and cannot coordinate across layers \cite{heuristic_scheduling}.
\item \textit{Analytical queueing models} (M/M/c, Jackson networks) become intractable with correlated arrivals and dynamic inter-layer transfers \cite{queueing_theory_limits}.
\item \textit{Static capacity allocation} fails to respond to varying load conditions, leading to suboptimal resource utilization \cite{static_allocation}.
\end{itemize}

\textbf{Why DRL?} Deep reinforcement learning offers a compelling alternative: it can learn coordinated, state-dependent policies through interaction with the environment, naturally handling the multi-objective, multi-layer structure of vertical queueing. DRL has demonstrated success in analogous domains---datacenter scheduling \cite{datacenter_scheduling}, network routing \cite{network_routing_drl}, and traffic control \cite{traffic_signal_drl}---but its application to vertical queueing in UAM remains unexplored. This gap motivates our systematic investigation.

\subsection{Literature Review}
\label{subsec:literature}

Classical queueing theory (M/M/c, Jackson networks) provides analytical foundations for service systems \cite{kendall_notation, jackson_networks}, but becomes intractable with realistic features like correlated arrivals and dynamic routing \cite{queueing_complexity}. Deep reinforcement learning has emerged as a powerful alternative for complex optimization, with value-based methods (DQN \cite{dqn}, Rainbow \cite{rainbow}), policy gradient methods (A2C \cite{a2c}, PPO \cite{ppo}), and actor-critic methods (TD3 \cite{td3}, SAC \cite{sac}, TD7 \cite{td7}) achieving success in operations research domains including inventory management, job scheduling, and resource allocation \cite{drl_or_survey}. Recent applications to network routing \cite{network_routing_drl}, datacenter scheduling \cite{datacenter_scheduling}, and traffic control \cite{traffic_signal_drl} demonstrate DRL's potential for dynamic optimization.

\textbf{Comparison with Related DRL Scheduling Work.} Table~\ref{tab:related-work} positions our work relative to seminal DRL scheduling studies. DeepRM \cite{deeprm} pioneered DRL for resource management but addresses single-resource allocation without multi-layer structure. Park \cite{park} and Decima \cite{decima} advanced graph-based scheduling but focus on job DAGs rather than queueing dynamics. Our work uniquely addresses: (1) \textit{vertical multi-layer structure} with inter-layer transfers, (2) \textit{capacity-dependent state spaces} that create the capacity paradox, and (3) \textit{structural configuration optimization} (inverted vs. normal pyramid).

\begin{table}[htbp]
\centering
\caption{Comparison with Related DRL Scheduling Work}
\label{tab:related-work}
\small
\begin{tabular}{lccccc}
\toprule
\textbf{Work} & \textbf{Multi-Layer} & \textbf{Transfers} & \textbf{Capacity} & \textbf{Algorithms} & \textbf{Domain} \\
\midrule
DeepRM \cite{deeprm} & No & No & Fixed & 1 (PG) & Cluster \\
Park \cite{park} & No & No & Fixed & 1 (A3C) & Video \\
Decima \cite{decima} & No & No & Fixed & 1 (GNN) & Spark \\
\textbf{This work} & \textbf{Yes (5)} & \textbf{Yes} & \textbf{Variable} & \textbf{15} & \textbf{UAM} \\
\bottomrule
\end{tabular}
\end{table}

However, critical research gaps remain: (1) \textbf{Methodological gap}: No comprehensive DRL algorithm comparison exists for vertical queueing systems; (2) \textbf{Structural gap}: Optimal capacity configuration for vertical layers is unknown; (3) \textbf{Practical gap}: DRL performance under extreme load conditions is poorly understood; (4) \textbf{UAM gap}: Despite evolving regulatory frameworks (NASA UTM \cite{nasa_utm}, FAA regulations \cite{faa_drone_regulations}) and industry initiatives (Uber Elevate, EHang, Volocopter), DRL-based optimization for vertical layering in UAM remains unexplored, with current approaches relying on static rule-based systems.

\subsection{Research Questions and Objectives}
\label{subsec:research-questions}

The main research question guiding this work is: \textit{Which deep reinforcement learning algorithms are most effective for optimizing vertical layered queueing systems in Urban Air Mobility, and what structural configurations maximize system performance?}

To address this question, we establish five specific research objectives:

\begin{enumerate}
\item \textbf{Algorithm Comparison}: Systematically evaluate top-performing DRL algorithms (A2C, PPO, TD7) against traditional heuristic baselines to identify the most effective approaches for vertical queueing optimization.

\item \textbf{Structural Analysis}: Investigate the impact of capacity configuration (inverted pyramid vs. normal pyramid) on system performance to provide design guidelines for UAM infrastructure.

\item \textbf{Capacity Planning}: Analyze the relationship between total system capacity and performance under varying load conditions to understand capacity-performance trade-offs.

\item \textbf{Practical Insights}: Identify algorithm-specific trade-offs between sample efficiency and performance to inform real-world deployment decisions.

\item \textbf{Generalization Testing}: Validate findings across heterogeneous traffic patterns and system configurations to ensure robustness and practical applicability.
\end{enumerate}

\subsection{Main Contributions}
\label{subsec:contributions}

This research makes the following contributions to the field of deep reinforcement learning and operations research:

\subsubsection{Methodological Contributions}

\begin{enumerate}
\item \textbf{Comprehensive DRL Benchmark}: We present a systematic comparison of top-performing DRL algorithms for vertical queueing systems, providing empirical guidance for algorithm selection in UAM applications.

\item \textbf{MCRPS/D/K Framework}: We introduce an extended queueing framework that incorporates multi-layer correlated arrivals, random batch service, and dynamic inter-layer transfers, capturing the complexity of real-world UAM operations.

\item \textbf{Rigorous Statistical Validation}: We conduct large-scale experiments (500,000 timesteps per algorithm across multiple random seeds) with robust statistical analysis, including effect size calculations (Cohen's d) and significance testing.
\end{enumerate}

\subsubsection{Empirical Findings}

\begin{enumerate}
\item \textbf{DRL Superiority}: We demonstrate that DRL algorithms achieve over 50\% performance improvement compared to traditional heuristic methods, establishing the practical value of DRL for vertical queueing optimization.

\item \textbf{Structural Optimality}: We show that inverted pyramid capacity configurations [8,6,4,3,2] consistently outperform normal pyramid structures, with approximately 9.5\% performance advantage at moderate loads (5$\times$ baseline), providing direct design guidelines for UAM infrastructure.

\item \textbf{Capacity Paradox}: We identify a counter-intuitive phenomenon where low-capacity systems (K=10) outperform high-capacity systems (K=30+) under extreme load conditions, challenging conventional assumptions about capacity planning.

\item \textbf{Algorithm Efficiency}: We find that A2C achieves strong performance among the evaluated algorithms, while PPO offers a robust alternative, informing practical deployment decisions.
\end{enumerate}

\subsubsection{Practical Contributions}

\begin{enumerate}
\item \textbf{Design Guidelines}: We provide actionable recommendations for UAM infrastructure capacity allocation based on empirical evidence and statistical validation.

\item \textbf{Algorithm Selection Framework}: We offer a practical trade-off analysis between training efficiency and performance for real-world deployment scenarios.

\item \textbf{Generalization Validation}: We demonstrate robustness across 5 heterogeneous traffic patterns and multiple capacity configurations, ensuring practical applicability.

\item \textbf{Architectural Validation}: We conduct comprehensive ablation studies demonstrating that capacity-aware action clipping is essential for system stability. Removing this constraint leads to 100\% crash rate despite identical network capacity, validating that HCA2C's performance stems from architectural design beyond parameter scaling.
\end{enumerate}

\subsection{Paper Organization}
\label{subsec:organization}

The remainder of this paper is organized as follows. Section~\ref{sec:methodology} introduces the MCRPS/D/K queueing framework, describes the DRL algorithms evaluated, details the experimental design including training parameters and evaluation metrics, and explains the statistical analysis approach. Section~\ref{sec:results} presents the main findings organized into subsections covering algorithm performance comparison, structural analysis, capacity paradox investigation, and generalization testing. Section~\ref{sec:discussion} interprets the empirical findings, provides theoretical explanations for observed phenomena, discusses practical implications for UAM system design, acknowledges limitations, and proposes future research directions. Section~\ref{sec:conclusion} summarizes the key contributions, highlights actionable insights for practitioners, and emphasizes the broader impact of this research on DRL applications in operations research.

%% Section 2: Methodology
\section{Methodology}
\label{sec:methodology}

\subsection{MCRPS/D/K Queueing Framework}
\label{subsec:framework}

We introduce the MCRPS/D/K queueing framework to model vertical layered queueing systems for UAM airspace management. The framework extends classical queueing notation to incorporate multi-layer correlated arrivals (MC), random batch service (R-S), pressure-based dynamics (P), and dynamic inter-layer transfers (D) with finite capacity constraints (K).

\subsubsection{MDP Formulation}

We formulate the vertical queueing optimization problem as a Markov Decision Process (MDP), defined by the tuple $\langle \mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R}, \gamma \rangle$, where:

The \textbf{state space} $\mathcal{S}$ captures the complete system configuration at each timestep. Each state $s \in \mathcal{S}$ is a 29-dimensional vector:

\begin{equation}
s = [q_0, \ldots, q_4, k_0, \ldots, k_4, \frac{q_0}{k_0}, \ldots, \frac{q_4}{k_4}, \mu_0, \ldots, \mu_4, \lambda_0, \ldots, \lambda_4, t, \sum_{i=0}^{4} q_i, \bar{w}, c]
\label{eq:state-space}
\end{equation}

where $q_i$ denotes queue length at layer $i$, $k_i$ is capacity, $\mu_i$ is service rate, $\lambda_i$ is arrival rate, $t$ is current timestep, $\bar{w}$ is average waiting time, and $c$ is a crash indicator.

The \textbf{action space} $\mathcal{A}$ consists of 11-dimensional continuous control vectors:

\begin{equation}
a = [p_0, \ldots, p_4, T_{01}, T_{12}, T_{23}, T_{34}, \alpha_0, \alpha_4] \in [0,1]^5 \times [-1,1]^4 \times [0,1]^2
\label{eq:action-space}
\end{equation}

where $p_i \in [0,1]$ represents service allocation priority for layer $i$, $T_{ij} \in [-1,1]$ controls inter-layer transfers between adjacent layers, and $\alpha_0, \alpha_4 \in [0,1]$ govern admission control at boundary layers.

The \textbf{transition probability} $\mathcal{P}: \mathcal{S} \times \mathcal{A} \times \mathcal{S} \rightarrow [0,1]$ defines the system dynamics:

\begin{equation}
\mathcal{P}(s'|s,a) = P(s_{t+1} = s' | s_t = s, a_t = a)
\label{eq:transition}
\end{equation}

The transition function is stochastic due to random arrivals (Poisson process) and batch service selection (uniform distribution).

The \textbf{reward function} $\mathcal{R}: \mathcal{S} \times \mathcal{A} \times \mathcal{S} \rightarrow \mathbb{R}$ quantifies system performance:

\begin{equation}
\mathcal{R}(s,a,s') = R_{\text{throughput}} + R_{\text{wait}} + R_{\text{queue}} + R_{\text{crash}} + R_{\text{balance}} + R_{\text{transfer}}
\label{eq:reward-mdp}
\end{equation}

A \textbf{policy} $\pi: \mathcal{S} \rightarrow \mathcal{P}(\mathcal{A})$ maps states to probability distributions over actions. The DRL algorithms learn a parameterized policy $\pi_\theta$ that maximizes expected cumulative reward.

The \textbf{value function} under policy $\pi$ is defined as:

\begin{equation}
V^\pi(s) = \mathbb{E}_\pi \left[ \sum_{t=0}^{\infty} \gamma^t R_t \mid s_0 = s \right]
\label{eq:value-function}
\end{equation}

where $\gamma \in [0,1]$ is the discount factor (set to 0.99 in our experiments).

The \textbf{action-value function} (Q-function) is:

\begin{equation}
Q^\pi(s,a) = \mathbb{E}_\pi \left[ \sum_{t=0}^{\infty} \gamma^t R_t \mid s_0 = s, a_0 = a \right]
\label{eq:q-function}
\end{equation}

The optimal policy $\pi^*$ satisfies the Bellman optimality equation:

\begin{equation}
V^*(s) = \max_{a \in \mathcal{A}} \left[ \mathcal{R}(s,a) + \gamma \sum_{s' \in \mathcal{S}} \mathcal{P}(s'|s,a) V^*(s') \right]
\label{eq:bellman}
\end{equation}

This MDP formulation provides the mathematical foundation for applying DRL algorithms to the vertical queueing optimization problem.

\subsubsection{System Architecture}

The system consists of five vertical layers ($L_0$ to $L_4$) representing altitude zones in UAM airspace, consistent with the layered airspace structure defined in NASA's UTM Concept of Operations \cite{nasa_utm_conops} and FAA's UAS Traffic Management framework \cite{faa_utm}. The five-layer design corresponds to typical altitude stratification in low-altitude UAM operations (0--400 ft AGL), where each layer spans approximately 80 ft of vertical separation to ensure safe drone operations \cite{uam_airspace_design}. Each layer $i$ has finite capacity $k_i$, forming configuration vector $\mathbf{K} = [k_0, k_1, k_2, k_3, k_4]$. The service mechanism employs batch processing with random selection, and dynamic transfers between adjacent layers enable adaptive load balancing.

\begin{figure}[!htb]
\centering
\includegraphics[width=0.85\textwidth]{figures/fig1.png}
\caption{System Architecture: The DRL-based MCRPS/D/K system showing the interaction between the environment (left) comprising arrival processes, five vertical queue layers, and servers, and the DRL agent (right) comprising actor-critic neural networks and replay buffer.}
\label{fig:system-architecture}
\end{figure}

\subsubsection{Queue Dynamics and Constraints}

Queue length at layer $i$ evolves as: $q_i(t+1) = q_i(t) + A_i(t) - D_i(t) + \sum_{j \neq i} T_{ji}(t) - \sum_{j \neq i} T_{ij}(t)$, where $A_i(t)$ denotes arrivals, $D_i(t)$ represents departures, and $T_{ij}(t)$ is transfer volume. Each layer enforces strict capacity constraints: $0 \leq q_i(t) \leq k_i$. The system crashes if any layer exceeds capacity: $\text{Crash}(t) = \mathbb{I}(\exists i: q_i(t) > k_i)$.

Arrivals follow a Poisson process with rate $\lambda_{\text{total}}$, split across layers using weights $\mathbf{w} = [0.30, 0.25, 0.20, 0.15, 0.10]$. These weights reflect the empirically observed traffic density distribution in UAM operations, where lower altitudes experience higher traffic volumes due to takeoff/landing operations, last-mile delivery patterns, and proximity to vertiports \cite{uam_traffic_patterns, thipphavong2018uam}. Service capacity at layer $i$ is $s_i = \min(q_i, \mu_i)$, with batch size $B_i \sim \text{Uniform}(1, s_i)$. Transfers occur when pressure differentials ($p_i = q_i/k_i$) exceed thresholds, with volume $T_{ij}(t) = \min(\lfloor \delta \cdot q_i(t) \rfloor, k_j - q_j(t))$ when $p_i > \theta_{\text{up}}$ and $p_j < \theta_{\text{down}}$.

We evaluate three capacity configurations: \textbf{Inverted pyramid} $[8,6,4,3,2]$, \textbf{Normal pyramid} $[2,3,4,6,8]$, and \textbf{Uniform} $[k,k,k,k,k]$ (total $K=23$ for pyramids).

\subsection{Deep Reinforcement Learning Algorithms}
\label{subsec:drl-algorithms}

We evaluate top-performing DRL algorithms spanning multiple categories, providing comprehensive coverage of modern DRL approaches for operations research applications.

\subsubsection{Algorithm Categories}

\textbf{Policy Gradient Methods:} We evaluate Advantage Actor-Critic (A2C) \cite{a2c}, a synchronous variant of A3C with advantage estimation, and Proximal Policy Optimization (PPO) \cite{ppo}, which employs a clipped surrogate objective for stable policy updates.

\textbf{Actor-Critic Methods:} This category includes Twin Delayed Deep Deterministic Policy Gradient (TD3) \cite{td3}, which addresses overestimation bias through twin Q-networks; Soft Actor-Critic (SAC) \cite{sac}, employing a maximum entropy framework for exploration; TD7 \cite{td7}, an enhanced version of TD3 with seven algorithmic improvements; and Deep Deterministic Policy Gradient (DDPG) \cite{ddpg} for deterministic continuous control.

\textbf{Value-Based Methods:} We include Deep Q-Network (DQN) \cite{dqn} for Q-value approximation, Rainbow \cite{rainbow} combining six DQN extensions (double Q-learning, dueling architecture, prioritized replay, multi-step learning, distributional RL, and noisy networks), and Recurrent Replay Distributed DQN (R2D2) \cite{r2d2} with recurrent architecture for partial observability.

\textbf{Distributed and Advanced Methods:} This category encompasses IMPALA (Importance Weighted Actor-Learner Architecture) \cite{impala} with decoupled acting and learning, APEX-DQN \cite{apex} with distributed prioritized experience replay, Quantile Regression DQN (QRDQN) \cite{qrdqn} for distributional RL, C51 \cite{c51} for categorical distributional RL, and Implicit Quantile Networks (IQN) \cite{iqn} for implicit quantile function approximation.

\textbf{Hierarchical Architecture:} We also evaluate HCA2C (Hierarchical Capacity-Aware A2C), a custom architecture that extends A2C with capacity-aware action clipping and hierarchical policy decomposition. HCA2C is designed to encode domain knowledge about capacity constraints directly into the policy architecture, providing a comparison point for understanding the value of architectural inductive biases versus standard algorithm improvements.

The DRL algorithms are implemented using standard architectures with fully connected networks. We conducted preliminary experiments evaluating all 15 algorithms (11 DRL algorithms + 4 heuristic baselines) under identical conditions. Figure~\ref{fig:preliminary-ranking} presents the complete performance ranking from these preliminary experiments, revealing a clear performance hierarchy with three distinct tiers: Top-Tier ($>$4,000 reward: A2C, PPO, TD7, R2D2, SAC), Mid-Tier (2,000--4,000: TD3, Heuristic, Rainbow, Priority, FCFS, SJF), and Low-Tier ($<$2,000: IMPALA, DDPG, Random).

Based on these preliminary results, we selected the top 3 performing algorithms (A2C, PPO, TD7) for comprehensive validation testing, as they consistently achieved Top-Tier performance with rewards exceeding 4,300. This selection strategy ensures that our detailed analysis focuses on the most promising candidates while the preliminary screening provides a complete algorithmic landscape. Table~\ref{tab:algorithm-summary} summarizes all evaluated algorithms with their performance tiers.

\begin{figure}[!htb]
\centering
\includegraphics[width=0.85\textwidth]{figures/figure3_performance_ranking.png}
\caption{Preliminary Algorithm Performance Ranking: Complete evaluation of 15 algorithms (11 DRL + 4 heuristic baselines) under identical conditions. Performance tiers are indicated by color: Top-Tier (green, $>$4,000), Mid-Tier (orange, 2,000--4,000), and Low-Tier (red, $<$2,000). Based on these results, we selected the top 3 performers (A2C, PPO, TD7) for comprehensive validation testing.}
\label{fig:preliminary-ranking}
\end{figure}

\begin{table}[htbp]
\centering
\caption{Summary of All Evaluated Algorithms (15 Total)}
\label{tab:algorithm-summary}
\small
\begin{tabular}{llccp{3.2cm}}
\toprule
\textbf{Algorithm} & \textbf{Type} & \textbf{Mean Reward} & \textbf{Tier} & \textbf{Reference} \\
\midrule
\multicolumn{5}{l}{\textit{Top-Tier Performers (Selected for Validation)}} \\
A2C & On-policy & 4,438 & Top & \cite{mnih2016a2c} \\
PPO & On-policy & 4,420 & Top & \cite{schulman2017ppo} \\
TD7 & Off-policy & 4,352 & Top & \cite{fujimoto2023td7} \\
\midrule
\multicolumn{5}{l}{\textit{Other DRL Algorithms}} \\
R2D2 & Value-based & 4,289 & Top & \cite{kapturowski2018r2d2} \\
SAC & Off-policy & 4,283 & Top & \cite{haarnoja2018sac} \\
TD3 & Off-policy & 3,973 & Mid & \cite{fujimoto2018td3} \\
Rainbow & Value-based & 2,361 & Mid & \cite{hessel2018rainbow} \\
IMPALA & Distributed & 1,682 & Low & \cite{espeholt2018impala} \\
DDPG & Off-policy & 1,490 & Low & \cite{lillicrap2015ddpg} \\
\midrule
\multicolumn{5}{l}{\textit{Heuristic Baselines}} \\
Heuristic & Rule-based & 2,861 & Mid & --- \\
Priority & Rule-based & 2,040 & Mid & --- \\
FCFS & Rule-based & 2,025 & Mid & --- \\
SJF & Rule-based & 2,011 & Mid & --- \\
Random & Baseline & 295 & Low & --- \\
\bottomrule
\end{tabular}
\vspace{0.1cm}

\small\textit{Note: All 15 algorithms were evaluated in preliminary experiments. Top 3 performers (A2C, PPO, TD7) were selected for comprehensive validation testing across multiple configurations and load conditions.}
\end{table}

\subsubsection{State and Action Space Design}

The state space comprises 29 dimensions capturing comprehensive system information: queue lengths ($q_0, \ldots, q_4$), capacities ($k_0, \ldots, k_4$), utilization ratios ($q_i/k_i$), service rates ($\mu_0, \ldots, \mu_4$), arrival rates ($\lambda_0, \ldots, \lambda_4$), current timestep, total system load ($\sum q_i$), average waiting time, and a crash indicator flag.

The action space consists of 11 continuous dimensions: service allocation priorities for each layer (5 dimensions, range [0,1]), inter-layer transfer decisions for adjacent layer pairs (4 dimensions, range [-1,1]), and admission control decisions for top and bottom layers (2 dimensions, range [0,1]).

\textbf{Action-to-System Mapping.} The continuous action outputs are mapped to discrete system operations as follows:
\begin{itemize}
\item \textit{Service priorities} $p_i \in [0,1]$: The effective service rate at layer $i$ is scaled by $(0.5 + 0.5 \cdot p_i)$, so $p_i = 0$ yields 50\% of base rate and $p_i = 1$ yields 100\%. For example, if $p_0 = 0.73$, layer 0 operates at $0.5 + 0.5 \times 0.73 = 86.5\%$ of its maximum service rate $\mu_0$.
\item \textit{Transfer decisions} $T_{ij} \in [-1,1]$: Transfers occur when $|T_{ij}| > 0.3$ (threshold). The transfer volume is $\lfloor |T_{ij}| \times 0.3 \times q_i \rfloor$ units, with direction determined by sign. For example, $T_{01} = 0.8$ triggers transfer of $\lfloor 0.8 \times 0.3 \times q_0 \rfloor$ units from layer 0 to layer 1.
\item \textit{Admission control} $\alpha_i \in [0,1]$: Incoming arrivals at boundary layers are accepted with probability $\alpha_i$. For example, $\alpha_0 = 0.6$ means 60\% of arrivals at layer 0 are admitted.
\end{itemize}

\textbf{Design Rationale:} The state space design follows three key principles. First, \textit{Markov property preservation} requires including all information necessary for optimal decision-making without requiring history. Queue lengths and capacities provide instantaneous system state, while utilization ratios ($q_i/k_i$) enable pressure-based reasoning. Second, \textit{temporal awareness} through the timestep variable allows the agent to learn time-dependent patterns in arrival processes. Third, \textit{safety monitoring} via the crash indicator enables the agent to learn crash-avoidance behaviors through the large negative penalty ($w_4 = 10000$).

The action space design balances expressiveness with learning tractability. Service allocation priorities (5 dimensions) enable fine-grained control over layer-specific service rates, allowing the agent to prioritize high-pressure layers dynamically. Inter-layer transfers (4 dimensions) provide load balancing capabilities between adjacent layers, with the range [-1,1] allowing bidirectional transfers. Admission control at boundary layers (2 dimensions) prevents system overload by rejecting arrivals when necessary. The continuous action space (rather than discrete) enables smooth policy gradients and is well-suited for policy gradient methods (A2C, PPO) and actor-critic algorithms (TD3, SAC, TD7).

\subsubsection{Reward Function: Weighted Sum Scalarization}

The vertical queueing optimization involves multiple competing objectives that must be aggregated into a scalar reward signal for DRL training. We employ the \textit{weighted sum scalarization} method from multi-objective optimization theory, which transforms the vector-valued objective $\mathbf{J}(\pi) \in \mathbb{R}^m$ into a scalar reward:

\begin{equation}
R(s, a, s') = \sum_{j=1}^{m} w_j \cdot f_j(s, a, s')
\label{eq:scalarization}
\end{equation}

where $\mathbf{w} = [w_1, \ldots, w_m]^T$ is the weight vector satisfying $w_j \geq 0$ and $f_j(\cdot)$ are the individual objective functions. This scalarization approach guarantees that the optimal policy $\pi^*$ lies on the Pareto front when $w_j > 0$ for all $j$ \cite{miettinen1999nonlinear}.

\textbf{Hierarchical Objective Structure.} We decompose the reward into six objective functions organized in a \textit{lexicographic priority structure}:

\begin{equation}
R(t) = \underbrace{R_{\text{crash}}(t)}_{\text{Tier 1: Safety}} + \underbrace{R_{\text{throughput}}(t)}_{\text{Tier 2: Performance}} + \underbrace{R_{\text{wait}}(t) + R_{\text{queue}}(t) + R_{\text{balance}}(t) + R_{\text{transfer}}(t)}_{\text{Tier 3: Quality of Service}}
\label{eq:reward-hierarchical}
\end{equation}

Table~\ref{tab:reward-components} specifies the mathematical formulation of each objective function, where the weight magnitudes encode the lexicographic priority: $w_4 \gg w_1 > w_5 > w_6 > w_2 > w_3$.

\begin{table}[htbp]
\centering
\caption{Reward Function Components with Lexicographic Priority Structure}
\label{tab:reward-components}
\small
\begin{tabular}{clccc}
\toprule
\textbf{Tier} & \textbf{Objective} & \textbf{Formulation $f_j(s,a,s')$} & \textbf{Weight} & \textbf{Priority} \\
\midrule
1 & Safety & $-\mathbb{I}(\exists i: q_i > k_i)$ & $w_4 = 10^4$ & Dominant \\
\midrule
2 & Throughput & $\sum_{i=0}^{4} D_i(t)$ & $w_1 = 1.0$ & Primary \\
\midrule
\multirow{4}{*}{3} & Balance & $-\sigma(\rho_0, \ldots, \rho_4)$ & $w_5 = 0.5$ & Secondary \\
 & Transfer & $\sum_{i,j} \mathbb{I}(T_{ij} > 0)$ & $w_6 = 0.2$ & Secondary \\
 & Waiting & $-\sum_{i=0}^{4} \bar{w}_i(t)$ & $w_2 = 0.1$ & Tertiary \\
 & Queue & $-\sum_{i=0}^{4} q_i(t)$ & $w_3 = 0.05$ & Tertiary \\
\bottomrule
\end{tabular}
\end{table}

\noindent Here $D_i(t)$ denotes service completions, $\bar{w}_i(t)$ is mean waiting time, $\rho_i = q_i/k_i$ is utilization ratio, $\sigma(\cdot)$ computes standard deviation, and $T_{ij}$ represents inter-layer transfer volume.

\textbf{Theoretical Justification.} The weight structure implements an \textit{$\epsilon$-constraint approximation} to lexicographic optimization: the safety constraint ($w_4 = 10^4$) ensures that any policy violating capacity constraints is strictly dominated, as the crash penalty exceeds the maximum achievable reward from all other objectives combined. This guarantees that the learned policy satisfies the hard constraint $P(\text{crash}) \approx 0$ before optimizing secondary objectives.

The robustness of this scalarization is empirically validated in Section~\ref{subsec:generalization}: four diverse weight configurations produce identical structural rankings with zero variance, confirming that the observed performance differences reflect fundamental system properties rather than reward engineering artifacts.

\subsection{Experimental Design}
\label{subsec:experimental-design}

\subsubsection{Training and Evaluation Protocol}

All algorithms were trained for 500,000 timesteps using the Stable-Baselines3 framework \cite{stable_baselines3}. To ensure reproducibility, we employed five fixed random seeds (42, 43, 44, 45, 46) for each algorithm. Evaluation was conducted every 10,000 timesteps during training, with each evaluation comprising 50 episodes using deterministic policies (no exploration noise). We recorded mean episode reward, standard deviation, mean episode length, and crash rate (percentage of episodes ending in capacity violations).

\subsubsection{Training Hyperparameters}

We provide comprehensive hyperparameter specifications to ensure reproducibility across all algorithms.

\begin{table}[htbp]
\centering
\caption{Common Hyperparameters Across All DRL Algorithms}
\label{tab:hyperparams-common}
\begin{tabular}{lll}
\toprule
\textbf{Hyperparameter} & \textbf{Value} & \textbf{Justification} \\
\midrule
Learning rate & $3 \times 10^{-4}$ & Standard for policy gradient methods \\
Discount factor $\gamma$ & 0.99 & Standard for episodic tasks \\
Batch size & 64 & Balance stability and efficiency \\
Replay buffer size & 100,000 & Sufficient for 500K timesteps \\
Training frequency & Every 4 steps & Standard for off-policy methods \\
Gradient clipping & 0.5 & Prevent exploding gradients \\
Random seeds & [42, 43, 44, 45, 46] & Ensure reproducibility \\
Total timesteps & 500,000 & Sufficient for convergence \\
Evaluation frequency & Every 10,000 steps & Track learning progress \\
Evaluation episodes & 50 & Reduce variance in estimates \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[htbp]
\centering
\caption{Algorithm-Specific Hyperparameters}
\label{tab:hyperparams-specific}
\begin{tabular}{lll}
\toprule
\textbf{Algorithm} & \textbf{Hyperparameter} & \textbf{Value} \\
\midrule
\multirow{3}{*}{PPO} & Clip range $\epsilon$ & 0.2 \\
 & Number of epochs & 10 \\
 & GAE lambda $\lambda$ & 0.95 \\
\midrule
\multirow{2}{*}{SAC} & Temperature $\alpha$ & 0.2 (auto-tuned) \\
 & Target entropy & $-\dim(\mathcal{A})$ \\
\midrule
\multirow{3}{*}{TD3/TD7} & Policy delay & 2 \\
 & Target policy noise & 0.2 \\
 & Noise clip & 0.5 \\
\midrule
\multirow{3}{*}{Rainbow} & N-step returns & 3 \\
 & Prioritized replay $\alpha$ & 0.6 \\
 & Importance sampling $\beta$ & $0.4 \rightarrow 1.0$ \\
\midrule
\multirow{2}{*}{R2D2} & LSTM hidden size & 512 \\
 & Burn-in period & 40 steps \\
\midrule
\multirow{2}{*}{IMPALA} & V-trace $\bar{\rho}$ & 1.0 \\
 & V-trace $\bar{c}$ & 1.0 \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Baseline Implementations}

We compare DRL algorithms against four traditional heuristic baselines: (1) First-Come-First-Served (FCFS), serving requests in arrival order without prioritization; (2) Shortest Job First (SJF), prioritizing requests with shortest expected service time; (3) Priority-Based scheduling, assigning priority based on layer position to reflect altitude-based urgency; and (4) a custom Adaptive Heuristic combining load balancing, pressure-based transfers, and threshold-based admission control. Detailed algorithmic descriptions for all heuristic baselines are provided in Table~\ref{tab:algorithm-summary} and supplementary materials.

\subsubsection{Computational Infrastructure}

All experiments were conducted on a high-performance computing system with the following specifications: NVIDIA RTX 3090 GPU (24GB VRAM), 32GB RAM, and Intel i9-10900K CPU. The software environment consisted of Python 3.8, PyTorch 1.10, Stable-Baselines3 1.5.0, and Gym 0.21. All algorithms were trained sequentially using five random seeds (42-46), with deterministic evaluation to ensure reproducibility.

\subsubsection{Reproducibility}

To ensure full reproducibility of our results, we provide the following specifications: (1) Fixed random seeds [42, 43, 44, 45, 46] were used for all experiments; (2) Deterministic evaluation was employed with no exploration noise during testing; (3) The custom MCRPS/D/K environment (version 1.0) was used consistently across all experiments; (4) All hyperparameters are documented in Tables~\ref{tab:hyperparams-common} and~\ref{tab:hyperparams-specific}; (5) Network architectures follow standard configurations with fully connected layers (256-256 hidden units) as detailed in the supplementary materials; (6) Code and trained models will be made available upon publication; (7) Training logs and evaluation results are available for verification; (8) Hyperparameter sensitivity was validated across four diverse reward configurations, demonstrating robustness to weight specifications.

\subsubsection{Ablation Studies}

We conducted three systematic ablation studies. Study 1 (Structural Comparison) compared inverted pyramid [8,6,4,3,2] versus normal pyramid [2,3,4,6,8] configurations at 5$\times$ baseline load using A2C and PPO (n=30 per algorithm per structure, total n=60 per structure). Study 2 (Capacity Scan) tested total capacities $K \in \{10, 15, 20, 25, 30, 40\}$ under 10$\times$ extreme load across uniform, inverted, and reverse pyramid shapes to identify the capacity paradox. Study 3 (Generalization Testing) validated findings across 5 heterogeneous traffic patterns with varying arrival weights and service rates using the top 3 performers (A2C, PPO, TD7).

\subsection{Statistical Analysis Methods}
\label{subsec:statistical-analysis}

We employ independent samples t-tests to evaluate hypotheses that DRL algorithms outperform heuristics and that inverted pyramid configurations outperform normal pyramids. We report mean, standard deviation, standard error ($SE = \sigma/\sqrt{n}$), t-statistics, p-values, Cohen's d effect sizes ($d = (\mu_1 - \mu_2) / \sigma_{\text{pooled}}$), and 95\% confidence intervals. All experiments use fixed random seeds (42-46) with deterministic evaluation.

This study reports Cohen's d ranging from d=0.28 (small) to d=412.62 (extremely large) depending on load. While d>300 may appear unusual, these values are legitimate in computational experiments with converged algorithms and low variance. At high loads (7$\times$-10$\times$), coefficient of variation (CV) falls below 0.1\%, producing large d values when $\sigma_{\text{pooled}}$ is small. Effect sizes increase with load because variance decreases as system behavior becomes deterministic under stress: 3$\times$ load (d=0.28, CV=2.1\%), 5$\times$ load (d=6.31, CV=0.12\%), 7$\times$ load (d=302.55, CV=0.05\%), 10$\times$ load (d=412.62, CV=0.02\%). We focus on practical significance (approximately 9.5\% improvement at 5$\times$ load) and report CV alongside effect sizes for interpretation.

\subsection{Theoretical Analysis}
\label{subsec:theoretical-analysis}

This section establishes the theoretical foundations for our multi-objective optimization approach, including formal definitions of Pareto optimality, knee point detection methods, and complexity analysis that explains the observed capacity paradox.

\subsubsection{Multi-Objective Optimization Framework}

The vertical queueing optimization problem is inherently multi-objective, requiring simultaneous optimization of competing performance metrics. We formulate this as a multi-objective optimization problem (MOOP) with six objectives:

\begin{definition}[Multi-Objective Optimization Problem]
The vertical queueing MOOP seeks a policy $\pi^*$ that maximizes the objective vector:
\begin{equation}
\max_{\pi \in \Pi} \mathbf{J}(\pi) = [J_1(\pi), J_2(\pi), J_3(\pi), J_4(\pi), J_5(\pi), J_6(\pi)]^T
\label{eq:moo-vector}
\end{equation}
where the six objectives are:
\begin{itemize}
\item $J_1(\pi)$: \textbf{Throughput} -- total requests served per episode
\item $J_2(\pi)$: \textbf{Load Balance} -- uniformity of utilization across layers (1 - Gini coefficient)
\item $J_3(\pi)$: \textbf{Efficiency} -- throughput per unit resource consumption
\item $J_4(\pi)$: \textbf{Transfer Efficiency} -- successful inter-layer transfers
\item $J_5(\pi)$: \textbf{Stability} -- inverse of crash probability
\item $J_6(\pi)$: \textbf{Anti-Penalty} -- avoidance of queue overflow penalties
\end{itemize}
\end{definition}

These objectives exhibit inherent conflicts: maximizing throughput ($J_1$) may compromise stability ($J_5$) under high load, while aggressive load balancing ($J_2$) may reduce transfer efficiency ($J_4$). This conflict structure necessitates Pareto-based analysis.

\subsubsection{Pareto Optimality Theory}

\begin{definition}[Pareto Dominance]
A policy $\pi_a$ \textit{Pareto dominates} policy $\pi_b$, denoted $\pi_a \succ \pi_b$, if and only if:
\begin{equation}
\forall i \in \{1,\ldots,6\}: J_i(\pi_a) \geq J_i(\pi_b) \quad \land \quad \exists j \in \{1,\ldots,6\}: J_j(\pi_a) > J_j(\pi_b)
\label{eq:pareto-dominance}
\end{equation}
\end{definition}

\begin{definition}[Pareto Optimal Set]
The Pareto optimal set $\mathcal{P}^*$ contains all non-dominated policies:
\begin{equation}
\mathcal{P}^* = \{\pi \in \Pi : \nexists \pi' \in \Pi \text{ such that } \pi' \succ \pi\}
\label{eq:pareto-set}
\end{equation}
The image of $\mathcal{P}^*$ in objective space forms the \textit{Pareto front} $\mathcal{F}^* = \{\mathbf{J}(\pi) : \pi \in \mathcal{P}^*\}$.
\end{definition}

\begin{theorem}[Non-Dominated Sorting Complexity]
\label{thm:nds-complexity}
The non-dominated sorting algorithm correctly identifies all Pareto optimal solutions from a population of $N$ solutions with $M$ objectives in time complexity $\mathcal{O}(MN^2)$.
\end{theorem}

\begin{proof}[Proof Sketch]
For each solution, dominance comparison against all other solutions requires $\mathcal{O}(M)$ comparisons per pair, yielding $\mathcal{O}(MN)$ per solution and $\mathcal{O}(MN^2)$ total. The algorithm correctly identifies non-dominated solutions by exhaustive pairwise comparison, ensuring completeness.
\end{proof}

\subsubsection{Knee Point Detection Theory}

Among Pareto optimal solutions, \textit{knee points} represent particularly desirable trade-offs where small improvements in one objective require large sacrifices in others.

\begin{definition}[Knee Point]
A solution $\pi^* \in \mathcal{P}^*$ is a \textit{knee point} if it maximizes the composite score:
\begin{equation}
S(\pi) = \alpha \cdot Q(\pi) + \beta \cdot D(\pi) + \gamma \cdot B(\pi)
\label{eq:knee-score}
\end{equation}
where $Q(\pi)$ is the quality score, $D(\pi)$ is the diversity score, $B(\pi)$ is the balance score, and $\alpha + \beta + \gamma = 1$ are weighting coefficients. We use $\alpha = 0.4$, $\beta = 0.4$, $\gamma = 0.2$, following the principle established by Branke et al. \cite{branke2004finding} that quality and diversity should receive equal emphasis in knee point identification, as a good knee point must be both high-performing and representative of a distinct trade-off region. The lower weight for balance ($\gamma = 0.2$) treats objective uniformity as a secondary criterion, acknowledging that some degree of specialization may be acceptable in practical deployments.
\end{definition}

The three component scores are defined as follows:

\textbf{Quality Score} measures proximity to the ideal point $\mathbf{J}^{\text{ideal}} = [1, 1, 1, 1, 1, 1]^T$ in normalized objective space:
\begin{equation}
Q(\pi) = 1 - \frac{\|\hat{\mathbf{J}}(\pi) - \mathbf{1}\|_2}{\max_{\pi' \in \mathcal{P}^*} \|\hat{\mathbf{J}}(\pi') - \mathbf{1}\|_2}
\label{eq:quality-score}
\end{equation}
where $\hat{\mathbf{J}}(\pi) = [\hat{J}_1(\pi), \ldots, \hat{J}_6(\pi)]^T$ is the min-max normalized objective vector with $\hat{J}_i(\pi) = (J_i(\pi) - J_i^{\min}) / (J_i^{\max} - J_i^{\min})$.

\textbf{Diversity Score} measures local sparsity using $k$-nearest neighbor distances:
\begin{equation}
D(\pi) = \frac{1}{k} \sum_{j=1}^{k} d(\pi, \pi_j^{\text{NN}})
\label{eq:diversity-score}
\end{equation}
where $\pi_j^{\text{NN}}$ denotes the $j$-th nearest neighbor on the Pareto front in objective space, and $d(\cdot, \cdot)$ is Euclidean distance. We use $k = 5$ in our analysis.

\textbf{Balance Score} penalizes solutions with extreme trade-offs using the coefficient of variation:
\begin{equation}
B(\pi) = \frac{1}{1 + \text{CV}(\hat{\mathbf{J}}(\pi))}
\label{eq:balance-score}
\end{equation}
where $\text{CV}(\mathbf{x}) = \sigma(\mathbf{x}) / \mu(\mathbf{x})$ is the coefficient of variation.

\begin{proposition}[Knee Point Characterization]
Knee points identified by Equation~\eqref{eq:knee-score} satisfy three desirable properties: (1) high overall performance (quality), (2) representation of distinct trade-off regions (diversity), and (3) balanced objective achievement without extreme sacrifices (balance).
\end{proposition}

\subsubsection{Hypervolume Indicator}

The hypervolume indicator provides a scalar measure of Pareto front quality:

\begin{definition}[Hypervolume]
The hypervolume of Pareto front $\mathcal{F}^*$ with respect to reference point $\mathbf{r}$ is:
\begin{equation}
\text{HV}(\mathcal{F}^*, \mathbf{r}) = \text{Vol}\left(\bigcup_{\mathbf{J} \in \mathcal{F}^*} [\mathbf{J}, \mathbf{r}]\right)
\label{eq:hypervolume}
\end{equation}
where $[\mathbf{J}, \mathbf{r}]$ denotes the hyperrectangle bounded by $\mathbf{J}$ and $\mathbf{r}$.
\end{definition}

\begin{theorem}[Hypervolume Monotonicity]
\label{thm:hv-monotone}
If $\mathcal{F}_1^* \subset \mathcal{F}_2^*$ in the Pareto dominance sense (every point in $\mathcal{F}_1^*$ is dominated by some point in $\mathcal{F}_2^*$), then $\text{HV}(\mathcal{F}_1^*, \mathbf{r}) \leq \text{HV}(\mathcal{F}_2^*, \mathbf{r})$.
\end{theorem}

This monotonicity property makes hypervolume a reliable metric for comparing solution quality across different algorithms and configurations.

\subsubsection{State Space Complexity Analysis}

\begin{theorem}[State Space Explosion]
\label{thm:state-space}
For a vertical queueing system with $L$ layers and capacity vector $\mathbf{K} = [k_0, \ldots, k_{L-1}]$, the state space size is:
\begin{equation}
|\mathcal{S}| = \prod_{i=0}^{L-1} (k_i + 1)
\label{eq:state-space-size}
\end{equation}
\end{theorem}

\begin{proof}
Each layer $i$ can have queue length $q_i \in \{0, 1, \ldots, k_i\}$, yielding $(k_i + 1)$ possible values. The total state space is the Cartesian product of individual layer states, giving $|\mathcal{S}| = \prod_{i=0}^{L-1} (k_i + 1)$.
\end{proof}

\begin{corollary}[Capacity-Complexity Relationship]
For uniform capacity distribution $k_i = K/L$:
\begin{equation}
|\mathcal{S}| = \left(\frac{K}{L} + 1\right)^L \approx e^{L \ln(K/L + 1)}
\label{eq:uniform-complexity}
\end{equation}
showing exponential growth in both total capacity $K$ and number of layers $L$.
\end{corollary}

For our five-layer system: inverted pyramid $[8,6,4,3,2]$ yields $|\mathcal{S}| = 9 \times 7 \times 5 \times 4 \times 3 = 3,780$ states; uniform $[6,6,6,6,6]$ yields $|\mathcal{S}| = 7^5 = 16,807$ states; and uniform $[10,10,10,10,10]$ yields $|\mathcal{S}| = 11^5 = 161,051$ states.

\subsubsection{Sample Complexity Bounds}

\begin{theorem}[Learning Complexity]
\label{thm:sample-complexity}
The sample complexity for learning an $\epsilon$-optimal policy in the MCRPS/D/K system is bounded by:
\begin{equation}
N_{\text{samples}} = \mathcal{O}\left(\frac{|\mathcal{S}| \cdot |\mathcal{A}|}{(1-\gamma)^3 \epsilon^2}\right)
\label{eq:sample-complexity}
\end{equation}
where $\gamma$ is the discount factor and $\epsilon$ is the optimality gap.
\end{theorem}

\begin{proof}[Proof Sketch]
This follows from standard PAC-MDP bounds. The $(1-\gamma)^{-3}$ factor arises from the effective horizon $H = 1/(1-\gamma)$ and the variance of value estimates. The $|\mathcal{S}| \cdot |\mathcal{A}|$ factor reflects the need to visit each state-action pair sufficiently often for accurate Q-value estimation.
\end{proof}

With $\gamma = 0.99$ and $|\mathcal{A}| = 11$ continuous dimensions (discretized), the sample complexity ratio between K=30 and K=10 systems is approximately $|\mathcal{S}|_{K=30} / |\mathcal{S}|_{K=10} \approx 69\times$, explaining why high-capacity systems require substantially more training to achieve comparable performance.

\subsubsection{Capacity Paradox: Theoretical Explanation}

\begin{proposition}[Capacity Paradox Mechanism]
\label{prop:capacity-paradox}
Under extreme load ($\rho \to 1$), low-capacity systems (K=10) outperform high-capacity systems (K=30+) due to three compounding factors:
\begin{enumerate}
\item \textbf{State space explosion}: $|\mathcal{S}|_{K=30}/|\mathcal{S}|_{K=10} \approx 69$, requiring proportionally more samples for policy convergence.
\item \textbf{Sparse reward signals}: In larger state spaces, the probability of encountering informative reward signals during random exploration decreases as $\mathcal{O}(1/|\mathcal{S}|)$.
\item \textbf{Delayed feedback}: Larger capacity buffers mask developing instabilities, delaying corrective learning signals until catastrophic failure occurs.
\end{enumerate}
\end{proposition}

\begin{theorem}[Critical Load Threshold]
\label{thm:critical-load}
For a given capacity $K$, there exists a critical load factor $\rho_c(K)$ beyond which system stability degrades:
\begin{equation}
\rho_c(K) = 1 - \frac{c}{\sqrt{K}}
\label{eq:critical-load}
\end{equation}
where $c > 0$ is a system-dependent constant.
\end{theorem}

\begin{proof}[Proof Sketch]
From heavy-traffic queueing theory, the diffusion approximation shows that queue length variance scales as $\mathcal{O}(K/(1-\rho)^2)$ near capacity. The critical threshold where variance exceeds manageable bounds occurs when $(1-\rho)^2 \propto 1/K$, yielding $\rho_c(K) = 1 - c/\sqrt{K}$.
\end{proof}

This theorem explains why K=10 systems maintain stability at 10$\times$ load while K=30 systems experience catastrophic failure: $\rho_c(10) > \rho_c(30)$ due to the inverse square-root relationship. The smaller system reaches its critical threshold at higher relative load, paradoxically making it more robust under extreme conditions.

\subsubsection{Structural Advantage: Theoretical Foundation}

\begin{theorem}[Optimal Capacity Allocation]
\label{thm:optimal-allocation}
For a vertical queueing system with arrival weights $\mathbf{w} = [w_0, \ldots, w_{L-1}]$ and total capacity $K$, the optimal capacity allocation minimizes maximum utilization:
\begin{equation}
\mathbf{k}^* = \arg\min_{\mathbf{k}} \max_{i} \rho_i \quad \text{subject to} \quad \sum_{i=0}^{L-1} k_i = K
\label{eq:optimal-allocation}
\end{equation}
where $\rho_i = \lambda_i / (\mu_i \cdot k_i)$ is the utilization at layer $i$.
\end{theorem}

\begin{proof}
By Lagrangian optimization with constraint $\sum k_i = K$:
\begin{equation}
\mathcal{L}(\mathbf{k}, \nu) = \max_i \frac{\lambda_i}{\mu_i k_i} + \nu\left(\sum_{i=0}^{L-1} k_i - K\right)
\end{equation}
At optimum, all utilizations are equal: $\rho_0 = \rho_1 = \cdots = \rho_{L-1}$. This yields $k_i^* \propto \lambda_i / \mu_i$. For equal service rates ($\mu_i = \mu$), we have $k_i^* \propto \lambda_i \propto w_i$.
\end{proof}

\begin{corollary}[Inverted Pyramid Optimality]
Given arrival weights $\mathbf{w} = [0.30, 0.25, 0.20, 0.15, 0.10]$, the inverted pyramid configuration $[8,6,4,3,2]$ achieves near-optimal capacity-flow matching with coefficient of variation $\text{CV} = 0.11$, compared to normal pyramid $[2,3,4,6,8]$ with $\text{CV} = 0.89$.
\end{corollary}

\begin{definition}[System Bottleneck]
Layer $i$ is a \textit{bottleneck} if $\rho_i = \max_j \rho_j$.
\end{definition}

\begin{proposition}[Bottleneck Probability]
The probability of layer $i$ being a bottleneck under random load fluctuations is:
\begin{equation}
P(\text{bottleneck}_i) \propto \frac{w_i}{k_i}
\label{eq:bottleneck-prob}
\end{equation}
\end{proposition}

For inverted pyramid: $P(\text{bottleneck}_0) = 0.30/8 = 0.0375$. For normal pyramid: $P(\text{bottleneck}_0) = 0.30/2 = 0.15$ (4$\times$ higher). This explains the structural advantage: inverted pyramids distribute bottleneck risk more evenly, while normal pyramids concentrate risk at high-traffic layers.

\subsubsection{System Stability Theory}

We now establish formal stability conditions for the MCRPS/D/K system, connecting our framework to classical queueing theory.

\begin{theorem}[MCRPS/D/K Stability Condition]
\label{thm:stability}
The MCRPS/D/K system reaches steady state if and only if:
\begin{equation}
\max_{i \in \{0,\ldots,L-1\}} \rho_i < 1
\label{eq:stability-condition}
\end{equation}
where $\rho_i = \lambda_i^{\text{eff}}/(\mu_i \cdot k_i)$ is the effective utilization at layer $i$, and the effective arrival rate accounts for inter-layer transfers:
\begin{equation}
\lambda_i^{\text{eff}} = w_i \cdot \lambda_{\text{total}} + \sum_{j \neq i} T_{ji} - \sum_{j \neq i} T_{ij}
\label{eq:effective-arrival}
\end{equation}
\end{theorem}

\begin{proof}
We apply the Foster-Lyapunov criterion for positive recurrence. Define the Lyapunov function $V(\mathbf{q}) = \sum_{i=0}^{L-1} q_i^2$ where $\mathbf{q} = [q_0, \ldots, q_{L-1}]$ is the queue length vector. The drift at state $\mathbf{q}$ is:
\begin{equation}
\Delta V(\mathbf{q}) = \mathbb{E}[V(\mathbf{q}_{t+1}) - V(\mathbf{q}_t) | \mathbf{q}_t = \mathbf{q}]
\end{equation}
For each layer $i$, the expected queue length change is $\mathbb{E}[\Delta q_i] = \lambda_i^{\text{eff}} - \mu_i \cdot \min(q_i, k_i)$. When $q_i > 0$ and $\rho_i < 1$, we have $\mathbb{E}[\Delta q_i] < 0$. Summing over all layers, when $\max_i \rho_i < 1$, there exists $\epsilon > 0$ and $M > 0$ such that $\Delta V(\mathbf{q}) < -\epsilon$ for all $\|\mathbf{q}\| > M$. By the Foster-Lyapunov theorem \cite{meyn2012markov}, this ensures positive recurrence and steady-state existence.
\end{proof}

\begin{theorem}[Waiting Time Upper Bound]
\label{thm:waiting-bound}
Under the stability condition (Theorem~\ref{thm:stability}), the expected total waiting time in the MCRPS/D/K system is bounded by:
\begin{equation}
\mathbb{E}[W] \leq \sum_{i=0}^{L-1} \frac{\rho_i}{(1-\rho_i) \cdot \mu_i \cdot k_i}
\label{eq:waiting-bound}
\end{equation}
\end{theorem}

\begin{proof}[Proof Sketch]
Applying Kingman's bound \cite{kingman1961single} to each layer independently and summing yields the upper bound. The bound tightens as $\rho_i \to 0$ (light traffic) and diverges as $\rho_i \to 1$ (heavy traffic), consistent with the capacity paradox where systems operating near $\rho = 1$ exhibit unstable behavior.
\end{proof}

\begin{theorem}[Reduction to Classical M/M/c]
\label{thm:mmc-reduction}
When inter-layer transfers are disabled ($T_{ij} = 0$ for all $i,j$) and arrivals are independent across layers, the MCRPS/D/K system reduces to $L$ independent M/M/$k_i$ queues. In this case, the expected waiting time at layer $i$ follows the Erlang-C formula:
\begin{equation}
\mathbb{E}[W_i] = \frac{C(k_i, \rho_i)}{\mu_i \cdot k_i \cdot (1-\rho_i)}
\label{eq:erlang-c}
\end{equation}
where $C(k,\rho) = \frac{(k\rho)^k}{k!} \cdot \frac{1}{1-\rho} \cdot \left[\sum_{n=0}^{k-1} \frac{(k\rho)^n}{n!} + \frac{(k\rho)^k}{k!} \cdot \frac{1}{1-\rho}\right]^{-1}$ is the Erlang-C probability.
\end{theorem}

This reduction theorem establishes that MCRPS/D/K generalizes classical queueing models: the dynamic transfers and correlated arrivals extend M/M/c while preserving its fundamental stability properties. The DRL agent learns to exploit these extensions, achieving performance beyond what static M/M/c analysis would predict.

\subsubsection{Exploration Difficulty}

The exploration challenge scales with the product of state and action space sizes. High-capacity systems face sparse reward signals during early training, as the probability of discovering effective policies through random exploration decreases with state space size. Combined with the theoretical bounds established above, this analysis provides a principled explanation for the capacity paradox observed empirically: larger state spaces require exponentially more exploration to discover optimal policies, making low-capacity systems paradoxically easier to optimize under extreme load conditions.

%% Section 3: Results
\section{Results}
\label{sec:results}

\subsection{Algorithm Performance Comparison}
\label{subsec:algorithm-performance}

\subsubsection{Overall Performance Ranking}

As shown in Figure~\ref{fig:preliminary-ranking} and Table~\ref{tab:algorithm-summary}, our preliminary experiments evaluated all 15 algorithms under identical conditions, revealing a clear three-tier performance hierarchy. The Top-Tier algorithms (A2C, PPO, TD7, R2D2, SAC) achieved mean rewards exceeding 4,200, significantly outperforming Mid-Tier algorithms (TD3, Heuristic, Rainbow, Priority, FCFS, SJF) with rewards between 2,000--4,000, and Low-Tier algorithms (IMPALA, DDPG, Random) with rewards below 2,000.

Based on these preliminary results, we selected the top 3 performing algorithms (A2C, PPO, TD7) for comprehensive validation testing. A2C achieves the highest performance (4,438 $\pm$ 128) among all evaluated algorithms, followed by PPO (4,420 $\pm$ 136) and TD7 (4,352 $\pm$ 51). Notably, TD7 exhibits the lowest variance among top performers, indicating exceptional training stability. All three selected DRL algorithms demonstrate robust performance across different experimental conditions, establishing the effectiveness of learning-based approaches for vertical queueing optimization.

\subsubsection{Statistical Validation}

Independent samples t-tests confirm DRL superiority over heuristic baselines. This advantage stems from three key factors: (1) \textit{adaptive policy learning} that discovers non-obvious strategies through trial-and-error exploration, (2) \textit{multi-objective optimization} that simultaneously balances throughput, waiting time, queue length, crash avoidance, load balance, and inter-layer transfers, and (3) \textit{state-dependent decision-making} that leverages the full 29-dimensional state space for context-aware control decisions.

\subsubsection{Convergence Behavior Analysis}

To understand learning dynamics, we analyzed convergence curves across algorithm categories. Table~\ref{tab:convergence-phases} characterizes the distinct learning phases observed.

\begin{table}[htbp]
\centering
\caption{Learning Phase Characteristics by Algorithm Category}
\label{tab:convergence-phases}
\small
\begin{tabular}{lccc}
\toprule
\textbf{Category} & \textbf{Phase 1 (0-50K)} & \textbf{Phase 2 (50-200K)} & \textbf{Phase 3 (200K+)} \\
\midrule
Policy Gradient & Rapid rise (+65\%) & Refinement (+12\%) & Plateau \\
Actor-Critic & Slow start (+25\%) & Acceleration (+45\%) & Gradual rise \\
Value-Based & Unstable ($\pm$30\%) & Stabilization (+35\%) & Slow convergence \\
Heuristic & Flat (baseline) & Flat & Flat \\
\bottomrule
\end{tabular}
\end{table}

Policy gradient methods (A2C, PPO) exhibit a characteristic ``fast-start'' pattern: they achieve 65\% of final performance within the first 50K timesteps, then refine policies incrementally. This behavior aligns with their on-policy nature---each update directly improves the current policy without the lag introduced by replay buffers. In contrast, actor-critic methods (TD3, SAC, TD7) show a ``slow-start, fast-finish'' pattern, requiring 50-100K timesteps to populate replay buffers before acceleration. Value-based methods (DQN, Rainbow) exhibit the most unstable early learning due to the challenge of discretizing the 11-dimensional continuous action space.

\begin{figure}[!htb]
\centering
\includegraphics[width=0.75\textwidth]{figures/fig2_algorithm_robustness.pdf}
\caption{Algorithm Robustness Analysis: Performance consistency across multiple random seeds and evaluation episodes.}
\label{fig:algorithm-robustness}
\end{figure}

\subsection{Structural Analysis: Inverted vs Normal Pyramid}
\label{subsec:structural-analysis}

\subsubsection{Structural Comparison Results}

We conducted a systematic comparison of inverted pyramid [8,6,4,3,2] versus normal pyramid [2,3,4,6,8] capacity configurations at 5$\times$ baseline load using A2C and PPO algorithms (n=30 per algorithm per structure, total n=60 per structure). Table~\ref{tab:structural-comparison} presents the detailed results.

The inverted pyramid configuration achieved a combined mean reward of 722,952.90 (95\% CI: [721,194.42, 724,711.38]), while the normal pyramid configuration achieved 660,181.65 (95\% CI: [656,001.81, 664,361.49]). This represents a difference of 62,771.25 reward points, corresponding to a 9.5\% performance improvement at 5$\times$ load. The difference is highly statistically significant (p < 0.001) with a Cohen's d effect size of 6.31, indicating a very large effect with coefficient of variation below 0.2\%.

Importantly, this structural advantage exhibits load-dependent scaling. At 3$\times$ load, the effect size is d=0.28 (small effect, CV=2.1\%), increasing to d=6.31 at 5$\times$ load (very large effect, CV=0.12\%), d=302.55 at 7$\times$ load (extremely large effect, CV=0.05\%), and d=412.62 at 10$\times$ load (extremely large effect, CV=0.02\%). As explained in Section~\ref{subsec:statistical-analysis}, these increasing effect sizes reflect decreasing variance as system behavior becomes more deterministic under stress, rather than growing performance differences.

\begin{figure}[!htb]
\centering
\includegraphics[width=0.75\textwidth]{figures/fig2_structure_comparison_en.pdf}
\caption{Structural Comparison: Performance comparison between inverted pyramid [8,6,4,3,2] and normal pyramid [2,3,4,6,8] configurations across different load levels.}
\label{fig:structural-comparison}
\end{figure}

\input{tables/tab_structural_comparison.tex}

\subsubsection{Capacity-Flow Matching Principle}

The inverted pyramid's superior performance stems from capacity-flow matching: Layer 0 (capacity=8, traffic weight=0.30, ratio=26.67) through Layer 4 (capacity=2, weight=0.10, ratio=20.00) align capacity with traffic demand. The normal pyramid creates bottlenecks at Layer 0 (capacity=2, weight=0.30, ratio=6.67) while over-provisioning Layer 4 (capacity=8, weight=0.10, ratio=80.00). From queueing theory, optimal capacity allocation minimizes maximum utilization: $\min_{\mathbf{k}} \max_i \rho_i$ subject to $\sum k_i = K$, yielding $k_i^* \propto w_i$. The inverted pyramid approximates this optimum (CV: 0.11), while the normal pyramid exhibits extreme variation (CV: 0.89), explaining the 9.5\% performance gap.

\subsection{Capacity Paradox: Less is More Under Extreme Load}
\label{subsec:capacity-paradox}

\subsubsection{Capacity Scan Results}

Under 10$\times$ extreme load conditions, we tested total capacities $K \in \{10, 15, 20, 25, 30, 40\}$ with uniform distribution across layers. Table~\ref{tab:capacity-scan} reveals a counter-intuitive ``capacity paradox'': K=10 achieves the highest performance (reward: 11,146, 0\% crash rate), while K=30 (reward: 13, 100\% crash rate) and K=40 (reward: -30, 100\% crash rate) experience catastrophic failure. This finding---where lower capacity outperforms higher capacity by orders of magnitude---challenges conventional capacity planning assumptions.

\begin{figure}[!htb]
\centering
\includegraphics[width=0.85\textwidth]{figures/fig4_capacity_paradox.pdf}
\caption{Capacity Paradox: Performance degradation as total capacity increases under 10$\times$ extreme load, showing counter-intuitive "less is more" phenomenon.}
\label{fig:capacity-paradox}
\end{figure}

\input{tables/tab_capacity_scan.tex}

\subsubsection{Theoretical Explanation}

The capacity paradox arises from three compounding factors: (1) \textit{State space complexity}: $|\mathcal{S}|_{K=30} = 16,807$ versus $|\mathcal{S}|_{K=10} = 243$, representing a 69$\times$ increase in the number of states the agent must explore; (2) \textit{Exploration challenge}: larger state spaces with sparse reward signals make it exponentially harder to discover effective policies; and (3) \textit{System dynamics}: low capacity forces the DRL agent to learn aggressive preemptive strategies (proactive transfers and admission control), while high capacity permits passive strategies that accumulate hidden instabilities until catastrophic failure.

\textbf{Load-Dependent Nature of the Paradox.} Critically, the capacity paradox is \textit{load-dependent}: at moderate loads (3-5$\times$ baseline), K=30 outperforms K=10 as conventionally expected, because the larger state space can be adequately explored within the training budget and the additional capacity provides genuine buffering benefits. The paradox emerges only at extreme loads ($\geq$8$\times$ baseline) where: (a) the system operates near saturation, amplifying the consequences of suboptimal policies; (b) the exploration-exploitation trade-off becomes critical, as random exploration in large state spaces rarely encounters the narrow region of stable policies; and (c) the ``buffer masking'' effect dominates---larger buffers hide developing instabilities until catastrophic cascade failure. This load-dependence is consistent with the critical load threshold theorem (Theorem~\ref{thm:critical-load}): $\rho_c(K) = 1 - c/\sqrt{K}$, which predicts that smaller systems can operate closer to theoretical capacity limits.

Extended training experiments (500K timesteps, 5$\times$ the standard duration) confirm that K=30 and K=40 configurations maintain 100\% crash rates at 10$\times$ load, rejecting the hypothesis that the paradox is merely a training artifact. However, we acknowledge that with sufficient training (potentially millions of timesteps), larger capacity systems might eventually learn stable policies---the paradox reflects practical sample complexity constraints rather than fundamental impossibility.

\input{tables/tab_extended_training.tex}

\subsubsection{Critical Load Transition Analysis}

The capacity paradox exhibits a sharp phase transition behavior. Based on our experimental observations at 5$\times$ and 10$\times$ load levels, combined with the theoretical framework established in Theorem~\ref{thm:critical-load}, we identify a critical load threshold between 5$\times$ and 6$\times$ baseline load.

At 5$\times$ load, our structural comparison experiments show that both K=10 and K=30 systems maintain stable operation with 0\% crash rates, with larger capacity providing expected benefits. However, at 10$\times$ load (Table~\ref{tab:capacity-scan}), the relationship inverts dramatically: K=10 achieves 11,146 reward with 0\% crash rate, while K=30 collapses to 13 reward with 100\% crash rate---a performance difference exceeding 85,000\%.

This transition aligns with the theoretical prediction from Theorem~\ref{thm:critical-load}: $\rho_c(K) = 1 - c/\sqrt{K}$, which predicts $\rho_c(10) > \rho_c(30)$ due to the inverse square-root relationship. The smaller system can operate closer to theoretical capacity limits, paradoxically making it more robust under extreme conditions. The critical threshold occurs when the effective utilization crosses $\rho_c(30)$ while remaining below $\rho_c(10)$, triggering the paradox.

\subsection{Generalization Testing: Robustness Validation}
\label{subsec:generalization}

\subsubsection{Performance Across Heterogeneous Traffic Patterns}

We evaluated the top three algorithms (A2C, PPO, TD7) across five heterogeneous traffic patterns with varying arrival weight distributions. Table~\ref{tab:generalization} demonstrates consistent ranking (A2C $>$ PPO $>$ TD7) across all patterns with low variance, indicating that algorithm selection has greater impact than traffic pattern variations on system performance.

\input{tables/tab_generalization.tex}

\begin{figure}[!htb]
\centering
\includegraphics[width=0.85\textwidth]{figures/fig5_heatmap.pdf}
\caption{Performance Heatmap Across Algorithms and Conditions: Comprehensive visualization of algorithm performance across multiple experimental conditions, demonstrating consistent performance patterns and robustness of top-performing algorithms.}
\label{fig:performance-heatmap}
\end{figure}

\subsubsection{Reward Function Sensitivity Analysis}

To validate that our findings are not artifacts of specific reward function tuning, we tested four diverse weight configurations: baseline, throughput-focused, balance-focused, and efficiency-focused. Table~\ref{tab:reward-sensitivity} presents the results at 6$\times$ load with K=10.

All four weight configurations produce consistent structural rankings: inverted pyramid outperforms normal pyramid regardless of reward weight configuration. The cross-configuration variance is 0.0, representing strong evidence of robustness. This finding demonstrates that structural advantages are insensitive to reward function weights, with the system converging to consistent rankings regardless of reward configuration. This eliminates concerns that results depend on specific hyperparameter choices and confirms that the structural advantage is a robust, fundamental property that holds across diverse reward formulations.

\input{tables/tab_reward_sensitivity.tex}

\subsubsection{State Space Ablation Study}

To validate the design of our 29-dimensional state representation, we conducted an ablation study comparing four state space configurations with progressively increasing feature sets. Table~\ref{tab:state-ablation} presents the results using A2C with 100,000 training timesteps across three random seeds.

\begin{table}[htbp]
\centering
\caption{State Space Ablation Study Results}
\label{tab:state-ablation}
\small
\begin{tabular}{llccc}
\toprule
\textbf{Config} & \textbf{Features} & \textbf{Dim} & \textbf{Reward} & \textbf{Throughput} \\
\midrule
Minimal & $\mathbf{q}, \mathbf{k}, \boldsymbol{\rho}$ & 15 & 9,760 $\pm$ 2,720 & 1,068 \\
Core & Minimal + $\boldsymbol{\mu}$ & 20 & 11,327 $\pm$ 1,850 & 1,250 \\
Extended & Core + $\boldsymbol{\lambda}$ & 25 & 9,858 $\pm$ 1,225 & 1,040 \\
Full & Extended + $t, L, \bar{w}, c$ & 29 & \textbf{11,826 $\pm$ 1,816} & \textbf{1,241} \\
\bottomrule
\end{tabular}
\end{table}

\noindent Here $\mathbf{q}$ denotes queue lengths, $\mathbf{k}$ capacities, $\boldsymbol{\rho}$ utilization ratios, $\boldsymbol{\mu}$ service rates, $\boldsymbol{\lambda}$ arrival rates, $t$ timestep, $L$ total load, $\bar{w}$ average waiting time, and $c$ crash indicator.

The results demonstrate that the full 29-dimensional state space achieves the highest performance (11,826 reward), validating our state representation design. Key observations include: (1) adding service rate information (Core) provides a 16\% improvement over Minimal, indicating that service dynamics are critical for policy learning; (2) the Extended configuration shows decreased performance despite additional features, suggesting that arrival rate information alone may introduce noise without the contextual features; (3) the Full configuration recovers and exceeds Core performance by incorporating system-level metrics (timestep, total load, average wait, crash indicator) that provide essential context for decision-making. These findings support the theoretical motivation for our state space design: the agent requires both local layer information and global system context to learn effective policies.

\subsubsection{Algorithm Convergence Analysis}

To provide practical guidance for deployment, we analyzed the convergence behavior of the top-performing algorithms. Table~\ref{tab:convergence} presents the timesteps required to reach 90\% of final performance and the sample efficiency metrics.

\begin{table}[htbp]
\centering
\caption{Algorithm Convergence and Computational Cost Analysis}
\label{tab:convergence}
\small
\begin{tabular}{lrrrrr}
\toprule
\textbf{Algorithm} & \textbf{Steps to 90\%} & \textbf{Final Reward} & \textbf{Train Time} & \textbf{Inference} & \textbf{Memory} \\
\midrule
A2C & 85,000 & 4,438 & 6.9 min & 0.12 ms & 245 MB \\
PPO & 120,000 & 4,420 & 30.8 min & 0.15 ms & 312 MB \\
TD7 & 280,000 & 4,324 & 382.0 min & 0.31 ms & 1.2 GB \\
SAC & 195,000 & 4,298 & 156.3 min & 0.24 ms & 856 MB \\
TD3 & 210,000 & 4,276 & 145.7 min & 0.22 ms & 734 MB \\
\bottomrule
\end{tabular}
\vspace{0.1cm}

\small\textit{Note: Train Time for 500K steps on RTX 3090. Inference time per decision. A2C achieves best performance with lowest computational cost, enabling real-time deployment ($<$1ms latency requirement for UAM).}
\end{table}

A2C demonstrates the fastest convergence, reaching 90\% of its final performance in only 85,000 timesteps---approximately 1.4$\times$ faster than PPO and 3.3$\times$ faster than TD7. Critically for real-world deployment, A2C's inference time (0.12 ms) is well below the typical 10-100 ms decision latency requirement for UAM systems, and its memory footprint (245 MB) enables deployment on edge devices. TD7, despite strong final performance, requires 55$\times$ longer training time and 5$\times$ more memory, making it impractical for resource-constrained scenarios. This computational analysis reinforces A2C as the recommended algorithm for practical UAM deployments.

\subsection{Multi-Objective Pareto Analysis: Empirical Validation}
\label{subsec:pareto-validation}

To empirically validate the multi-objective optimization framework presented in Section~\ref{subsec:theoretical-analysis}, we conducted a comprehensive Pareto analysis by evaluating 10,000 randomly generated policy configurations across the six-dimensional objective space.

\subsubsection{Pareto Front Identification}

Using non-dominated sorting (Theorem~\ref{thm:nds-complexity}), we identified 91 Pareto optimal solutions from 10,000 candidates, representing 0.91\% of the solution space. This relatively small Pareto ratio indicates strong objective conflicts, where improving one objective typically requires sacrificing others. Table~\ref{tab:pareto-summary} summarizes the Pareto front characteristics.

\begin{table}[htbp]
\centering
\caption{Pareto Analysis Summary Statistics (n=10,000 solutions)}
\label{tab:pareto-summary}
\small
\begin{tabular}{lcccc}
\toprule
\textbf{Objective} & \textbf{Mean} & \textbf{Std} & \textbf{Min} & \textbf{Max} \\
\midrule
Throughput & 8.581 & 1.110 & 4.733 & 10.267 \\
Balance & 4.928 & 0.071 & 4.653 & 5.000 \\
Efficiency & 0.362 & 0.094 & 0.215 & 0.557 \\
Transfer & 0.000 & 0.000 & 0.000 & 0.000 \\
Stability & 1.890 & 0.030 & 1.785 & 1.920 \\
Anti-Penalty & 0.000 & 0.000 & 0.000 & 0.000 \\
\midrule
\multicolumn{5}{l}{\textit{Summary Metrics}} \\
Total Solutions & \multicolumn{4}{c}{10,000} \\
Pareto Optimal & \multicolumn{4}{c}{91 (0.91\%)} \\
Knee Points & \multicolumn{4}{c}{5} \\
Hypervolume & \multicolumn{4}{c}{0.2282} \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Objective Conflict Analysis}

The correlation analysis reveals significant conflicts between objectives, validating the multi-objective nature of the problem. Figure~\ref{fig:pareto-front} visualizes the Pareto front across key objective pairs, while Figure~\ref{fig:objective-conflicts} presents the complete correlation matrix.

\begin{figure}[!htb]
\centering
\includegraphics[width=0.95\textwidth]{figures/pareto_front_analysis.pdf}
\caption{Pareto Front Analysis: Visualization of 10,000 solutions across six objective pairs. Red points indicate Pareto optimal solutions (91 solutions, 0.91\%), gold stars mark knee points (5 solutions), and light blue points represent dominated solutions. The sparse Pareto front demonstrates strong objective conflicts.}
\label{fig:pareto-front}
\end{figure}

\begin{figure}[!htb]
\centering
\includegraphics[width=0.65\textwidth]{figures/objective_conflicts_matrix.pdf}
\caption{Objective Correlation Matrix: Pairwise correlations between six objectives. Strong negative correlations (red) indicate conflicting objectives, while positive correlations (blue) indicate synergistic objectives. Key conflicts include Balance-Efficiency ($r=-0.82$) and Throughput-Stability ($r=-0.70$).}
\label{fig:objective-conflicts}
\end{figure}

The key findings from the correlation analysis are:

\begin{itemize}
\item \textbf{Balance vs Efficiency} ($r = -0.818$): Strong negative correlation indicates that achieving uniform load distribution across layers reduces overall system efficiency, as resources are diverted from high-throughput layers.

\item \textbf{Throughput vs Efficiency} ($r = 0.775$): Positive correlation suggests that high-throughput policies tend to be more efficient, as they maximize output per unit resource.

\item \textbf{Throughput vs Stability} ($r = -0.703$): Negative correlation confirms the fundamental trade-off between aggressive throughput maximization and system stability---pushing for higher throughput increases crash risk.
\end{itemize}

These empirically observed conflicts align with the theoretical framework: the Throughput-Stability trade-off ($r = -0.703$) explains why the crash penalty weight $w_4 = 10000$ is necessary to prevent policies from sacrificing stability for throughput gains.

\subsubsection{Knee Point Characterization}

Using the multi-criteria knee point detection method (Equation~\ref{eq:knee-score}), we identified 5 knee points representing the most balanced trade-off solutions. These knee points achieve:

\begin{itemize}
\item High quality scores ($Q > 0.85$): Close to the ideal point in normalized objective space
\item High diversity scores ($D > 0.70$): Well-distributed across the Pareto front
\item High balance scores ($B > 0.80$): Low coefficient of variation across objectives
\end{itemize}

The knee points provide actionable policy recommendations: they represent configurations that achieve near-optimal performance across all objectives without extreme trade-offs, making them suitable starting points for practical UAM deployment.

\subsubsection{Hypervolume Validation}

The computed hypervolume indicator $\text{HV} = 0.2282$ provides a scalar measure of Pareto front quality. This value serves as a baseline for comparing different algorithmic approaches: higher hypervolume indicates better coverage of the objective space. The relatively modest hypervolume reflects the challenging nature of the six-dimensional optimization problem, where achieving high values across all objectives simultaneously is inherently difficult due to the strong conflicts identified above.

\input{sections/ablation_study_simple}

\input{sections/hca2c_ablation}

%% Section 4: Discussion
\section{Discussion}
\label{sec:discussion}

\subsection{What Does DRL Learn? Policy Analysis}

To understand \textit{why} DRL outperforms heuristics, we analyze the learned policies by examining action distributions across different system states. This analysis reveals three key behavioral patterns that distinguish DRL from rule-based approaches.

\textbf{Adaptive Service Prioritization.} Unlike fixed-priority heuristics, the learned A2C policy dynamically adjusts service priorities based on utilization ratios. When layer $i$ approaches saturation ($\rho_i > 0.8$), the policy increases $p_i$ to accelerate service at that layer. Quantitatively, the correlation between $\rho_i$ and $p_i$ is $r = 0.73$ ($p < 0.001$), indicating strong state-dependent adaptation. In contrast, heuristic baselines maintain fixed priorities regardless of system state.

\textbf{Preemptive Transfer Activation.} The DRL policy learns to initiate inter-layer transfers \textit{before} queues reach capacity, rather than reactively after overflow. Analysis of transfer decisions shows that A2C activates transfers when $\rho_i > 0.6$ (preemptive threshold), while the adaptive heuristic only transfers when $\rho_i > 0.9$ (reactive threshold). This 0.3 difference in activation threshold explains much of the performance gap: preemptive transfers prevent cascade failures that occur when multiple layers simultaneously approach capacity.

\textbf{Coordinated Multi-Layer Control.} Perhaps most importantly, DRL learns to coordinate actions across layers. The mutual information between adjacent layer actions is $I(a_i; a_{i+1}) = 0.42$ bits, indicating significant coordination. When layer 0 increases service priority, layer 1 simultaneously prepares for increased downstream arrivals by reducing its own admission rate. This coordinated behavior emerges naturally from end-to-end policy optimization and cannot be achieved by independent per-layer heuristics.

These behavioral differences explain the significant performance improvement: DRL discovers non-obvious strategies (preemptive transfers, coordinated control) that exploit the system's multi-layer structure in ways that simple heuristics cannot.

\subsection{Interpretation of Key Findings}

Our results establish three principal findings. First, DRL algorithms significantly outperform heuristics, demonstrating effective handling of multi-layer correlated arrivals, dynamic transfers, and finite capacity constraints. A2C's rapid convergence and TD7's double-jump learning pattern suggest policy gradient methods achieve faster, more stable convergence than actor-critic methods.

Second, inverted pyramid configurations validate the capacity-flow matching principle: allocating capacity proportional to traffic demand minimizes bottlenecks. Load-dependent effect size scaling (d=0.28 at 3$\times$ to d=412.62 at 10$\times$) reflects decreasing variance as system behavior becomes deterministic under stress, characteristic of computational experiments with converged algorithms.

Third, the capacity paradox reveals fundamental DRL limitations under extreme conditions. K=10 outperforming K=30+ by orders of magnitude, validated through extended training, demonstrates state space complexity can overwhelm learning capacity, challenging the assumption that more capacity always improves performance.

\subsection{Why A2C Outperforms Other Algorithms}

A2C's superior performance over other DRL algorithms can be attributed to three factors specific to the vertical queueing domain:

\textbf{On-Policy Learning Stability.} The MCRPS/D/K system exhibits non-stationary dynamics under varying load conditions, where queue states and optimal actions shift as the system approaches capacity limits. A2C's on-policy updates ensure that the policy is always evaluated on data generated by the current policy, avoiding the distribution shift problems that affect off-policy methods like TD3 and SAC. When system dynamics change rapidly near capacity thresholds, off-policy methods suffer from stale experience in their replay buffers, leading to suboptimal policy updates.

\textbf{Advantage Estimation for Multi-Objective Trade-offs.} The advantage function $A(s,a) = Q(s,a) - V(s)$ provides a natural mechanism for evaluating action quality relative to the current state value. In our multi-objective setting with six competing objectives (throughput, balance, efficiency, transfer, stability, anti-penalty), this relative evaluation helps the agent identify actions that improve multiple objectives simultaneously without being dominated by the large crash penalty ($w_4 = 10^4$). The baseline subtraction in advantage estimation reduces variance while preserving the signal for beneficial actions across all objectives.

\textbf{Synchronous Updates for High-Variance Rewards.} Unlike asynchronous methods (A3C, IMPALA), A2C's synchronous updates provide more stable gradient estimates. This stability is crucial for the high-variance reward signals in queueing systems where crash penalties can dominate the learning signal. The synchronous batch updates average over multiple trajectories, reducing the impact of outlier episodes that end in system crashes and providing more reliable policy improvement directions.

These characteristics explain why policy gradient methods (A2C, PPO) consistently outperform actor-critic methods (TD3, SAC, TD7) and value-based methods (DQN, Rainbow) in our experiments, despite the latter categories' theoretical advantages in sample efficiency for other domains.

\input{sections/hca2c_ablation_discussion}

\subsection{Practical Implications for UAM System Design}

Our findings translate into concrete design guidelines for UAM system operators and infrastructure planners:

\textbf{Capacity Configuration.} For normal to moderate loads (1-5$\times$ baseline), operators should implement inverted pyramid configurations that allocate higher capacity to high-traffic altitude zones, delivering approximately 9.5\% performance improvements. The capacity-flow matching principle (Theorem~\ref{thm:optimal-allocation}) provides a quantitative basis: $k_i^* \propto w_i$, where capacity at each layer should be proportional to its expected traffic weight.

\textbf{Algorithm Selection.} A2C offers optimal performance among evaluated algorithms with fastest convergence (85K steps to 90\% performance), making it ideal for resource-constrained deployments. PPO provides a robust alternative with slightly slower convergence but comparable final performance and excellent stability across random seeds. For systems requiring maximum sample efficiency, A2C's 52.2 efficiency score (reward per 1000 training steps) significantly exceeds TD7's 15.4.

\textbf{Extreme Load Management.} Under extreme loads (10$\times$ baseline), lower capacity systems (K=10-20) paradoxically outperform higher capacity systems by maintaining stability. This counter-intuitive finding suggests that operators facing capacity expansion decisions should first optimize their DRL policies for existing capacity before investing in infrastructure expansion. The critical load threshold (Theorem~\ref{thm:critical-load}) provides guidance: $\rho_c(K) = 1 - c/\sqrt{K}$ indicates that smaller systems can operate closer to theoretical capacity limits.

\textbf{State Space Design.} The ablation study validates that comprehensive state representations (29 dimensions) outperform minimal representations (15 dimensions) by 21\%. System designers should include not only local queue information but also global system metrics (total load, average wait time, crash indicators) to enable effective policy learning.

\subsection{Limitations and Future Research}

\textbf{Model Simplifications.} Our MCRPS/D/K framework makes several simplifying assumptions that may limit direct applicability to real-world UAM systems. We discuss each assumption and its potential impact on our findings:

\begin{itemize}
\item \textit{Fixed five-layer structure}: Real UAM airspace may require dynamic layer allocation based on traffic density and weather conditions. Our model assumes static layer boundaries. \textbf{Impact}: The structural findings (inverted pyramid optimality) should generalize to systems with more layers, as the capacity-flow matching principle (Theorem~\ref{thm:optimal-allocation}) is layer-count agnostic. However, the specific capacity ratios [8,6,4,3,2] are optimized for our five-layer configuration.

\item \textit{Homogeneous aircraft}: We assume identical service requirements across all aircraft, whereas real UAM involves heterogeneous vehicle types (delivery drones, air taxis, emergency vehicles) with different priorities and service times. \textbf{Impact}: Heterogeneous priorities would require extending the action space to include priority-based service decisions. The capacity paradox finding may be amplified in heterogeneous systems due to increased state space complexity.

\item \textit{Centralized control}: Our DRL agent has global state visibility and centralized decision-making authority. Real UAM systems may require distributed control with partial observability and communication constraints. \textbf{Impact}: Distributed implementations would likely require multi-agent DRL formulations, potentially reducing performance due to coordination overhead. The algorithm ranking (A2C > PPO > TD7) may change under partial observability.

\item \textit{Simplified dynamics}: We model arrivals as Poisson processes with fixed weights, omitting time-varying demand patterns, weather disruptions, and regulatory constraints (e.g., no-fly zones, altitude restrictions). \textbf{Impact}: Time-varying patterns would require recurrent architectures or meta-learning approaches. Our generalization tests across five traffic patterns provide partial validation, but fundamentally non-stationary environments remain untested.

\item \textit{Custom simulator}: All experiments use our MCRPS/D/K simulator rather than established UAM simulation platforms (e.g., BlueSky, SUMO), limiting validation of real-world applicability. \textbf{Impact}: While our simulator captures essential queueing dynamics, it omits physical constraints (aircraft dynamics, collision avoidance, weather effects). Integration with established platforms is a priority for future work.
\end{itemize}

\textbf{Experimental Limitations.} Several experimental choices may affect generalizability:
\begin{itemize}
\item \textit{Uniform hyperparameters}: All algorithms use identical hyperparameters (learning rate $3 \times 10^{-4}$, batch size 64), which may not reflect each algorithm's optimal configuration.
\item \textit{Fixed training budget}: The 500K timestep budget may disadvantage algorithms requiring longer convergence (e.g., TD7, SAC), though extended training experiments partially address this concern.
\item \textit{Limited traffic diversity}: While we test five traffic patterns, all are variations of the baseline distribution; fundamentally different patterns (e.g., time-varying, event-driven) remain untested.
\end{itemize}

\textbf{Future Research Directions.} Based on these limitations, we identify several promising directions:
\begin{enumerate}
\item \textit{Hierarchical DRL architectures} to mitigate the capacity paradox through multi-level policy decomposition, potentially enabling effective learning in larger state spaces.
\item \textit{Real-world validation} through integration with established UAM simulators (e.g., BlueSky, SUMO) or collaboration with UAM operators for field testing.
\item \textit{Distributed multi-agent formulations} where each layer operates semi-autonomously with local observations and inter-agent communication.
\item \textit{Meta-learning approaches} for rapid adaptation to novel traffic patterns without full retraining.
\item \textit{Domain generalization} to investigate whether the capacity paradox and structural optimality findings transfer to analogous systems (data center scheduling, network routing, hospital resource allocation).
\end{enumerate}

%% Section 5: Conclusion
\section{Conclusion}
\label{sec:conclusion}

This research addresses the question: \textit{which DRL algorithms are most effective for vertical layered queueing systems, and what structural configurations maximize performance?} Through systematic evaluation of top-performing algorithms across 500,000 training timesteps, we establish six principal findings that directly answer this question.

\textbf{Finding 1: DRL Algorithm Effectiveness.} A2C emerges as the optimal algorithm among those evaluated, demonstrating fast convergence and strong performance. DRL algorithms significantly outperform heuristic baselines, demonstrating that learning-based approaches substantially outperform rule-based methods for this domain. For practitioners, we recommend A2C for resource-constrained deployments and PPO as a robust alternative when training stability is prioritized.

\textbf{Finding 2: Structural Configuration.} Inverted pyramid configurations [8,6,4,3,2] consistently outperform normal pyramids by approximately 9.5\% at moderate loads (5$\times$ baseline). This finding is theoretically grounded in the optimal capacity allocation theorem: $k_i^* \propto w_i$, meaning capacity should be proportional to arrival weights. For UAM system designers, this translates to allocating more capacity at lower altitudes where traffic density is highest.

\textbf{Finding 3: Capacity Paradox.} Under extreme load ($\geq$8$\times$ baseline), low-capacity systems (K=10) paradoxically outperform high-capacity systems (K=30+) due to state space explosion and sample complexity constraints. However, this effect is load-dependent---at moderate loads, larger capacity provides expected benefits. Practitioners should recognize that capacity expansion alone does not guarantee performance improvement; policy optimization for existing capacity may be more cost-effective.

\textbf{Finding 4: State Space Design.} The 29-dimensional state representation outperforms minimal (15-dim) representations by 21\%, validating the importance of including both local queue information and global system metrics for effective policy learning.

\textbf{Finding 5: Architectural Design.} Through comprehensive ablation studies, we demonstrate that capacity-aware action clipping is essential for achieving optimal performance. Removing this constraint leads to 66\% performance degradation (78,973 vs 228,945) despite identical network capacity (821K parameters), validating that performance gains stem not only from increased network capacity but critically from architectural design that encodes domain knowledge about capacity constraints.

\textbf{Finding 6: Performance-Stability Trade-off.} The HCA2C algorithm comparison reveals a fundamental trade-off between performance and training stability. While A2C achieves peak performance (771,222 at load 5.0$\times$), it exhibits high training variance (average CV=31.55\%). HCA2C demonstrates exceptional stability (average CV=0.20\%) but suffers from limited performance and catastrophic failure under extreme load (7.0$\times$). PPO provides a balanced middle ground (CV=7.39\%) with consistent positive performance across all load levels. These findings indicate that algorithm selection should be guided by application requirements: HCA2C for safety-critical systems requiring predictable training outcomes, A2C for maximum performance when multiple training runs are feasible, and PPO for balanced performance and stability.

\textbf{Broader Impact.} While our findings are derived from a simplified MCRPS/D/K model, the underlying principles---capacity-flow matching, load-dependent complexity effects, architectural inductive biases, performance-stability trade-offs, and the value of comprehensive state representations---may generalize to analogous layered service systems in data centers, network routing, and healthcare resource allocation. As UAM systems transition from concept to reality, these evidence-based design principles offer a foundation for building safe, efficient vertical airspace management systems, though real-world validation remains essential before operational deployment.

%% ============================================================================
%% ACKNOWLEDGMENTS (Optional)
%% ============================================================================

% \section*{Acknowledgments}
% TODO: Add acknowledgments if applicable

%% ============================================================================
%% AUTHOR CONTRIBUTIONS (Required by Applied Soft Computing)
%% ============================================================================

\section*{Author Contributions}

\textbf{ZhiHan Wang}: Conceptualization, Methodology, Software, Validation, Formal analysis, Investigation, Data curation, Writing - Original Draft, Writing - Review \& Editing, Visualization, Project administration.

%% ============================================================================
%% AUTHOR BIOGRAPHIES
%% ============================================================================

\section*{Author Biographies}

\textbf{ZhiHan Wang} is a Master's student at SClab, China University of Petroleum (Beijing). His research interests include deep reinforcement learning, queueing theory, and optimization algorithms for urban air mobility systems. His current work focuses on applying advanced DRL algorithms to complex queueing systems and investigating structural design principles for layered service architectures.

%% ============================================================================
%% DATA AVAILABILITY STATEMENT
%% ============================================================================

\section*{Data Availability Statement}

The data that support the findings of this study are available from the corresponding author upon reasonable request. This includes:

\begin{itemize}
\item Training logs and evaluation results for DRL algorithms across 500,000 timesteps
\item Experimental data for structural comparison studies (inverted vs normal pyramid configurations)
\item Capacity scan results across K=10, 15, 20, 25, 30, 40 configurations
\item Extended training validation data (100K vs 500K timesteps)
\item Generalization testing results across 5 heterogeneous traffic patterns
\item Reward function sensitivity analysis data across 4 weight configurations
\end{itemize}

The custom MCRPS/D/K environment implementation and trained model checkpoints will be made publicly available in a GitHub repository upon publication. All experiments were conducted using publicly available software frameworks (Python 3.8, PyTorch 1.10, Stable-Baselines3 1.5.0, Gym 0.21) with fixed random seeds [42, 43, 44, 45, 46] to ensure reproducibility.

%% ============================================================================
%% FUNDING STATEMENT
%% ============================================================================

\section*{Funding}

This research received no specific grant from any funding agency in the public, commercial, or not-for-profit sectors.

%% ============================================================================
%% CONFLICT OF INTEREST STATEMENT
%% ============================================================================

\section*{Conflict of Interest Statement}

The author declares that he has no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.

%% ============================================================================
%% BIBLIOGRAPHY
%% ============================================================================

\bibliographystyle{elsarticle-num}
\bibliography{references}

%% ============================================================================
%% APPENDICES
%% ============================================================================

\appendix

%% Appendix A: Load Sensitivity Analysis
\section{Load Sensitivity Analysis}
\label{app:load-sensitivity}

We conducted comprehensive load sensitivity analysis across 7 load levels (3$\times$-10$\times$) with K=10 and K=30 configurations using A2C and PPO (140 total runs). Results reveal a three-phase pattern: (1) At low loads (3-4$\times$), K=30 outperforms K=10 by 112-141% as expected; (2) At moderate loads (6-7$\times$), transition occurs with K=10 beginning to outperform K=30; (3) At extreme loads (8-10$\times$), K=10 achieves stable performance while K=30 experiences complete collapse (100% crash rate). The critical transition point is between 4$\times$ and 6$\times$ load, where K=30's crash rate jumps from 0% to 84%. Detailed results and analysis are provided in supplementary materials.

%% Appendix B: Structural Comparison Generalization
\section{Structural Comparison Generalization}
\label{app:structural-generalization}

We validated structural findings across 5 heterogeneous traffic patterns with varying arrival weights and service rates. The inverted pyramid [8,6,4,3,2] consistently outperforms normal pyramid [2,3,4,6,8] across all patterns, with advantages ranging from 8.7% to 11.2% (mean: 9.8%, SD: 0.9%). ANOVA confirms structural effect is highly significant (F=234.56, p<0.001) and consistent across traffic patterns (interaction effect: F=1.23, p=0.31). Complete experimental design, results tables, and statistical analysis are provided in supplementary materials.

\end{document}
