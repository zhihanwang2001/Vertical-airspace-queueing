% ============================================================
% Batch 4: Framework and Methodology
% ~3,150 words total
% Created: 2026-01-05
% Based on: Documentation/guides/Final_Paper_Chinese_Version.md §4
% ============================================================

\section{Framework and Methodology}
\label{sec:methodology}

The MCRPS/SD/K framework integrates queueing theory, pressure-triggered control mechanisms, Pareto multi-objective optimization, and deep reinforcement learning to address the challenges of vertical airspace management under extreme load conditions. Figure~\ref{fig:framework_architecture} illustrates the complete system architecture with five integrated layers. This section presents the three core methodological components: pressure-triggered transfer mechanisms (§4.1), Pareto optimization methodology (§4.2), and deep RL integration with queueing-aware state-action design (§4.3).

\begin{figure}[t]
\centering
\includegraphics[width=\textwidth]{Figures/publication/figure2_architecture.png}
\caption{MCRPS/SD/K framework architecture. The system integrates five key layers: (1) Vertical layered queueing system (L1-L5) with inverted pyramid capacity [2,3,4,6,8], (2) Pressure measurement and fairness control using Gini coefficient, (3) Deep reinforcement learning layer with 29-dimensional state space and 11-dimensional hybrid action space, (4) Pareto multi-objective optimization identifying 262 optimal solutions and 13 knee points from 10,000 sampled configurations, (5) Real-time deployment layer with dynamic service rate adjustment and emergency transfer mechanisms. Data flows from queueing system through pressure measurement to DRL state input, with policy actions feeding into Pareto optimization and deployment execution, completing the closed-loop control.}
\label{fig:framework_architecture}
\end{figure}

\subsection{Pressure-Triggered Transfer Mechanisms}
\label{sec:pressure_transfer}

\textbf{Design Rationale for Downward Transfers.} Although higher layers (L5: $C=8$, $\mu=1.2$) offer faster service and larger capacity, downward transfers to lower layers serve as \textbf{temporary overflow buffers} to prevent global blockage during extreme congestion peaks. The key intuition operates at three levels:

\begin{itemize}
\item \textbf{Normal operation}: The system prioritizes high-layer efficiency via dynamic service rate modulation ($\mu$ adjustment), rarely triggering transfers (experimental frequency $<$0.2\%)

\item \textbf{Peak shaving}: During traffic bursts, L5$\rightarrow$L1 transfers reduce peak queue lengths by 37\%, providing emergency relief

\item \textbf{Physical plausibility}: Lower layers, though slower ($\mu_1=0.4$), serve as short-term spillover buffers superior to order rejection; backlog gradually clears via priority mechanisms after high-layer pressure subsides
\end{itemize}

\textbf{Stability Sufficient Conditions.} Under arrival control (via \texttt{arrival\_multiplier} action, §4.3) soft-constraining each layer to $\rho_\ell < 1$ (where $\rho_\ell = \lambda_\ell / (\mu_\ell C_\ell)$ is traffic intensity), downward transfers diluting high-layer congestion guarantee \textbf{global queue positive recurrence} (Harris recurrence) and \textbf{fluid limit convergence} (Kurtz theorem). Detailed drift analysis appears in Appendix C.

\textbf{Pressure Metric.} For layer $\ell$ at time $t$, we define a composite pressure metric:
\begin{equation}
P^t_\ell = \beta_1 \left(\frac{Q^t_\ell}{C_\ell}\right) + \beta_2 \left(1 - \frac{\mu^t_\ell}{\mu_{\max,\ell}}\right) + \beta_3 G^t_\ell
\label{eq:pressure}
\end{equation}
where $\beta_1$, $\beta_2$, $\beta_3$ are pressure weight coefficients, $\mu_{\max,\ell}$ denotes the maximum nominal service rate for layer $\ell$, and $G^t_\ell$ represents local fairness measured via Gini coefficient. The pressure differential $\Delta P^t_\ell = P^t_\ell - \bar{P}^t$ (where $\bar{P}^t$ is the mean pressure across all layers) drives transfer decisions.

\textbf{Implementation Note.} The complete pressure formula (Equation~\ref{eq:pressure}) represents the theoretical framework design for conceptual modeling. \textbf{Current experimental implementation uses a simplified version} $P_\ell = Q_\ell/C_\ell$ (i.e., $\beta_1=1, \beta_2=\beta_3=0$) for three reasons: (1) \textbf{Dominant factor}: Under the inverted pyramid capacity design, the congestion term $Q_\ell/C_\ell$ is the core driver for triggering transfers (capacity varies significantly from $C_1=2$ to $C_5=8$); (2) \textbf{Service rate encoded in DRL state}: The service redundancy term $(1-\mu^t_\ell/\mu_{\max,\ell})$ is already exposed to the RL agent through the \texttt{service\_rates} dimension in the state space, avoiding redundant encoding in the pressure metric; (3) \textbf{Fairness optimized via reward function}: Local fairness $G^t_\ell$ is optimized through the global Gini coefficient reward term (weight +5, see §4.3). The simplified version demonstrates stable performance in experiments; full formula implementation with $\beta$ weight tuning is reserved for future work exploring more complex pressure modeling strategies.

\textbf{Transfer Strategy (Current Implementation).}
\begin{itemize}
\item \textbf{Primary transfer direction}: When source layer pressure significantly exceeds target layer pressure ($\Delta P > \theta_{\text{transfer}}$, where $\theta_{\text{transfer}}=0.3$ calibrated via grid search over $\{0.1, 0.2, 0.3, 0.4\}$ achieving optimal fairness-throughput tradeoff) and target layer has spare capacity, execute \textbf{downward transfers} (L5$\rightarrow$L4$\rightarrow\cdots\rightarrow$L1)

\item \textbf{Transfer triggering conditions}: (1) Order waiting time exceeds threshold; (2) Source-target pressure differential exceeds threshold; (3) Target layer has not reached capacity limit; (4) Global fairness constraint satisfied

\item \textbf{Load balancing mechanism}: Transfers from high layers (large $C$, large $\mu$) to low layers (small $C$, small $\mu$) primarily serve emergency regulation rather than routine operation. System design prioritizes high-layer efficiency via \textbf{service rate modulation} ($\mu$ dynamic adjustment) over frequent transfers

\item \textbf{Stability guarantee}: Transfers activate only when pressure differentials are significant, avoiding unnecessary inter-layer perturbations
\end{itemize}

\textbf{Fairness Control via Gini Coefficient.} We monitor global fairness through the \textbf{Gini index} over effective layer workloads $x_i = U_i = Q_i/C_i$ (occupancy rates at each layer):
\begin{equation}
G_t = \frac{\sum_{i=1}^{n} \sum_{j=1}^{n} |x_i - x_j|}{2n^2 \bar{x}}
\label{eq:gini}
\end{equation}
where $n=5$ is the number of layers and $\bar{x} = \frac{1}{n}\sum_{i=1}^{n} x_i$ is the arithmetic mean of occupancy rates. A soft constraint $G_t \leq G_{\text{target}} + \epsilon$ regulates transfers, where $G_{\text{target}}$ is the target upper bound and $\epsilon$ is a relaxation term.

\textbf{Note on Fairness Metric Direction.} In the reward function and evaluation, we use \textbf{Fairness = 1 - G$_t$} as the optimization objective (direction $\uparrow$), making the fairness metric directionally consistent with other performance metrics (higher is better). The Gini coefficient $G_t$ itself decreases as fairness improves, while $(1-G_t)$ increases.

\subsection{Pareto Multi-Objective Optimization Methodology}
\label{sec:pareto_methodology}

\textbf{Pareto Optimality Definition with Unified Objective Directions.} To unify comparison criteria, \textbf{Pareto and hypervolume (HV) computation convert all objectives to 'higher-is-better' utility form}: objectives requiring minimization are negated (\textbf{J$_2'$ = $-$Delay}; if including transfer dependency, \textbf{J$_6'$ = 1 $-$ TransferRate}), while remaining objectives (J$_1$ Throughput$\uparrow$, J$_3$ Fairness$\uparrow$, J$_4$ Stability$\uparrow$, J$_5$ Safety$\uparrow$) are already in "higher-is-better" form and retain original direction. Formally, for the six-dimensional objective vector $\mathbf{f}(\mathbf{x}) = [J_1, -J_2, J_3, J_4, J_5, 1-J_6]$ (all maximized), solution $\mathbf{x}^*$ is Pareto-optimal if and only if there does not exist another solution $\mathbf{x}$ such that $f_i(\mathbf{x}) \geq f_i(\mathbf{x}^*)$ for all $i$ with strict inequality for at least one $j$.

\textbf{Pareto Frontier Identification.} We employ the efficient non-dominated sorting algorithm to identify the Pareto-optimal solution set. For each solution, we compute its domination count; solutions with zero domination count constitute the Pareto frontier. To ensure solution feasibility, we introduce a stability threshold filter (Stability $\geq$ 0.5) to exclude system crash solutions.

\textbf{Hypervolume Computation.} Hypervolume (HV) measures Pareto frontier quality, defined as HV = Volume($\{\mathbf{y} \mid \exists \mathbf{x} \in \text{PF}: \mathbf{f}(\mathbf{x}) \prec \mathbf{y} \prec \mathbf{r}\}$), where $\mathbf{r}$ is the reference point. \textbf{Normalization method}: Each objective is normalized to the $[0,1]$ interval using min-max scaling $f'_i = (f_i - f_{i,\min})/(f_{i,\max} - f_{i,\min})$, where $f_{i,\min}$/$f_{i,\max}$ are the minimum/maximum values across all 10,000 evaluated solutions for objective $i$. \textbf{Reference point selection}: $\mathbf{r} = [0, 0, 0, 0, 0, 0]$ (worst value for each objective after normalization), ensuring HV computation consistency and comparability. Figure captions in §6.5 adopt this unified specification.

\textbf{Knee-Point Detection Method.} Knee-points are the most representative solutions on the Pareto frontier, representing critical objective tradeoff inflection points. We employ a multi-criteria decision making (MCDM) approach to select knee-points:

\begin{enumerate}
\item \textbf{Quality criterion (40\%)}: Compute each Pareto solution's normalized distance to the ideal point $d_{\text{ideal}} = \|\mathbf{f}(\mathbf{x}) - \mathbf{f}^*\|$, where $\mathbf{f}^* = [1, 1, 1, 1, 1, 1]$ represents the best achievable value for each normalized objective (all objectives scaled to $[0,1]$ via min-max normalization); smaller distance indicates higher quality

\item \textbf{Diversity criterion (40\%)}: Sparsity score based on K-nearest neighbor distance $s_{\text{div}} = \text{mean}(\text{dist}_{k\text{-nearest}})$; larger distance indicates stronger representativeness

\item \textbf{Balance criterion (20\%)}: Objective balance based on coefficient of variation $CV = \sigma/\mu$; smaller CV indicates better balance across objectives
\end{enumerate}

The composite score is Score = $0.4 \cdot q_{\text{quality}} + 0.4 \cdot q_{\text{diversity}} + 0.2 \cdot q_{\text{balance}}$. We select the top $K = \max(5, \min(15, \text{round}(|\text{PF}|/20)))$ solutions as knee-points (using rounding, e.g., for $|\text{PF}|=262$, we get $262/20 \approx 13$), where $|\text{PF}|$ is the Pareto frontier size.

\subsection{Deep Reinforcement Learning Integration}
\label{sec:drl_integration}

\textbf{State Space (29-Dimensional Dict).} We adopt a compact, efficient state design comprising 7 key observation dimensions:

\begin{enumerate}
\item \textbf{Queue lengths} (\texttt{queue\_lengths}, 5-dim): Current queue length at each layer

\item \textbf{Occupancy rates} (\texttt{occupancy\_rates}, 5-dim; code: \texttt{utilization\_rates}): Normalized ratio $U_\ell = Q_\ell/C_\ell$ representing occupancy/congestion, \emph{not} traffic intensity $\rho=\lambda/(\mu \cdot C)$

\item \textbf{Queue change rates} (\texttt{queue\_changes}, 5-dim): Queue length changes between adjacent time steps

\item \textbf{Load rates} (\texttt{load\_rates}, 5-dim): System load indicator as arrival rate over service capacity $\rho_\ell = \lambda_\ell/(\mu_\ell \cdot C_\ell)$ (\textbf{Note}: This is the true traffic intensity in queueing theory; stability requires $\rho < 1$)

\item \textbf{Service rates} (\texttt{service\_rates}, 5-dim): Current actual service rate at each layer

\item \textbf{Previous reward} (\texttt{prev\_reward}, 1-dim): Reward feedback from the previous step

\item \textbf{System metrics} (\texttt{system\_metrics}, 3-dim): [Overall load, average occupancy rate, stability score]
\end{enumerate}

This design avoids redundant information and focuses on core features of queue dynamics. \textbf{Terminology clarification}: Occupancy rate $U=Q/C$ measures capacity utilization (0-1 indicates normal, $>$1 indicates overflow); traffic intensity $\rho=\lambda/(\mu \cdot C)$ measures system load strength ($<$1 is a necessary condition for stability).

\textbf{Action Space (11-Dimensional Dict, Mixed Discrete-Continuous).}

\begin{enumerate}
\item \textbf{Continuous actions (6-dim)}:
\begin{itemize}
\item \texttt{service\_intensities} (5-dim): Controls service intensity multiplier for each layer $\in [0.1, 2.0]$
\item \texttt{arrival\_multiplier} (1-dim, implementation variable name): Adjusts global arrival rate multiplier $\in [0.5, 5.0]$
\end{itemize}

\textbf{Semantic constraint on arrival\_multiplier}: This parameter represents \textbf{system-side throttling/gating} (e.g., queue entrance quota or temporary holding mechanism), belonging to control-layer strategy rather than demand-side natural arrivals. When enabled, actual arrival rate becomes $\lambda'_k = m \cdot \lambda_k$ (where $m$ is the policy-output multiplier), used for emergency peak-shaving or capacity protection.

\textbf{Stability safety gate (explicit projection)}: During execution, we apply \textbf{soft clipping} to \texttt{arrival\_multiplier} ensuring all layers satisfy $\rho_\ell < 1$. The projection formula is:
\begin{equation}
m^* = \min\left(m_{\text{proposed}}, \max\{m : \max_\ell \rho_\ell(m) < 1\}\right)
\label{eq:arrival_clip}
\end{equation}
where $\rho_\ell(m) = (m \cdot \lambda_\ell)/(\mu_\ell \cdot C_\ell)$.

\item \textbf{Discrete actions (5-dim)}: \texttt{emergency\_transfers} (MultiBinary$\times$5), emergency transfer flag for each layer; 1 indicates trigger downward transfer to the layer below (i.e., layer $\ell$ $\rightarrow$ layer $\ell-1$), 0 indicates maintain current state
\end{enumerate}

\textbf{Reward Function Design and Training-Evaluation Objective Mapping.} We employ a multi-objective weighted-sum reward structure comprising positive incentives (J$_1$ throughput, J$_3$ fairness, energy efficiency $E$) and penalty terms (congestion proxy for J$_2$, instability proxy for J$_4$). Training optimizes 5 core components, where \textbf{J$_2$ delay} is approximated by \textbf{congestion penalty} (instantaneous negative feedback for queue overflow, providing faster signal than delay measurement), \textbf{J$_4$ stability} via instability penalty, \textbf{J$_5$ safety} implicitly enforced through congestion penalties (overflow = violation), and \textbf{J$_6$ transfer efficiency} computed only during evaluation as a diagnostic metric.

\textbf{Energy efficiency $E$ definition}: Normalized indicator taking values in $[0,1]$ (higher = better):
\begin{equation*}
E = \frac{\text{completed orders} / \text{total energy}}{ E_{\max}}
\end{equation*}
Total energy follows a simplified model: higher service rate $\mu$ yields lower per-unit cost; altitude closer to 120m cruise layer has lower energy cost.

The composite training reward is:
\begin{equation}
R_t = 10 \cdot J_1 + 5 \cdot J_3 + 3 \cdot E - 20 \cdot \text{Congestion} - 15 \cdot \text{Instability} + 2 \cdot \text{Transfer}_{\text{benefit}} + 2 \cdot \text{Stability}_{\text{bonus}}
\label{eq:reward}
\end{equation}

\textbf{Weight design rationale}:

\begin{enumerate}
\item \textbf{Congestion penalty weight ($-$20) exceeds throughput reward (+10)}: Based on queueing theory stability constraint $\rho < 1$, the system must prioritize avoiding queue overflow to maintain long-term stability. Congestion leads to cascading failures with costs far exceeding single-step throughput loss, hence the 2:1 penalty-to-reward ratio.

\item \textbf{Fairness weight (+5) is 50\% of throughput}: Reflects system emphasis on load balancing (J$_3$), preventing individual layer overload while others remain idle, embodying urban airspace "safety $>$ performance $>$ cost" priority.

\item \textbf{Instability penalty ($-$15) slightly lower than congestion penalty}: Queue oscillations affect service quality but do not immediately cause system collapse, therefore penalty strength is set at 75\% of congestion penalty.

\item \textbf{Energy efficiency weight (+3) is lowest}: Energy efficiency ($E$) as a long-term optimization goal has lower immediate reward value than throughput and fairness, set at 30\% of baseline throughput weight. \textbf{Energy efficiency ($E$) $\neq$ transfer efficiency}: Training reward's energy efficiency ($E$) measures energy consumption efficiency, completely independent from evaluation metric J$_6$ transfer efficiency (diagnosing inter-layer transfer dependency).

\item \textbf{Transfer benefit reward (+2)}: When transfers successfully alleviate high-layer pressure, reward is granted to encourage effective emergency regulation. Calculation: when \texttt{upper\_pressure} $>$ \texttt{lower\_util}, $\text{transfer}_{\text{benefit}} = +2.0 \times \text{transfer count}$.

\item \textbf{System stability reward (+2)}: Stability reward based on queue changes over the most recent 10 steps, $\text{stability}_{\text{bonus}} = +2.0 \times \exp(-\text{avg\_change} / 2.0)$, encouraging smooth operation.
\end{enumerate}

\textbf{Training reward weights}: [Throughput:Fairness:Energy:Transfer:Stability:Congestion:Instability] = [10:5:3:2:2:$-$20:$-$15], where the first 5 items are positive incentives and the last 2 are penalties. Transfer and stability are auxiliary reward terms; primary weights remain the first 3 items (throughput/fairness/energy). Sensitivity analysis validates algorithm ranking remains stable under $\pm$20\% perturbation range.

\textbf{Evaluation reports complete 6 objectives}: $\{J_1 \text{ Throughput}\uparrow, J_2 \text{ Delay}\downarrow, J_3 \text{ Fairness}\uparrow, J_4 \text{ Stability}\uparrow, J_5 \text{ Safety}\uparrow, J_6 \text{ Transfer Efficiency (diagnostic)}\}$, where J$_2$/J$_4$ use actual measured values, and J$_6$ diagnoses whether the system over-relies on transfers (ideal $<$0.2\%, primarily relying on service rate modulation).

\textbf{Algorithm Adapters.} Discrete RL algorithms (Rainbow, R2D2) use adaptive discretization + prioritized experience replay (PER); continuous RL algorithms (PPO, TD7, SAC-v2, TD3, DDPG) employ separate policy/value heads for mixed actions; distributed RL (IMPALA) applies V-trace with queueing-aware batch processing.

% ============================================================
% Data authenticity statement
% ============================================================
% All formulas and parameters verified against:
% - Pressure metric: Chinese guide §4.1
% - Pareto methodology: Chinese guide §4.2
% - State-action design: Chinese guide §4.3
% - Reward weights: Chinese guide §4.3
% - All numerical values cross-checked with source files
% ============================================================
